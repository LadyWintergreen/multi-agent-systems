{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assignment 3 - MARL\n",
        "\n",
        "**Team: Wintergreen Systems**\n",
        "\n",
        "- Parisa\n",
        "- Sudha\n",
        "- Saniya\n",
        "- Elizabeth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f6oNYfPhAgq-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import copy\n",
        "import numpy as np\n",
        "import itertools\n",
        "import random\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.offsetbox import OffsetImage, AnnotationBbox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "from tqdm import tqdm\n",
        "matplotlib.use(\"TkAgg\")\n",
        "BOLD = \"\\033[1m\"\n",
        "RESET = \"\\033[0m\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multi-Agent Reinforcement Learning Environment for Package Delivery\n",
        "\n",
        "This class defines an environment where multiple agents work together to pick up, handover and deliver packages.\n",
        "\n",
        "**Attributes:**\n",
        "- reward_pickup (int): Reward for picking up a package.\n",
        "- reward_deliver (int): Reward for successfully delivering a package.\n",
        "- reward_handover (int): Reward for a successful handover between agents.\n",
        "- reward_move (int): Reward for a regular movement action.\n",
        "- reward_stuck (int): Penalty for an agent getting stuck.\n",
        "- Initial location coordinated of the agents are defined in: agentsA_coord and agentsB_coord\n",
        "- package collected or not for botht types of agents is defines in:  agentsA_collect and agentsB_collect.\n",
        "- agentsA_step (list of int): Number of steps taken by agents in group A.\n",
        "- agentsB_step (list of int): Number of steps taken by agents in group B.\n",
        "- test_length (int): Counter to keep track of agent steps which is required for reward calculation later.\n",
        "- packages_delivered (int): Number of packages successfully delivered.\n",
        "\n",
        "**Methods:**\n",
        "- reset_env(): Reset the environment to its initial state.\n",
        "- get_state(): Get the current state of the environment as a NumPy array.\n",
        "- move_agent(actionA, actionB): Move agents according to the specified actions and update rewards.\n",
        "- get_grid(): Get the current grid representation.\n",
        "- setup_grid(): Set up the initial grid with agents and locations.\n",
        "- plot_grid(snapshot, ax=None): Visualize the current state of the environment.\n",
        "\n",
        "**Purpose of this class:**\n",
        "The purpose of the `Environment` class is to define and manage the simulation environment for a multi-agent reinforcement learning scenario where agents are tasked with picking up, handing over and delivering packages in a grid-based world. This class serves as the foundation for creating and controlling the environment in which agents interact and learn.\n",
        "\n",
        "1. **Grid-Based Simulation:** The class creates a grid-based environment with specified dimensions where the agents operate. It sets up the grid and initial positions of key elements within the environment.\n",
        "\n",
        "2. **Agent Interaction:** The class facilitates interactions between multiple agents (groups A and B) by allowing them to take actions and move within the grid. Agents can perform actions like picking up, delivering, or moving.\n",
        "\n",
        "3. **Reward Management:** The class handles reward calculation for agents based on their actions. Agents receive rewards for successful package pickup, delivery, and handovers, while also incurring penalties for movement and getting stuck.\n",
        "\n",
        "It is the foundation of creating a controlled environment in which agents can learn and make decisions while engaging in a package delivery task. It provides a structured framework for the multi-agent reinforcement learning algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Environment():\n",
        "    \n",
        "    #fields:\n",
        "    #a grid of size defined by parameter\n",
        "    #b, a tuple repsenting the delivery location\n",
        "    #a, a tuple representing the package location\n",
        "    #size, an int representing the horizontal and vertical dimensions of the grid\n",
        "    def __init__(self, size:int, verbose=0):\n",
        "        self.size = size\n",
        "        self.actions = ['north', 'south', 'west', 'east']\n",
        "        self.reward_pickup = 500\n",
        "        self.reward_deliver = 500\n",
        "        self.reward_handover = 1000\n",
        "        self.reward_move = -5\n",
        "        self.reward_stuck = -100\n",
        "        self.reset_env()\n",
        "        self.verbose = verbose\n",
        "    \n",
        "    def reset_env(self):\n",
        "        self.a=(0,0)\n",
        "        self.b=(self.size-1,self.size-1)\n",
        "        \n",
        "        combinations = list(itertools.product(list(range(self.size)), repeat=2))\n",
        "        # Remove (0, 0) and (size-1, size-1)\n",
        "        combinations = [(x, y) for x, y in combinations if (x, y) != (0, 0) and (x, y) != (self.size-1, self.size-1)]\n",
        "        # rd.shuffle(combinations)\n",
        "        agentA1, agentA2, agentB1, agentB2 = random.sample(combinations, 4)\n",
        "        self.agentsA_coord = [agentA1, agentA2]\n",
        "        self.agentsB_coord = [agentB1, agentB2]\n",
        "\n",
        "        self.agentsA_collect = [0,0]\n",
        "        self.agentsB_collect = [0,0]\n",
        "\n",
        "        self.terminated = False\n",
        "\n",
        "        self.agentsA_step = [0,0]\n",
        "        self.agentsB_step = [0,0]\n",
        "        self.test_length = 0\n",
        "\n",
        "        self.packages_delivered = 0\n",
        "\n",
        "        self.setup_grid()\n",
        "\n",
        "    # def coordinates_to_noisy_ohv(self, coordinate:int):\n",
        "    #     ohv = np.zeros(4)\n",
        "    #     noise = np.random.normal(-1e-5, 1e-5, ohv.shape)\n",
        "    #     ohv[coordinate] = 1\n",
        "    #     ohv = ohv + noise\n",
        "    #     return ohv\n",
        "    #     this is a legacy method from a one-hot representation of it\n",
        "    \n",
        "    # def get_optimal_distance(self):\n",
        "    #     return abs(self.agent_coords[0]-self.a[0]) + abs(self.agent_coords[0]-self.a[1]) + abs(self.a[0]-self.b[0]) + abs(self.a[1]-self.b[1])\n",
        "\n",
        "\n",
        "    #method to set up the original grid including a location\n",
        "    # def setup_grid(self):\n",
        "    #     grid = np.zeros((self.size, self.size))\n",
        "    #     self.grid = grid\n",
        "    #     grid[self.a] = 1\n",
        "    #     grid[self.b] = 2\n",
        "    #     for agentA in self.agentsA_coord:\n",
        "    #         grid[agentA] = 3\n",
        "    #     for agentB in self.agentsB_coord:\n",
        "    #         grid[agentB] = 4\n",
        "\n",
        "    def get_state(self):\n",
        "        #This method previously unpacked the coordinates into a tuple which can be used to index the qmatrix or for easy incorporation to the Deep Q Network\n",
        "        # coords = [self.agent_coords[0], self.agent_coords[1], self.a[0], self.a[1], self.b[0], self.b[1]]\n",
        "        # coords = [self.coordinates_to_noisy_ohv(c) for c in coords]\n",
        "        # coords.append(np.array(self.collected))\n",
        "        # return np.hstack(coords)\n",
        "        # return np.ndarray.flatten(np.array(coords))\n",
        "        return np.array([self.agentsA_coord[0][0],self.agentsA_coord[0][1],self.agentsA_coord[1][0],self.agentsA_coord[1][1],\n",
        "                         self.agentsB_coord[0][0],self.agentsB_coord[0][1],self.agentsB_coord[1][0],self.agentsB_coord[1][1],\n",
        "                         self.agentsA_collect[0], self.agentsA_collect[1], self.agentsB_collect[0], self.agentsB_collect[1]])\n",
        "\n",
        "    \n",
        "    #Method which updates the location of the agent on the grid. Currently just zeroes whatever it landed on - can include other logic instead\n",
        "    def move_agent(self, actionA, actionB):\n",
        "        x, y = self.a\n",
        "        self.grid[x, y] = 1  # Place \"a\" on the grid\n",
        "        x, y = self.b\n",
        "        self.grid[x, y] = 2  # Place \"b\" on the grid\n",
        "\n",
        "        agentsA_reward = [0,0]\n",
        "        agentsB_reward = [0,0]\n",
        "        agentsA_deliver = [0,0]\n",
        "        agentsB_deliver = [0,0]\n",
        "\n",
        "        for i in range(len(self.agentsA_step)):\n",
        "            self.agentsA_step[i] += 1\n",
        "        for i in range(len(self.agentsB_step)):\n",
        "            self.agentsB_step[i] += 1\n",
        "        \n",
        "        self.test_length += 1\n",
        "\n",
        "        agentsA_coord_old = self.agentsA_coord\n",
        "        agentsB_coord_old = self.agentsB_coord\n",
        "\n",
        "        #Assign new coordinates for agent A to exist at\n",
        "        for i in range(len(self.agentsA_coord)):\n",
        "            new_x, new_y = self.agentsA_coord[i]\n",
        "            if actionA[i] == 'north' and self.agentsA_coord[i][1] > 0:\n",
        "                new_y -= 1\n",
        "            elif actionA[i] == 'south' and self.agentsA_coord[i][1] < self.size - 1:\n",
        "                new_y += 1\n",
        "            elif actionA[i] == 'west' and self.agentsA_coord[i][0] > 0:\n",
        "                new_x -= 1\n",
        "            elif actionA[i] == 'east' and self.agentsA_coord[i][0] < self.size - 1:\n",
        "                new_x += 1\n",
        "\n",
        "            #Update the grid based on new agent coordinates\n",
        "            self.grid[self.agentsA_coord[i]] = 0\n",
        "            self.agentsA_coord[i] = (new_x, new_y)\n",
        "            self.grid[self.agentsA_coord[i]] = 3\n",
        "\n",
        "        #Assign new coordinates for agent B to exist at\n",
        "        for i in range(len(self.agentsB_coord)):\n",
        "            new_x, new_y = self.agentsB_coord[i]\n",
        "            if actionB[i] == 'north' and self.agentsB_coord[i][1] > 0:\n",
        "                new_y -= 1\n",
        "            elif actionB[i] == 'south' and self.agentsB_coord[i][1] < self.size - 1:\n",
        "                new_y += 1\n",
        "            elif actionB[i] == 'west' and self.agentsB_coord[i][0] > 0:\n",
        "                new_x -= 1\n",
        "            elif actionB[i] == 'east' and self.agentsB_coord[i][0] < self.size - 1:\n",
        "                new_x += 1\n",
        "\n",
        "            #Update the grid based on new agent coordinates\n",
        "            self.grid[self.agentsB_coord[i]] = 0\n",
        "            self.agentsB_coord[i] = (new_x, new_y)\n",
        "            self.grid[self.agentsB_coord[i]] = 4\n",
        "        \n",
        "        # Agent A:\n",
        "        for i in range(len(self.agentsA_coord)):\n",
        "            if self.agentsA_collect[i] == 0:\n",
        "                if self.agentsA_coord[i] == self.a:\n",
        "                    if self.verbose>2:\n",
        "                        print(\"A{} pickup\".format(i))\n",
        "                    # self.agentsA_step[i] = 0\n",
        "                    self.agentsA_collect[i]=1\n",
        "                    agentsA_reward[i] = self.reward_pickup\n",
        "                else:\n",
        "                    agentsA_reward[i] = self.reward_move #-(self.agentsA_step[i]**2)\n",
        "            else:\n",
        "                for j in range(len(self.agentsB_coord)):\n",
        "                    if self.agentsA_coord[i] == agentsB_coord_old[j]:\n",
        "                        if self.agentsB_coord[j] == agentsA_coord_old[i]:\n",
        "                            if self.agentsB_collect[j] == 0:\n",
        "                                if self.agentsA_collect[i] == 1:\n",
        "                                    self.agentsA_collect[i]=0\n",
        "                                    agentsA_deliver[i]=1\n",
        "                                    self.agentsB_collect[j]=1\n",
        "                                    agentsA_reward[i] = self.reward_handover - ((np.abs(self.agentsA_coord[i][0] - self.a[0]) + np.abs(self.agentsA_coord[i][1] - self.a[1])) ** 2)\n",
        "                                    agentsB_reward[j] = self.reward_handover - ((np.abs(self.agentsB_coord[j][0] - self.b[0]) + np.abs(self.agentsB_coord[j][1] - self.b[1])) ** 2)\n",
        "                                    if self.verbose>2:\n",
        "                                        print(\"A{} B{} handover\".format(i,j))\n",
        "                                        print(\"\\tagentsA_reward\",agentsA_reward[i], self.agentsA_step[i])\n",
        "                                        print(\"\\tagentsB_reward\",agentsB_reward[j], self.agentsB_step[j])\n",
        "                                    self.agentsA_step[i]=0 ; self.agentsB_step[j]=0\n",
        "\n",
        "                if agentsA_deliver[i]==0:\n",
        "                    agentsA_reward[i] = self.reward_move #-(self.agentsA_step[i]**2)\n",
        "\n",
        "            # if self.agentsA_coord[i] == agentsA_coord_old[i]:\n",
        "            #     agentsB_reward[i] = self.reward_stuck\n",
        "\n",
        "        # Agent B:\n",
        "        for i in range(len(self.agentsB_coord)):\n",
        "            if self.agentsB_collect[i] == 1:\n",
        "                if self.agentsB_coord[i] == self.b:\n",
        "                    if self.verbose>2:\n",
        "                        print(\"B{} deliver\".format(i))\n",
        "                    # self.agentsB_step[i]=0\n",
        "                    agentsB_deliver[i]=1\n",
        "                    self.agentsB_collect[i]=0\n",
        "                    agentsB_reward[i] = self.reward_deliver\n",
        "                    self.packages_delivered += 1\n",
        "                else:\n",
        "                    agentsB_reward[i] = self.reward_move #-(self.agentsB_step[i]**2)\n",
        "            else:\n",
        "                agentsB_reward[i] = self.reward_move #-(self.agentsB_step[i]**2)\n",
        "\n",
        "            # if self.agentsB_coord[i] == agentsB_coord_old[i]:\n",
        "            #     agentsB_reward[i] = self.reward_stuck\n",
        "\n",
        "        if self.test_length > 200:\n",
        "            if self.verbose>2:\n",
        "                print(\"terminated\")\n",
        "            self.terminated = True\n",
        "\n",
        "        if self.verbose>3:\n",
        "            print(\"agentsA_reward\",agentsA_reward)\n",
        "            print(\"agentsB_reward\",agentsB_reward)\n",
        "\n",
        "        return agentsA_reward, agentsB_reward, agentsA_deliver, agentsB_deliver, self.packages_delivered, self.terminated\n",
        "\n",
        "    def get_grid(self):\n",
        "        return self.grid.tolist()\n",
        "\n",
        "    #method to set up the original grid including a location\n",
        "    def setup_grid(self):\n",
        "        self.grid = np.zeros((self.size, self.size))\n",
        "            \n",
        "        x, y = self.a\n",
        "        self.grid[x, y] = 1  # Place \"a\" on the grid\n",
        "\n",
        "        x, y = self.b\n",
        "        self.grid[x, y] = 2  # Place \"b\" on the grid\n",
        "\n",
        "        # Place the \"home\" location (0) on the grid\n",
        "        for i in range(len(self.agentsA_coord)):\n",
        "            x, y = self.agentsA_coord[i]\n",
        "            self.grid[x, y] = 3\n",
        "\n",
        "        for i in range(len(self.agentsB_coord)):\n",
        "            x, y = self.agentsB_coord[i]\n",
        "            self.grid[x, y] = 4\n",
        "    \n",
        "    def plot_grid(self, snapshot, ax=None):\n",
        "        if ax is None:\n",
        "            fig, ax = plt.subplots()\n",
        "            ax.set_facecolor('white')\n",
        "        else:\n",
        "            ax.clear()\n",
        "\n",
        "        # Plot the grid\n",
        "        ax.imshow(np.array([[0]]), cmap=\"bone\", extent=[0, self.size, 0, self.size])\n",
        "\n",
        "        for i in range(self.size):\n",
        "            for j in range(self.size):\n",
        "                cell_value = snapshot[i][j]\n",
        "                images_to_display = []\n",
        "\n",
        "                if cell_value == 3:\n",
        "                    images_to_display.append(OffsetImage(agent_img, zoom=0.08, alpha=0.5))\n",
        "                elif cell_value == 4:\n",
        "                    images_to_display.append(OffsetImage(agent_img2, zoom=0.05, alpha=0.5))\n",
        "                elif cell_value == 1:\n",
        "                    images_to_display.append(OffsetImage(package_img, zoom=0.03, alpha=0.75))\n",
        "                elif cell_value == 2:\n",
        "                    images_to_display.append(OffsetImage(destinationB_img, zoom=0.05, alpha=0.75))\n",
        "\n",
        "                for image in images_to_display:\n",
        "                    ab = AnnotationBbox(image, (j + 0.5, self.size - i - 0.5), frameon=False)\n",
        "                    ax.add_artist(ab)\n",
        "\n",
        "        # Set axis properties\n",
        "        ax.set_xlim(0, self.size)\n",
        "        ax.set_ylim(0, self.size)\n",
        "        ax.set_xticks(np.arange(self.size) + 1)\n",
        "        ax.set_yticks(np.arange(self.size) + 1)\n",
        "        ax.set_xticklabels([])\n",
        "        ax.set_yticklabels([])\n",
        "        ax.grid(True, linewidth=2, color='white')\n",
        "\n",
        "        # Set title\n",
        "        ax.set_title(\"Package Delivery Agent\")\n",
        "\n",
        "        return ax\n",
        "agent_img = plt.imread('agent.jpg')\n",
        "agent_img2 = plt.imread(\"agent2.png\")\n",
        "package_img = plt.imread('package.jpg')\n",
        "destinationB_img = plt.imread('destinationB.jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Deep Q-Network (DQN) for RL\n",
        "\n",
        "This class defines a DQN model for reinforcement learning tasks. It consists of a neural network that approximates Q-values for state-action pairs and supports training and target network updates. This the the DQN which will be used to train both type of Agents (A and B).\n",
        "\n",
        "**Attributes:**\n",
        "- model (torch.nn.Sequential): The main neural network model for Q-value approximation.\n",
        "- model2 (torch.nn.Sequential): A target network with the same architecture as the main model.\n",
        "- loss_fn (torch.nn.MSELoss): Mean Squared Error loss function for training.\n",
        "\n",
        "**Methods:**\n",
        "- update_target(): Copy the state of the prediction network to the target network.\n",
        "- get_qvals(next_state): Get Q-values for the next state based on the prediction network.\n",
        "- get_maxQ(state): Get the maximum Q-value for a state based on the target network.\n",
        "- train_one_step(states, actions, targets): Perform a single training step with provided minibatches.\n",
        "\n",
        "**Purpose of this class:**\n",
        "The purpose of the `DQN` class is to provide a framework for creating and training Deep Q-Networks (DQNs) for reinforcement learning tasks for both type of Agents.\n",
        "\n",
        "1. **Neural Network Model:** The class defines a neural network model that approximates Q-values for state-action pairs. This model is the main part of the DQN and is used for making action predictions.\n",
        "\n",
        "2. **Target Network:** The class includes a target network, which is a copy of the main model. This target network is periodically updated with the state of the prediction network to stabilize training.\n",
        "\n",
        "3. **Loss Function:** It provides a Mean Squared Error (MSE) loss function, used to calculate the loss during training. The loss quantifies the error between predicted Q-values and target Q-values.\n",
        "\n",
        "4. **Training:** The class offers a method (`train_one_step`) for performing a single training step. It updates the model's weights based on a minibatch of states, actions, and corresponding TD (Temporal Difference) targets. This training process enables the model to learn optimal Q-values.\n",
        "\n",
        "5. **Q-Value Computation:** It provides methods for calculating Q-values. The `get_qvals` method computes Q-values for a given state-action pair based on the prediction network, while the `get_maxQ` method finds the maximum Q-value for a state based on the target network.\n",
        "\n",
        "6. **Target Network Update:** The class includes a method (`update_target`) for synchronizing the target network with the prediction network. This process is crucial for stabilizing training in reinforcement learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Dz-hPZbq4sli"
      },
      "outputs": [],
      "source": [
        "class DQN:\n",
        "    def __init__(self, learning_rate, state_size=100, action_size = 4):\n",
        "        l1 = state_size\n",
        "        l2 = 24\n",
        "        l3 = 24\n",
        "        l4 = action_size\n",
        "        self.model = torch.nn.Sequential(\n",
        "        torch.nn.Linear(l1, l2),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Linear(l2, l3),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Linear(l3,l4))\n",
        "\n",
        "        self.model2 = copy.deepcopy(self.model)\n",
        "        self.model2.load_state_dict(self.model.state_dict())\n",
        "        self.loss_fn = torch.nn.MSELoss()\n",
        "        self.learning_rate = learning_rate\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "\n",
        "# The function \"update_target\" copies the state of the prediction network to the target network. You need to use this in regular intervals.\n",
        "    def update_target(self):\n",
        "        self.model2.load_state_dict(self.model.state_dict())\n",
        "\n",
        "# The function \"get_qvals\" returns a numpy list of qvals for the next_state given by the argument based on the prediction network.\n",
        "    def get_qvals(self, next_state):\n",
        "        state1 = torch.from_numpy(next_state).float()\n",
        "        qvals_torch = self.model(state1)\n",
        "        qvals = qvals_torch.data.numpy()\n",
        "        return qvals\n",
        "\n",
        "# The function \"get_maxQ\" returns the maximum q-value for the state given by the argument based on the target network.\n",
        "    def get_maxQ(self,state):\n",
        "        return torch.max(self.model2(torch.from_numpy(state).float())).float()\n",
        "\n",
        "# The function \"train_one_step_new\" performs a single training step.\n",
        "# It returns the current loss (only needed for debugging purposes).\n",
        "# Its parameters are three parallel lists: a minibatch of states, a minibatch of actions,\n",
        "# a minibatch of the corresponding TD targets and the discount factor.\n",
        "    def train_one_step(self, states, actions, targets):\n",
        "        targets_reply = []\n",
        "        state1_batch = torch.from_numpy(states).float()\n",
        "        # state1_batch = torch.cat([torch.from_numpy(s).float() for s in states])\n",
        "        action_batch = torch.Tensor(actions)\n",
        "        Q1 = self.model(state1_batch)\n",
        "        X = Q1.gather(dim=1,index=action_batch.long().unsqueeze(dim=1)).squeeze()\n",
        "        Y = torch.tensor(targets).float()\n",
        "        loss = self.loss_fn(X, Y)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### AGENT A CLASS\n",
        "\n",
        "The AgentA class is a subclass of the DQN class for type A agents. It represents an agent that can interact with an environment, learn from experiences, and make decisions using Q-learning.\n",
        "\n",
        "**Attribites**:\n",
        "- statespace_size (int): The size of the state space.\n",
        "- gamma (float): The discount factor for future rewards.\n",
        "- learning_rate (float): The learning rate used for updating the Q-network.\n",
        "- start_epsilon (float): The initial exploration rate (epsilon-greedy strategy).\n",
        "- epsilon_decay_factor (float): The factor by which epsilon is decayed over time.\n",
        "- min_epsilon (float): The minimum value for epsilon.\n",
        "- replay_buffer_size (int): The size of the replay buffer for experience replay.\n",
        "- batch_size (int): The size of mini-batches for training the network.\n",
        "- network_copy_frequency (int): The frequency at which the target network is updated.\n",
        "\n",
        "**Method**:\n",
        "- remember(self, state, actionA, agentsA_reward, agentsA_deliver, next_state): Stores a transition (experience) in the agent's memory.\n",
        "- update_target(self): Updates the target Q-network based on the defined network_copy_frequency. It calls the update_target method from the parent class DQN.\n",
        "- act(self, state): Decides the action to take in the current state using an epsilon-greedy strategy. It explores with probability epsilon and exploits with probability 1 - epsilon.\n",
        "- process_minibatch(self, minibatch): Prepares the data for updating the Q-network based on a mini-batch of experiences.\n",
        "- save_model_parameters(self, destination): Saves the model parameters of the agent's Q-network to a specified file.\n",
        "- load_model_parameters(self, path): Loads model parameters for the agent's Q-network from a specified file.\n",
        "\n",
        "**Purpose of this class**:\n",
        "\n",
        "1. DQN Implementation: It is an implementation of the Deep Q-Network (`DQN`) algorithm, which is a popular method for solving reinforcement learning problems by approximating the optimal action-value function.\n",
        "\n",
        "2. Experience Storage: The `remember` method is responsible for storing experiences (state, action, reward, delivery flag, next state) in the agent's memory. Experience replay is a key feature of DQN algorithms, which helps the agent learn from past experiences.\n",
        "\n",
        "3. Epsilon-Greedy Policy: The `act` method implements an epsilon-greedy policy for action selection. It balances exploration (taking random actions) and exploitation (choosing the action with the highest estimated Q-value).\n",
        "\n",
        "4. Network Update: The `update_target` method is responsible for updating the target Q-network. In DQN, there are typically two Q-networks: one for learning (the primary network) and one for target values. The target network is periodically updated to stabilize training.\n",
        "\n",
        "5. Mini-Batch Processing: The `process_minibatch` method prepares the data for training the Q-network using mini-batches of experiences. It computes target Q-values for each experience, which are used to update the network.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AgentA(DQN):\n",
        "    def __init__(self, statespace_size=12, gamma=0.99, learning_rate=0.997, start_epsilon=1.0,\n",
        "                 epsilon_decay_factor=0.999, min_epsilon=0.05, replay_buffer_size=10000,\n",
        "                 batch_size=200, network_copy_frequency=10):\n",
        "        super().__init__(learning_rate=learning_rate, state_size=statespace_size, action_size=4)  # Call the parent constructor\n",
        "        self.statespace_size = statespace_size\n",
        "        self.gamma = gamma\n",
        "        # self.learning_rate = learning_rate\n",
        "        self.memory = []  # Using a list instead of a deque\n",
        "        self.epsilon = start_epsilon\n",
        "        self.epsilon_min = min_epsilon\n",
        "        self.epsilon_decay_factor = epsilon_decay_factor\n",
        "        self.replay_buffer_size = replay_buffer_size\n",
        "        self.batch_size = batch_size\n",
        "        self.network_copy_frequency = network_copy_frequency\n",
        "        self.steps_since_copy = 0  # Counter for network copy\n",
        "    \n",
        "    def remember(self, state, actionA, agentsA_reward, agentsA_deliver, next_state):\n",
        "        self.memory.append((state, actionA, agentsA_reward, agentsA_deliver, next_state))\n",
        "        while len(self.memory) > self.replay_buffer_size:\n",
        "            self.memory.pop(0)  # Remove the oldest experience\n",
        "\n",
        "    def update_target(self):\n",
        "        if self.steps_since_copy % self.network_copy_frequency == 0:\n",
        "            super().update_target()\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return np.random.choice(range(4))  # Random action\n",
        "        q_values = self.get_qvals(state)\n",
        "        return np.argmax(q_values)  # Greedy action\n",
        "\n",
        "    def process_minibatch(self, minibatch):\n",
        "        states = []\n",
        "        actions = []\n",
        "        targets = []\n",
        "        for state, actionA, agentsA_reward, agentsA_deliver, next_state in minibatch:\n",
        "            q_values = self.get_qvals(state)\n",
        "                \n",
        "            if agentsA_deliver==1:\n",
        "                q_values[actionA] = agentsA_reward\n",
        "            else:\n",
        "                q_values[actionA] = agentsA_reward + self.gamma * self.get_maxQ(next_state)\n",
        "            \n",
        "            actions.append(actionA)\n",
        "            targets.append(q_values[actionA])\n",
        "        \n",
        "            states.append(state)\n",
        "            \n",
        "        return np.array(states), np.array(actions), np.array(targets)\n",
        "\n",
        "    def save_model_parameters(self, destination):\n",
        "        torch.save(self.model2.state_dict(), destination)\n",
        "\n",
        "    def load_model_parameters(self, path):\n",
        "        self.model.load_state_dict(torch.load(path))\n",
        "        self.model2.load_state_dict(torch.load(path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### AGENT B CLASS\n",
        "\n",
        "The AgentB class is a subclass of the DQN class for type B agents. It represents an agent that can interact with an environment, learn from experiences, and make decisions using Q-learning.\n",
        "\n",
        "**Attribites**:\n",
        "- statespace_size (int): The size of the state space.\n",
        "- gamma (float): The discount factor for future rewards.\n",
        "- learning_rate (float): The learning rate used for updating the Q-network.\n",
        "- start_epsilon (float): The initial exploration rate (epsilon-greedy strategy).\n",
        "- epsilon_decay_factor (float): The factor by which epsilon is decayed over time.\n",
        "- min_epsilon (float): The minimum value for epsilon.\n",
        "- replay_buffer_size (int): The size of the replay buffer for experience replay.\n",
        "- batch_size (int): The size of mini-batches for training the network.\n",
        "- network_copy_frequency (int): The frequency at which the target network is updated.\n",
        "\n",
        "**Method**:\n",
        "- remember(self, state, actionB, agentsB_reward, agentsB_deliver, next_state): Stores a transition (experience) in the agent's memory.\n",
        "- update_target(self): Updates the target Q-network based on the defined network_copy_frequency. It calls the update_target method from the parent class DQN.\n",
        "- act(self, state): Decides the action to take in the current state using an epsilon-greedy strategy. It explores with probability epsilon and exploits with probability 1 - epsilon.\n",
        "- process_minibatch(self, minibatch): Prepares the data for updating the Q-network based on a mini-batch of experiences.\n",
        "- save_model_parameters(self, destination): Saves the model parameters of the agent's Q-network to a specified file.\n",
        "- load_model_parameters(self, path): Loads model parameters for the agent's Q-network from a specified file.\n",
        "\n",
        "**Purpose of this class**:\n",
        "\n",
        "1. DQN Implementation: It is an implementation of the Deep Q-Network (`DQN`) algorithm, which is a popular method for solving reinforcement learning problems by approximating the optimal action-value function.\n",
        "\n",
        "2. Experience Storage: The `remember` method is responsible for storing experiences (state, action, reward, delivery flag, next state) in the agent's memory. Experience replay is a key feature of DQN algorithms, which helps the agent learn from past experiences.\n",
        "\n",
        "3. Epsilon-Greedy Policy: The `act` method implements an epsilon-greedy policy for action selection. It balances exploration (taking random actions) and exploitation (choosing the action with the highest estimated Q-value).\n",
        "\n",
        "4. Network Update: The `update_target` method is responsible for updating the target Q-network. In DQN, there are typically two Q-networks: one for learning (the primary network) and one for target values. The target network is periodically updated to stabilize training.\n",
        "\n",
        "5. Mini-Batch Processing: The `process_minibatch` method prepares the data for training the Q-network using mini-batches of experiences. It computes target Q-values for each experience, which are used to update the network.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AgentB(DQN):\n",
        "    def __init__(self, statespace_size=12, gamma=0.99, learning_rate=0.997, start_epsilon=1.0,\n",
        "                 epsilon_decay_factor=0.999, min_epsilon=0.05, replay_buffer_size=10000,\n",
        "                 batch_size=200, network_copy_frequency=10):\n",
        "        super().__init__(learning_rate=learning_rate, state_size=statespace_size, action_size=4)  # Call the parent constructor\n",
        "        self.statespace_size = statespace_size\n",
        "        self.gamma = gamma\n",
        "        self.memory = []  # Using a list instead of a deque\n",
        "        self.epsilon = start_epsilon\n",
        "        self.epsilon_min = min_epsilon\n",
        "        self.epsilon_decay_factor = epsilon_decay_factor\n",
        "        self.replay_buffer_size = replay_buffer_size\n",
        "        self.batch_size = batch_size\n",
        "        self.network_copy_frequency = network_copy_frequency\n",
        "        self.steps_since_copy = 0  # Counter for network copy\n",
        "\n",
        "    def remember(self, state, actionB, agentsB_reward, agentsB_deliver, next_state):\n",
        "        self.memory.append((state, actionB, agentsB_reward, agentsB_deliver, next_state))\n",
        "        while len(self.memory) > self.replay_buffer_size:\n",
        "            self.memory.pop(0)  # Remove the oldest experience\n",
        "\n",
        "    def update_target(self):\n",
        "        if self.steps_since_copy % self.network_copy_frequency == 0:\n",
        "            super().update_target()\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return np.random.choice(range(4))  # Random action\n",
        "        q_values = self.get_qvals(state)\n",
        "        return np.argmax(q_values)  # Greedy action\n",
        "\n",
        "    def process_minibatch(self, minibatch):\n",
        "        states = []\n",
        "        actions = []\n",
        "        targets = []\n",
        "        for state, actionB, agentsB_reward, agentsB_deliver, next_state in minibatch:\n",
        "            q_values = self.get_qvals(state)\n",
        "\n",
        "            if agentsB_deliver==1:\n",
        "                q_values[actionB] = agentsB_reward\n",
        "            else:\n",
        "                q_values[actionB] = agentsB_reward + self.gamma * self.get_maxQ(next_state)\n",
        "            \n",
        "            actions.append(actionB)\n",
        "            targets.append(q_values[actionB])\n",
        "        \n",
        "            states.append(state)\n",
        "            \n",
        "        return np.array(states), np.array(actions), np.array(targets)\n",
        "\n",
        "    def save_model_parameters(self, destination):\n",
        "        torch.save(self.model2.state_dict(), destination)\n",
        "\n",
        "    def load_model_parameters(self, path):\n",
        "        self.model.load_state_dict(torch.load(path))\n",
        "        self.model2.load_state_dict(torch.load(path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_one_hot_vector(my_x, my_y, friend_x, friend_y, other1_x, other1_y, other2_x, other2_y, my_flag, friend_flag, other1_flag, other2_flag, grid_size=5):\n",
        "    \n",
        "    # Initialize a grid to represent the positions and flags of agents\n",
        "    grid = np.zeros((grid_size, grid_size, 2, 4))\n",
        "\n",
        "    # Set the positions of agents from group A\n",
        "    if my_flag == 0:\n",
        "        grid[my_x, my_y, 0, 0] = 1\n",
        "    else:\n",
        "        grid[my_x, my_y, 1, 0] = 1\n",
        "\n",
        "    if friend_flag == 0:\n",
        "        grid[friend_x, friend_y, 0, 1] = 1\n",
        "    else:\n",
        "        grid[friend_x, friend_y, 1, 1] = 1\n",
        "\n",
        "    # Set the positions of agents from group B\n",
        "    if other1_flag == 0:\n",
        "        grid[other1_x, other1_y, 0, 2] = 1\n",
        "    else:\n",
        "        grid[other1_x, other1_y, 1, 2] = 1\n",
        "\n",
        "    if other2_flag == 0:\n",
        "        grid[other2_x, other2_y, 0, 3] = 1\n",
        "    else:\n",
        "        grid[other2_x, other2_y, 1, 3] = 1\n",
        "\n",
        "\n",
        "    # Add a small noise to all the zeros\n",
        "    noise_level = 1e-6\n",
        "    grid += np.random.uniform(0, noise_level, size=grid.shape)\n",
        "\n",
        "    # Flatten the grid into a one-hot vector\n",
        "    one_hot_vector = grid.reshape(-1)\n",
        "\n",
        "    return one_hot_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run visualization:\n",
        "Runs a visualization of the trained agent's movement in the environment.\n",
        "\n",
        "The `run_visualisation` function runs a visualization of package delivery episodes using a trained transportation agent. It simulates the agent's actions in the environment and displays the evolving grid at each step of the episode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_visualisation(agentA, agentB, max_steps=50, size=5):\n",
        "    #Assumes a fully trained transport agent. \n",
        "    #Target network should either have been trained in the preceeding step, or loaded from a pickled pytorch weights file\n",
        "    env = Environment(size=size,verbose=3)\n",
        "    # Create a figure and axis outside the loop\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_facecolor('white')\n",
        "\n",
        "    # Initialize the plot once with the initial grid\n",
        "    m = ax.imshow(env.grid, cmap=\"bone\", extent=[0, env.size, 0, env.size])\n",
        "    snapshots = []\n",
        "    for step in range(max_steps):\n",
        "        state = env.get_state()\n",
        "        stateA1 = np.array([state[0], state[1], state[2], state[3], state[4], state[5], state[6], state[7], state[8], state[9], state[10], state[11]])\n",
        "        stateA2 = np.array([state[2], state[3], state[0], state[1], state[4], state[5], state[6], state[7], state[9], state[8], state[10], state[11]])\n",
        "        stateB1 = np.array([state[4],state[5], state[6],state[7], state[0], state[1],state[2],state[3],state[10], state[11], state[8],state[9]])\n",
        "        stateB2 = np.array([state[6],state[7], state[4],state[5], state[0], state[1],state[2],state[3], state[11], state[10], state[8],state[9]])\n",
        "\n",
        "        actionA1= np.argmax(agentA.get_qvals(stateA1))\n",
        "        actionA2 = np.argmax(agentA.get_qvals(stateA2))\n",
        "        actionB1= np.argmax(agentB.get_qvals(stateB1))\n",
        "        actionB2 = np.argmax(agentB.get_qvals(stateB2))\n",
        "\n",
        "        actionA = [env.actions[actionA1], env.actions[actionA2]]\n",
        "        actionB = [env.actions[actionB1], env.actions[actionB2]]\n",
        "\n",
        "        _, _, _, _, packages_delivered, terminated  = env.move_agent(actionA, actionB)\n",
        "        snapshots.append(env.get_grid())\n",
        "\n",
        "        if terminated or packages_delivered>5:\n",
        "            break\n",
        "    print(BOLD, \"total packages_delivered\", packages_delivered, RESET)\n",
        "    for s in snapshots:\n",
        "        env.plot_grid(s, ax)\n",
        "        # Redraw the plot\n",
        "        fig.canvas.draw()\n",
        "        plt.pause(0.2)  # Add a short pause for visualization\n",
        "    # plt.pause(1.5)\n",
        "    plt.close()\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train Function\n",
        "\n",
        "The purpose of the `train_function` is to begin the training process for two types of agents (A and B) in a package delivery task within a multi-agent reinforcement learning environment. The task involves agent A picking up packages and handing them over to agent B, who is responsible for delivering the packages to their destination. Here's how the function serves this purpose:\n",
        "\n",
        "1. **Training Environment (env):** The function takes an environment (`env`) as one of its inputs. This environment represents the simulated world in which agents A and B operate. The environment defines the states, action, rewards of the package delivery task.\n",
        "\n",
        "2. **Agent A and Agent B (agentA, agentB):** The function receives two agent types (`agentA` and `agentB`) as inputs. Agent type A handles package pickup, and agent type B manages package delivery.\n",
        "\n",
        "3. **Number of Training Episodes (episodes):** The function allows specifying the number of training episodes. An episode represents one complete run of the package delivery task, starting from the initial state and ending when specific termination conditions are met.\n",
        "\n",
        "4. **Training Loop:** The core purpose of the function is to run a training loop for a specified number of episodes. In each episode, the following happens:\n",
        "\n",
        "   a. **Environment Initialization:** The environment is reset, bringing it back to its initial state, and the agents are placed in the environment.\n",
        "\n",
        "   b. **Agent Interaction:** All agents (2 of Type A and 2 of Type B) interact with the environment by selecting actions based on their current observations and learned policies.\n",
        "\n",
        "   c. **Experience Collection:** As the agents interact with the environment, they collect experiences, including the current state, the action taken, the received rewards, and the resulting state.\n",
        "\n",
        "   d. **Replay Memory:** The experiences are stored in the agents' replay memory, which is used for training the DQN models. This replay memory helps stabilize the learning process by randomly sampling past experiences.\n",
        "\n",
        "   e. **Training:** Both agents may perform training steps using their collected experiences. The DQN models are updated to improve the agents' policies. The training aims to make the agents better at making decisions during the package delivery task.\n",
        "\n",
        "   f. **Training Metrics:** The function also tracks training metrics such as loss values and empirical rewards (cumulative rewards obtained in each episode). These metrics can be used to monitor the training progress and evaluate the performance of the agents.\n",
        "\n",
        "5. **Termination and Exploration:** The function ensures that training episodes are terminated based on certain conditions. It also handles exploration strategy - epsilon-greedy exploration (by adjusting the agents' exploration rate during training using epslion decay).\n",
        "\n",
        "The `train_function` is responsible for coordinating the training process of two types of agents (A and B) in a multi-agent reinforcement learning environment. It allows the agents to learn optimal policies for the package delivery task, and it tracks training metrics to evaluate their performance and progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_function(env, agentA, agentB, episodes=100):\n",
        "    loss_history = []\n",
        "    empirical_reward_history = []\n",
        "    # distance_history =[]\n",
        "\n",
        "    for episode in (range(episodes)): #tqdm\n",
        "        print(BOLD, \"episode\", episode, RESET)\n",
        "        env.reset_env()\n",
        "        state = env.get_state()  # Reset the environment and get the initial state\n",
        "        \n",
        "        # steps_to_opt = env.get_optimal_distance()\n",
        "        steps_in_episode = 0\n",
        "        # done = False\n",
        "        terminated = False\n",
        "        total_reward = 0\n",
        "        packages_delivered = 0\n",
        "        episode_losses = []\n",
        "\n",
        "        if (episode>0 and episode%25 == 0) or (episode == episodes-1):\n",
        "            agentA.save_model_parameters(\"logs/target_params_A_{}_onehot.pt\".format(episode))\n",
        "            agentB.save_model_parameters(\"logs/target_params_B_{}_onehot.pt\".format(episode))\n",
        "\n",
        "        # print(\"epsilon\", agentA.epsilon)\n",
        "        \n",
        "        while (packages_delivered<5 and not terminated):\n",
        "\n",
        "            stateA1 = np.array([state[0], state[1], state[2], state[3], state[4], state[5], state[6], state[7], state[8], state[9], state[10], state[11]])\n",
        "            stateA2 = np.array([state[2], state[3], state[0], state[1], state[4], state[5], state[6], state[7], state[9], state[8], state[10], state[11]])\n",
        "            stateB1 = np.array([state[4],state[5], state[6],state[7], state[0], state[1],state[2],state[3],state[10], state[11], state[8],state[9]])\n",
        "            stateB2 = np.array([state[6],state[7], state[4],state[5], state[0], state[1],state[2],state[3], state[11], state[10], state[8],state[9]])\n",
        "\n",
        "            # print(\"state\", state)\n",
        "            actionA1 = agentA.act(stateA1)\n",
        "            actionA2 = agentA.act(stateA2)\n",
        "\n",
        "            actionB1 = agentB.act(stateB1)\n",
        "            actionB2 = agentB.act(stateB2)\n",
        "\n",
        "            actionA = [env.actions[actionA1], env.actions[actionA2]]\n",
        "            actionB = [env.actions[actionB1], env.actions[actionB2]]\n",
        "\n",
        "            agentsA_reward, agentsB_reward, agentsA_deliver, agentsB_deliver, packages_delivered, terminated = env.move_agent(actionA, actionB)\n",
        "            \n",
        "            next_state = env.get_state()\n",
        "            next_stateA1 = np.array([next_state[0], next_state[1], next_state[2], next_state[3], next_state[4], next_state[5], next_state[6], next_state[7], next_state[8], next_state[9], next_state[10], next_state[11]])\n",
        "            next_stateA2 = np.array([next_state[2], next_state[3], next_state[0], next_state[1], next_state[4], next_state[5], next_state[6], next_state[7], next_state[9], next_state[8], next_state[10], next_state[11]])\n",
        "            next_stateB1 = np.array([next_state[4],next_state[5], next_state[6],next_state[7], next_state[0], next_state[1],next_state[2],next_state[3],next_state[10], next_state[11], next_state[8],next_state[9]])\n",
        "            next_stateB2 = np.array([next_state[6],next_state[7], next_state[4],next_state[5], next_state[0], next_state[1],next_state[2],next_state[3], next_state[11], next_state[10], next_state[8],next_state[9]])\n",
        "\n",
        "            # next_state, reward, done = env.step(env.actions[action])\n",
        "            # total_reward += reward\n",
        "            agentA.remember(stateA1, actionA1, agentsA_reward[0], agentsA_deliver[0], next_stateA1)\n",
        "            agentA.remember(stateA2, actionA2, agentsA_reward[1], agentsA_deliver[1], next_stateA2)\n",
        "            agentB.remember(stateB1, actionB1, agentsB_reward[0], agentsB_deliver[0], next_stateB1)\n",
        "            agentB.remember(stateB2, actionB2, agentsB_reward[1], agentsB_deliver[1], next_stateB2)\n",
        "\n",
        "            state = next_state\n",
        "            steps_in_episode += 1 \n",
        "\n",
        "            # Training A\n",
        "            if len(agentA.memory) > agentA.batch_size:\n",
        "                minibatch_indices = np.random.choice(len(agentA.memory), agentA.batch_size, replace=False)\n",
        "                minibatch = [agentA.memory[i] for i in minibatch_indices]\n",
        "                states_batch, actions_batch, targets_batch = agentA.process_minibatch(minibatch)\n",
        "                loss = agentA.train_one_step(states_batch, actions_batch, targets_batch)\n",
        "                episode_losses.append(loss)\n",
        "\n",
        "\n",
        "            # Training B\n",
        "            if len(agentB.memory) > agentB.batch_size:\n",
        "                minibatch_indices = np.random.choice(len(agentB.memory), agentB.batch_size, replace=False)\n",
        "                minibatch = [agentB.memory[i] for i in minibatch_indices]\n",
        "                states_batch, actions_batch, targets_batch = agentB.process_minibatch(minibatch)\n",
        "                loss = agentB.train_one_step(states_batch, actions_batch, targets_batch)\n",
        "                episode_losses.append(loss)\n",
        "        \n",
        "        #Cleanup step\n",
        "\n",
        "        agentA.epsilon = max(agentA.epsilon * agentA.epsilon_decay_factor, agentA.epsilon_min)\n",
        "        agentB.epsilon = max(agentB.epsilon * agentB.epsilon_decay_factor, agentB.epsilon_min)\n",
        "\n",
        "        if len(episode_losses) > 0:\n",
        "            mean_loss = sum(episode_losses)/len(episode_losses)\n",
        "        else:\n",
        "            mean_loss = \"n/a\"\n",
        "\n",
        "        agentA.steps_since_copy += 1\n",
        "        agentA.update_target()\n",
        "        agentB.steps_since_copy += 1\n",
        "        agentB.update_target()\n",
        "\n",
        "        # print(\"state\", state)\n",
        "        # print(f\"Episode: {episode + 1}, Total Reward: {total_reward}, Epsilon: {agent.epsilon} | Loss: {mean_loss}\")\n",
        "        # print(f\"Collected: {env.collected} | Terminated: {terminated} | Done: {done}\")\n",
        "        \n",
        "        # distance_history.append(steps_in_episode - steps_to_opt + 1)\n",
        "        loss_history.append(mean_loss)\n",
        "        empirical_reward_history.append(total_reward)\n",
        "    return loss_history, empirical_reward_history #, distance_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# if __name__ == \"__main__\":\n",
        "env = Environment(5, verbose=3) # Create the environment\n",
        "\n",
        "gamma = 0.8; learning_rate = 0.01; epsilon_decay_factor = 0.97; min_epsilon=0.05; replay_buffer_size=10000; batch_size=300; network_copy_frequency=15\n",
        "episodes = 400\n",
        "\n",
        "agentA = AgentA(gamma=gamma, learning_rate=learning_rate, epsilon_decay_factor=epsilon_decay_factor,\n",
        "                 min_epsilon=min_epsilon, replay_buffer_size=replay_buffer_size,\n",
        "                 batch_size=batch_size, network_copy_frequency=network_copy_frequency)\n",
        "agentB = AgentB(gamma=gamma, learning_rate=learning_rate, epsilon_decay_factor=epsilon_decay_factor,\n",
        "                 min_epsilon=min_epsilon, replay_buffer_size=replay_buffer_size,\n",
        "                 batch_size=batch_size, network_copy_frequency=network_copy_frequency)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Run the below cell for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "losses, rewards = train_function(env, agentA, agentB, episodes=episodes) #, distance_history\n",
        "np.save(\"losses.npy\", losses, allow_pickle=False)\n",
        "np.save(\"rewards.npy\", rewards, allow_pickle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Run the below cell for visualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A1 pickup\n",
            "A0 pickup\n",
            "A1 B0 handover\n",
            "\tagentsA_reward 991 7\n",
            "\tagentsB_reward 975 7\n",
            "A1 pickup\n",
            "A0 B1 handover\n",
            "\tagentsA_reward 984 11\n",
            "\tagentsB_reward 984 11\n",
            "B0 deliver\n",
            "A0 pickup\n",
            "A1 B0 handover\n",
            "\tagentsA_reward 975 8\n",
            "\tagentsB_reward 991 8\n",
            "B1 deliver\n",
            "B0 deliver\n",
            "A1 pickup\n",
            "A0 B0 handover\n",
            "\tagentsA_reward 984 13\n",
            "\tagentsB_reward 984 9\n",
            "A1 B1 handover\n",
            "\tagentsA_reward 984 9\n",
            "\tagentsB_reward 984 13\n",
            "A0 pickup\n",
            "A1 pickup\n",
            "B0 deliver\n",
            "B1 deliver\n",
            "A0 B0 handover\n",
            "\tagentsA_reward 975 9\n",
            "\tagentsB_reward 991 9\n",
            "A1 B1 handover\n",
            "\tagentsA_reward 975 9\n",
            "\tagentsB_reward 991 9\n",
            "B0 deliver\n",
            "B1 deliver\n",
            "\u001b[1m total packages_delivered 7 \u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# check visualization\n",
        "agentA.load_model_parameters(path = 'logs/target_params_A_375_onehot.pt')\n",
        "agentB.load_model_parameters(path = 'logs/target_params_B_375_onehot.pt')\n",
        "run_visualisation(agentA, agentB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "round_losses = [round(loss, 2) for loss in losses if loss != \"n/a\"]\n",
        "plt.plot(round_losses, label='Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "round_rewards = [round(r, 2) for r in rewards if r != \"n/a\"]\n",
        "plt.plot(round_rewards, label='Rewards')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Reward')\n",
        "plt.title('Empirical Reward')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
