{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2 - Deep Q\n",
    "\n",
    "**Team: Wintergreen Systems**\n",
    "\n",
    "- Parisa\n",
    "- Sudha\n",
    "- Saniya\n",
    "- Elizabeth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "import numpy as np\n",
    "import itertools\n",
    "import random as rd\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "matplotlib.use(\"TkAgg\")\n",
    "BOLD = \"\\033[1m\"\n",
    "RESET = \"\\033[0m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "Purpose of this branch is to test convergence properties of the network using one-hot-vectors for the state space instead of 0-3, as the OHV format may be easier for the network to operate on. Due to the need to operate on zero vectors we will\n",
    "\n",
    "- add a small noise function gamma to each OHV\n",
    "- use He initialisation for weight initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Environment Class\n",
    "\n",
    "The `Environment` class has been used to initialize the grid-based environment in which the package delivery agent operates.\n",
    "\n",
    "**Methods:**\n",
    "\n",
    " - `reset_env`: Resets the environment to an initial state and generates random locations for the package , destination and agent.\n",
    " - `coordinates_to_noisy_obv(coordinate:int)`: Converts the coordinates to noisy one-hot vector representation.\n",
    " - `setup_grid()`: Initializes the grid with package, destination and agent locations.\n",
    " - `get_state()`: Retrieves the current state of the environment.\n",
    " - `move_agent(action)`: Moves the agent in the specified direction and updates the environment.\n",
    " - `get_grid()`: Retrieves the current grid representation of the environment. The function reruens a list of lists representing the grid.\n",
    " - `plot_grid(snapshot, ax=None)`: This is the implementation of the visualization for the grid state throughout the journey of the agent from its start location to end destination. Visualization is implemented using Matplotlib. It displays images representing the agent, package, and destination locations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    \n",
    "    #fields:\n",
    "    #a grid of size defined by parameter\n",
    "    #b, a tuple repsenting the delivery location\n",
    "    #a, a tuple representing the package location\n",
    "    #size, an int representing the horizontal and vertical dimensions of the grid\n",
    "    def __init__(self, size:int):\n",
    "        self.size = size\n",
    "        self.actions = ['north', 'south', 'west', 'east']\n",
    "        self.reward_pickup = 50\n",
    "        self.reward_deliver = 200\n",
    "        self.reward_move = -1.5\n",
    "        self.reset_env()\n",
    "    \n",
    "    def reset_env(self):\n",
    "        combinations = list(itertools.product(list(range(self.size)), repeat=2))\n",
    "        rd.shuffle(combinations)\n",
    "        self.a, self.b, self.agent_coords = combinations[0:3]\n",
    "        self.collected = 0\n",
    "        self.done = False\n",
    "        self.terminated = False\n",
    "        self.test_length = 0 #an additional parameter to determine how long the environment has been active for\n",
    "        self.setup_grid()\n",
    "\n",
    "    # def coordinates_to_noisy_ohv(self, coordinate:int):\n",
    "    #     ohv = np.zeros(4)\n",
    "    #     noise = np.random.normal(-1e-5, 1e-5, ohv.shape)\n",
    "    #     ohv[coordinate] = 1\n",
    "    #     ohv = ohv + noise\n",
    "    #     return ohv\n",
    "    \n",
    "    def get_optimal_distance(self):\n",
    "        return abs(self.agent_coords[0]-self.a[0]) + abs(self.agent_coords[0]-self.a[1]) + abs(self.a[0]-self.b[0]) + abs(self.a[1]-self.b[1])\n",
    "\n",
    "\n",
    "    #method to set up the original grid including a location\n",
    "    def setup_grid(self):\n",
    "        grid = np.zeros((self.size, self.size))\n",
    "        self.grid = grid\n",
    "        grid[self.a] = 1\n",
    "        grid[self.b] = 2\n",
    "        grid[self.agent_coords] = -1\n",
    "\n",
    "    def get_state(self):\n",
    "        #This method unpacks the coordinates into a tuple which can be used to index the qmatrix or for easy incorporation to the Deep Q Network\n",
    "        # coords = [self.agent_coords[0], self.agent_coords[1], self.a[0], self.a[1], self.b[0], self.b[1]]\n",
    "        # coords = [self.coordinates_to_noisy_ohv(c) for c in coords]\n",
    "        # coords.append(np.array(self.collected))\n",
    "        # return np.hstack(coords)\n",
    "        # return np.ndarray.flatten(np.array(coords))\n",
    "        return np.array([self.agent_coords[0], self.agent_coords[1], self.collected, self.a[0], self.a[1], self.b[0], self.b[1]])\n",
    "        # return np.array([self.agent_coords[0]-self.a[0], self.agent_coords[1]-self.a[1],\n",
    "        #                   self.collected,\n",
    "        #                     self.agent_coords[0]-self.b[0], self.agent_coords[1]-self.b[1]])\n",
    "    \n",
    "    #Method which updates the location of the agent on the grid. Currently just zeroes whatever it landed on - can include other logic instead\n",
    "    def move_agent(self, action):\n",
    "        #For agent move, note that 0 = up, 1=down, 2=left, 3=right\n",
    "        self.test_length += 1\n",
    "\n",
    "        #Assign new coordinates for agent to exist at\n",
    "        new_x, new_y = self.agent_coords\n",
    "        if action == 'north' and self.agent_coords[1] > 0:\n",
    "            new_y -= 1\n",
    "        elif action == 'south' and self.agent_coords[1] < self.size - 1:\n",
    "            new_y += 1\n",
    "        elif action == 'west' and self.agent_coords[0] > 0:\n",
    "            new_x -= 1\n",
    "        elif action == 'east' and self.agent_coords[0] < self.size - 1:\n",
    "             new_x += 1\n",
    "        \n",
    "        #Update the grid based on new agent coordinates\n",
    "        self.grid[self.agent_coords] = 0\n",
    "        self.agent_coords = (new_x, new_y)\n",
    "        self.grid[self.agent_coords] = -1\n",
    "\n",
    "        #Handle logic based on new agent location\n",
    "        if self.collected == 0:\n",
    "            if self.agent_coords == self.a:\n",
    "                self.collected = 1\n",
    "                reward = self.reward_pickup\n",
    "            else:\n",
    "                reward = self.reward_move\n",
    "        \n",
    "        else:\n",
    "            if self.agent_coords == self.b:\n",
    "                self.done = True\n",
    "                reward = self.reward_deliver\n",
    "            else:\n",
    "                reward = self.reward_move\n",
    "\n",
    "                #This code was considered test code which could be useful in exiting early\n",
    "        if self.test_length > 70:\n",
    "            self.terminated = True\n",
    "        return reward, self.done, self.terminated\n",
    "\n",
    "    def get_grid(self):\n",
    "        return self.grid.tolist()\n",
    "\n",
    "    #method to set up the original grid including a location\n",
    "    def setup_grid(self):\n",
    "        self.grid = np.zeros((self.size, self.size))\n",
    "            \n",
    "        # Place the \"a\" and \"b\" locations on the grid\n",
    "        if self.a is None:\n",
    "            x, y = rd.randint(0, self.size - 1), rd.randint(0, self.size - 1)\n",
    "            while (x, y) == self.b:\n",
    "                x, y = rd.randint(0, self.size - 1), rd.randint(0, self.size - 1)\n",
    "            self.a = (x, y)  # A represented by 1\n",
    "        else:\n",
    "            x, y = self.a\n",
    "        self.grid[x, y] = 1  # Place \"a\" on the grid\n",
    "\n",
    "        if self.b is None:\n",
    "            x, y = rd.randint(0, self.size - 1), rd.randint(0, self.size - 1)\n",
    "            while (x, y) == self.a:\n",
    "                x, y = rd.randint(0, self.size - 1), rd.randint(0, self.size - 1)\n",
    "            self.b = (x, y)  # B represented by 2\n",
    "        else:\n",
    "            x, y = self.b\n",
    "        self.grid[x, y] = 2  # Place \"b\" on the grid\n",
    "\n",
    "        # Place the \"home\" location (0) on the grid\n",
    "        x, y = self.agent_coords\n",
    "        self.grid[x, y] = 0\n",
    "    \n",
    "    def plot_grid(self, snapshot, ax=None):\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.set_facecolor('white')\n",
    "        else:\n",
    "            ax.clear()\n",
    "        \n",
    "        # Plot the grid\n",
    "        ax.imshow(np.array([[0]]), cmap=\"bone\", extent=[0, self.size, 0, self.size])\n",
    "\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                cell_value = snapshot[i][j]\n",
    "                if cell_value == -1:\n",
    "                    # Display agent image in the cell\n",
    "                    imagebox = OffsetImage(agent_img, zoom=0.08)\n",
    "                    ab = AnnotationBbox(imagebox, (j + 0.5, self.size - i - 0.5), frameon=False)\n",
    "                    ax.add_artist(ab)\n",
    "                elif cell_value == 1:\n",
    "                    # Display package image in the cell\n",
    "                    imagebox = OffsetImage(package_img, zoom=0.03)\n",
    "                    ab = AnnotationBbox(imagebox, (j + 0.5, self.size - i - 0.5), frameon=False)\n",
    "                    ax.add_artist(ab)\n",
    "                elif cell_value == 2:\n",
    "                    # Display destination image in the cell\n",
    "                    imagebox = OffsetImage(destinationB_img, zoom=0.05)\n",
    "                    ab = AnnotationBbox(imagebox, (j + 0.5, self.size - i - 0.5), frameon=False)\n",
    "                    ax.add_artist(ab)\n",
    "                else:\n",
    "                    ax.text(j + 0.5, self.size - i - 0.5, self.grid[i, j], ha='center', va='center', fontsize=20, color='black')\n",
    "        \n",
    "        # Set axis properties\n",
    "        ax.set_xlim(0, self.size)\n",
    "        ax.set_ylim(0, self.size)\n",
    "        ax.set_xticks(np.arange(self.size) + 1)\n",
    "        ax.set_yticks(np.arange(self.size) + 1)\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.grid(True, linewidth=2, color='white')\n",
    "        \n",
    "        # Set title\n",
    "        ax.set_title(\"Package Delivery Agent\")\n",
    "        \n",
    "        # Show the plot\n",
    "        # plt.show()\n",
    "        return ax\n",
    "agent_img = plt.imread('agent.jpg')\n",
    "package_img = plt.imread('package.jpg')\n",
    "destinationB_img = plt.imread('destinationB.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### TransportAgent Class\n",
    "\n",
    " The `TransportAgent` class represents an agent for a transportation task using reinforcemnet learning with neural networks.\n",
    "\n",
    " **Methods**\n",
    "- `prepare_torch(self)`: This method initializes the agent's neural network architecture. It defines a multi-layered neural network with ReLU activation functions and prepares a target neural network for stability in training. The network architecture consists of an input layer with `statespace_size` neurons, followed by two hidden layers with 150 and 100 neurons, respectively, and an output layer with 4 neurons representing Q-values for each action.\n",
    "\n",
    "- `update_target(self)`: This method updates the target neural network with the main neural network's weights. It helps stabilize the training process by reducing the target network's \"chasing\" of the main network.\n",
    "\n",
    "- `remember(self, state, action, reward, next_state, done)`: Stores the agent's experiences in the memory buffer. Experiences are represented as tuples of `(state, action, reward, next_state, done)` and are added to the memory. If the memory buffer exceeds the specified `replay_buffer_size`, the oldest experience is removed.\n",
    "\n",
    "- `get_qvals(self, state)`: Computes Q-values for a given state. It takes a state as input, converts it to a PyTorch tensor, and computes the Q-values using the agent's main neural network.\n",
    "\n",
    "- `get_maxQ(self, s)`: Returns the maximum Q-value for a given state. It calculates the maximum Q-value using the target neural network.\n",
    "\n",
    "- `act(self, state)`: Selects an action based on epsilon-greedy policy. If a random number is less than the current epsilon value, a random action is chosen; otherwise, the action with the highest Q-value is selected.\n",
    "    \n",
    "- `process_minibatch(self, minibatch)`: Prepares the minibatch of experiences for training. It extracts states, actions, and target Q-values from the provided minibatch of experiences.\n",
    "\n",
    "- `train_one_step(self, states, actions, targets)`: Performs one step of training using a minibatch of experiences. It computes the loss between predicted and target Q-values and updates the agent's neural network's weights using backpropagation. Gradient clipping is applied to prevent exploding gradients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the TransportAgent agent\n",
    "#new statespace size should be 25 - this represents six 4-size one-hot-vectors, plus a collected flag\n",
    "class TransportAgent:\n",
    "    def __init__(self, statespace_size=7, gamma=0.99, learning_rate=0.01, start_epsilon=1.0,\n",
    "                 epsilon_decay_factor=0.997, min_epsilon=0.1, replay_buffer_size=1000,\n",
    "                 batch_size=200, network_copy_frequency=500):\n",
    "        self.statespace_size = statespace_size\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model2 = self.prepare_torch()\n",
    "        self.memory = []  # Using a list instead of a deque\n",
    "        self.epsilon = start_epsilon\n",
    "        self.epsilon_min = min_epsilon\n",
    "        self.epsilon_decay_factor = epsilon_decay_factor\n",
    "        self.replay_buffer_size = replay_buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.network_copy_frequency = network_copy_frequency\n",
    "        self.steps_since_copy = 0  # Counter for network copy\n",
    "\n",
    "    def prepare_torch(self):\n",
    "        l1 = self.statespace_size\n",
    "        l2 = 150\n",
    "        l3 = 100\n",
    "        l4 = 4\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(l1, l2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(l2, l3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(l3, l4)\n",
    "        )\n",
    "\n",
    "        model2 = copy.deepcopy(self.model)\n",
    "        model2.load_state_dict(self.model.state_dict())\n",
    "        # self.loss_fn = torch.nn.HuberLoss()\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        return model2\n",
    "\n",
    "    def update_target(self):\n",
    "        if self.steps_since_copy >= self.network_copy_frequency:\n",
    "            self.model2.load_state_dict(self.model.state_dict())\n",
    "            self.steps_since_copy = 0\n",
    "            # print(BOLD + \"Target model updated\" + RESET)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        while len(self.memory) > self.replay_buffer_size:\n",
    "            self.memory.pop(0)  # Remove the oldest experience\n",
    "\n",
    "    def get_qvals(self, state):\n",
    "        state1 = torch.from_numpy(state).float()\n",
    "        qvals_torch = self.model(state1)\n",
    "        qvals = qvals_torch.data.numpy()\n",
    "        return qvals\n",
    "\n",
    "    def get_maxQ(self, s):\n",
    "        return torch.max(self.model2(torch.from_numpy(s).float())).float()\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(range(4))  # Random action\n",
    "        q_values = self.get_qvals(state)\n",
    "        return np.argmax(q_values)  # Greedy action\n",
    "\n",
    "    def process_minibatch(self, minibatch):\n",
    "        states = []\n",
    "        actions = []\n",
    "        targets = []\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            q_values = self.get_qvals(state)\n",
    "            if done:\n",
    "                q_values[action] = reward\n",
    "            else:\n",
    "                q_values[action] = reward + self.gamma * self.get_maxQ(next_state)\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            targets.append(q_values[action])\n",
    "        return np.array(states), np.array(actions), np.array(targets) #this is returning a thruple of state transitions?\n",
    "\n",
    "    def train_one_step(self, states, actions, targets):\n",
    "        state1_batch = torch.from_numpy(states).float()\n",
    "        # state1_batch = torch.Tensor([torch.from_numpy(s).float() for s in states])\n",
    "        action_batch = torch.Tensor(actions)\n",
    "        Q1 = self.model(state1_batch)\n",
    "        X = Q1.gather(dim=1, index=action_batch.long().unsqueeze(dim=1)).squeeze()\n",
    "        Y = torch.tensor(targets)\n",
    "        # loss = torch.nn.HuberLoss()(X, Y)\n",
    "        loss = self.loss_fn(X, Y)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "    \n",
    "    def save_model_parameters(self, destination):\n",
    "        torch.save(self.model2.state_dict(), destination)\n",
    "\n",
    "    def load_model_parameters(self, path):\n",
    "        self.model.load_state_dict(torch.load(path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run visualization:\n",
    "Runs a visualization of the trained agent's movement in the environment.\n",
    "\n",
    "The `run_visualisation` function runs a visualization of package delivery episodes using a trained transportation agent. It simulates the agent's actions in the environment and displays the evolving grid at each step of the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_visualisation(agent, max_steps=20, size=4):\n",
    "    #Assumes a fully trained transport agent. \n",
    "    #Target network should either have been trained in the preceeding step, or loaded from a pickled pytorch weights file\n",
    "    env = Environment(size)\n",
    "    # Create a figure and axis outside the loop\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_facecolor('white')\n",
    "\n",
    "    # Initialize the plot once with the initial grid\n",
    "    m = ax.imshow(env.grid, cmap=\"bone\", extent=[0, env.size, 0, env.size])\n",
    "    snapshots = []\n",
    "    for step in range(max_steps):\n",
    "        state = env.get_state()\n",
    "        q_values = agent.get_qvals(state)\n",
    "        action = np.argmax(q_values)\n",
    "        _, done, _ = env.move_agent(env.actions[action])\n",
    "        snapshots.append(env.get_grid())\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    for s in snapshots:\n",
    "        env.plot_grid(s, ax)\n",
    "        # Redraw the plot\n",
    "        fig.canvas.draw()\n",
    "        plt.pause(0.2)  # Add a short pause for visualization\n",
    "    # plt.pause(1.5)\n",
    "    plt.close()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function\n",
    "\n",
    "The `train_function` function trains a the agent in the environment with the neural network that we have defined for a set of episode iterations. In this process the target network will be updated. This function encapsulates the core training logic for our agent.\n",
    "\n",
    "**Parameters:**\n",
    "\n",
    "- `env` (Environment): The environment in which the agent operates in.\n",
    "- `agent` (Agent): The learning agent that is being trained.\n",
    "- `episodes` (int, optional): The number of episodes to train the agent (default is 100).\n",
    "\n",
    "**Returns:**\n",
    "\n",
    "A tuple containing two lists:\n",
    "- `loss_history` (list): A list of mean loss values for each episode during training.\n",
    "- `empirical_reward_history` (list): A list of total rewards obtained for each episode during training.\n",
    "\n",
    "**Function Explanation:**\n",
    "\n",
    "- The training loop to iterate over a fixed number of episodes. For each episode:\n",
    "    - The environment is reset to the initial state.\n",
    "    - the agent takes actions within the environment until one of the termination conditions (`done` or `terminated`) is met. The key steps include:\n",
    "      - Selecting an action using the agent's policy (`agent.act(state)`).\n",
    "      - Executing the selected action in the environment and observing the resulting reward, done/termination status.\n",
    "      - Storing the transition (state, action, reward, next_state, done) in the agent's memory (`agent.remember()`).\n",
    "      - Accumulating the reward (`total_reward`) for the episode.\n",
    "\n",
    "- After each action in the episode, the agent performs training using experience stored in its memory:\n",
    "    - A check is made to ensure that there are enough experiences in the memory for training (`len(agent.memory) > agent.batch_size`).\n",
    "    - If sufficient experiences are available, a minibatch is randomly sampled from the memory.\n",
    "    - The minibatch is used to compute the loss and update the agent's neural network using the `agent.train_one_step()` method.\n",
    "\n",
    "- After the episode completes, a cleanup step is performed to decay the agent's exploration rate (`epsilon`) and calculate the mean loss for that episode. And  updates the target neural network with the main neural network's weights\n",
    "\n",
    "Finally, the function returns `loss_history` and `empirical_reward_history`, which is the mean loss and the total rewards for episodes providing insights into the training progress and performance of the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_function(env, agent, episodes=100):\n",
    "    loss_history = []\n",
    "    empirical_reward_history = []\n",
    "    distance_history =[]\n",
    "    for episode in range(episodes):\n",
    "        env.reset_env()\n",
    "        state = env.get_state()  # Reset the environment and get the initial state\n",
    "        steps_to_opt = env.get_optimal_distance()\n",
    "        steps_in_episode = 0\n",
    "        done = False\n",
    "        terminated = False\n",
    "        total_reward = 0\n",
    "        episode_losses = []\n",
    "        while (not done and not terminated):\n",
    "            # print(\"state\", state)\n",
    "            action = agent.act(state)\n",
    "            reward, done, terminated = env.move_agent(env.actions[action])\n",
    "            next_state = env.get_state()\n",
    "            # next_state, reward, done = env.step(env.actions[action])\n",
    "            total_reward += reward\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            # print(\"agent.memory[-1]\",agent.memory[-1])\n",
    "            state = next_state\n",
    "            steps_in_episode += 1\n",
    "\n",
    "            # Training\n",
    "            if len(agent.memory) > agent.batch_size:\n",
    "                minibatch_indices = np.random.choice(len(agent.memory), agent.batch_size, replace=False)\n",
    "                minibatch = [agent.memory[i] for i in minibatch_indices]\n",
    "                states_batch, actions_batch, targets_batch = agent.process_minibatch(minibatch)\n",
    "                loss = agent.train_one_step(states_batch, actions_batch, targets_batch)\n",
    "                episode_losses.append(loss)\n",
    "        \n",
    "\n",
    "        #Cleanup step\n",
    "\n",
    "        agent.epsilon = max(agent.epsilon * agent.epsilon_decay_factor, agent.epsilon_min)\n",
    "        if len(episode_losses) > 0:\n",
    "            mean_loss = sum(episode_losses)/len(episode_losses)\n",
    "        else:\n",
    "            mean_loss = \"n/a\"\n",
    "        agent.steps_since_copy += 1\n",
    "        agent.update_target()\n",
    "        # print(\"state\", state)\n",
    "        # print(f\"Episode: {episode + 1}, Total Reward: {total_reward}, Epsilon: {agent.epsilon} | Loss: {mean_loss}\")\n",
    "        # print(f\"Collected: {env.collected} | Terminated: {terminated} | Done: {done}\")\n",
    "        \n",
    "        distance_history.append(steps_in_episode - steps_to_opt + 1)\n",
    "        loss_history.append(mean_loss)\n",
    "        empirical_reward_history.append(total_reward)\n",
    "    return loss_history, empirical_reward_history, distance_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, ylabel):\n",
    "    plt.figure()\n",
    "    # Create a line plot\n",
    "    numeric_losses = [float(val) for val in history if isinstance(val, (int, float))]\n",
    "    plt.plot(numeric_losses)\n",
    "\n",
    "    # Add labels and a title\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(ylabel + ' Curve')\n",
    "\n",
    "    # Display the plot (this is necessary to show the plot in most environments)\n",
    "    plt.show()\n",
    "    plt.pause(1.5)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the entire program to intialize an environment of size, agent with hyperparameters required for the training.\n",
    "\n",
    "And carry out the training of agent in our gird env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Total Reward: 217.0, Epsilon: 0.99 | Loss: n/a\n",
      "Collected: 1 | Terminated: False | Done: True\n",
      "Episode: 2, Total Reward: -55.0, Epsilon: 0.9801 | Loss: n/a\n",
      "Collected: 1 | Terminated: True | Done: False\n",
      "Episode: 3, Total Reward: -106.5, Epsilon: 0.9702989999999999 | Loss: n/a\n",
      "Collected: 0 | Terminated: True | Done: False\n",
      "Episode: 4, Total Reward: -55.0, Epsilon: 0.96059601 | Loss: 165.16706910004487\n",
      "Collected: 1 | Terminated: True | Done: False\n",
      "\u001b[1mTarget model updated\u001b[0m\n",
      "Episode: 5, Total Reward: 191.5, Epsilon: 0.9509900498999999 | Loss: 56.87302826672065\n",
      "Collected: 1 | Terminated: False | Done: True\n",
      "Episode: 6, Total Reward: 188.5, Epsilon: 0.9414801494009999 | Loss: 142.88808911345726\n",
      "Collected: 1 | Terminated: False | Done: True\n",
      "Episode: 7, Total Reward: -55.0, Epsilon: 0.9320653479069899 | Loss: 92.14011344103746\n",
      "Collected: 1 | Terminated: True | Done: False\n",
      "Episode: 8, Total Reward: -55.0, Epsilon: 0.92274469442792 | Loss: 26.133459091186523\n",
      "Collected: 1 | Terminated: True | Done: False\n",
      "Episode: 9, Total Reward: 223.0, Epsilon: 0.9135172474836407 | Loss: 17.655465745925902\n",
      "Collected: 1 | Terminated: False | Done: True\n",
      "\u001b[1mTarget model updated\u001b[0m\n",
      "Episode: 10, Total Reward: 238.0, Epsilon: 0.9043820750088043 | Loss: 55.1786413192749\n",
      "Collected: 1 | Terminated: False | Done: True\n",
      "Episode: 11, Total Reward: 172.0, Epsilon: 0.8953382542587163 | Loss: 123.33098411560059\n",
      "Collected: 1 | Terminated: False | Done: True\n",
      "Episode: 12, Total Reward: -55.0, Epsilon: 0.8863848717161291 | Loss: 61.82179268313126\n",
      "Collected: 1 | Terminated: True | Done: False\n",
      "Episode: 13, Total Reward: 176.5, Epsilon: 0.8775210229989678 | Loss: 34.14273652843401\n",
      "Collected: 1 | Terminated: False | Done: True\n",
      "Episode: 14, Total Reward: -55.0, Epsilon: 0.8687458127689781 | Loss: 83.45125648337351\n",
      "Collected: 1 | Terminated: True | Done: False\n",
      "\u001b[1mTarget model updated\u001b[0m\n",
      "Episode: 15, Total Reward: 169.0, Epsilon: 0.8600583546412883 | Loss: 45.26934215000698\n",
      "Collected: 1 | Terminated: False | Done: True\n",
      "Episode: 16, Total Reward: 203.5, Epsilon: 0.8514577710948754 | Loss: 170.63100468028676\n",
      "Collected: 1 | Terminated: False | Done: True\n",
      "Episode: 17, Total Reward: 191.5, Epsilon: 0.8429431933839266 | Loss: 76.58689368643412\n",
      "Collected: 1 | Terminated: False | Done: True\n",
      "Episode: 18, Total Reward: 181.0, Epsilon: 0.8345137614500874 | Loss: 70.82903989156087\n",
      "Collected: 1 | Terminated: False | Done: True\n",
      "Episode: 19, Total Reward: 184.0, Epsilon: 0.8261686238355865 | Loss: 61.4765105040177\n",
      "Collected: 1 | Terminated: False | Done: True\n",
      "\u001b[1mTarget model updated\u001b[0m\n",
      "Episode: 20, Total Reward: 170.5, Epsilon: 0.8179069375972307 | Loss: 83.47714268077503\n",
      "Collected: 1 | Terminated: False | Done: True\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "env = Environment(4) # Create the environment\n",
    "agent = TransportAgent(gamma=0.65, learning_rate=0.01, start_epsilon=1.0, epsilon_decay_factor=0.99,\n",
    "                        min_epsilon=0.1, replay_buffer_size=5000, batch_size=200, network_copy_frequency=5)\n",
    "losses, rewards, distance_history = train_function(env, agent, episodes=250)\n",
    "agent.save_model_parameters(\"logs/target_params.pt\")\n",
    "np.save(\"losses.npy\", losses, allow_pickle=False)\n",
    "np.save(\"rewards.npy\", rewards, allow_pickle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check visualization\n",
    "run_visualisation(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check loss and rewards across episodes\n",
    "plot_history(losses, \"Loss\")\n",
    "plt.close()\n",
    "plot_history(rewards, \"Rewards\")\n",
    "plt.close()\n",
    "plot_history(distance_history, \"Distance to Optimal Solution\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
