{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1\n",
    "##### Team: Wintergreen Systems\n",
    "\n",
    "- Parisa\n",
    "- Sudha\n",
    "- Saniya\n",
    "- Elizabeth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.25.2.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random as rd\n",
    "import time\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "# from matplotlib import widgets\n",
    "import matplotlib\n",
    "from matplotlib import cm as cm\n",
    "import scipy as SP\n",
    "from PIL import Image\n",
    "import threading\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "import plotly\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, iplot\n",
    "import plotly.graph_objs as go\n",
    "from scipy import stats \n",
    "from IPython.display import clear_output\n",
    "plotly.offline.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.use('TkAgg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Class\n",
    "\n",
    "The `Environment` class has been used to initialize the grid-based environment in which the package delivery agent operates. It is initialized with a grid size, random agent starting location and package location 'A'. The environment has a fixed location for destination 'B', which is the bottom-right corner of the grid.\n",
    "\n",
    "**Methods:**\n",
    "\n",
    "- `set_agent_start(agent)`: Sets the agent's starting location in the environment.\n",
    "- `move_agent(agent_move)`: Updates the agent's location on the grid in respect to the movement action taken by the agent.\n",
    "- `get_grid()`: This will give the current state of the grid.\n",
    "- `plot_grid(snapshot, ax=None)`: This is the implementation of the visualization for the grid state throughout the journey of the agent from its start location to end destination. Visualization is implemented using Matplotlib. It displays images representing the agent, package, and destination locations.\n",
    "\n",
    "### Reward Structure Explanation:\n",
    "\n",
    "In the context of the package delivery problem, a reward structure is used to provide feedback to the agent's decisions and actions. This feedback is used as a guide to urge the agent towards making optimal decisions to achieve the goal of delivering the package efficiently.\n",
    "\n",
    "- `reward_pickup`: A positive reward given to the agent when it picks up the package. It encourages the agent to collect the package.\n",
    "- `reward_deliver`: A positive reward given to the agent when it successfully delivers the package to the destination. It incentivizes the agent to complete the delivery.\n",
    "- `reward_move`: A negative reward given to the agent for each move it makes. If the move is not helping it reach the package or deliver it, the reward will be negative. This will ensure that the agent minimizes unnecessary movements.\n",
    "\n",
    "These reward values are used to help the agent learn via reinforcement learning, over the period of training the agent will learn the best actions and decisions to make that will maximize its cumulative reward over time. It will learn to pick up the package, deliver it to the destination, and minimize the number of moves required to successfully complete this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    \n",
    "    #fields:\n",
    "    #a grid of size defined by parameter\n",
    "    #b, a tuple repsenting the delivery location\n",
    "    #a, a tuple representing the package location\n",
    "    #size, an int representing the horizontal and vertical dimensions of the grid\n",
    "    def __init__(self, size:int, a = None):\n",
    "        self.size = size\n",
    "        self.b = (size-1, size-1)\n",
    "        self.grid, self.a = self.setup_grid(size, a)\n",
    "        self.reward_pickup = 100\n",
    "        self.reward_deliver = 100\n",
    "        self.reward_move = -1\n",
    "\n",
    "    #method to set up the original grid including a location\n",
    "    def setup_grid(self, size, a):\n",
    "        grid = np.zeros((size,size))\n",
    "        if a is None:\n",
    "            x, y = rd.randint(0, size-1), rd.randint(0, size-1)\n",
    "            while (x,y) == self.b:\n",
    "                x, y = rd.randint(0, size-1), rd.randint(0, size-1)\n",
    "            grid[x,y] = 1  #A represented by 1\n",
    "        else:\n",
    "            x,y=a\n",
    "            grid[x,y] = 1\n",
    "        grid[size-1,size-1] = 2  #B represented by 2\n",
    "        return grid, (x,y)\n",
    "    \n",
    "    #method to add start location of agent to grid as well\n",
    "    def set_agent_start(self, agent):\n",
    "        self.agent_coords = (agent.x, agent.y)\n",
    "    \n",
    "    #Method which updates the location of the agent on the grid. Currently just zeroes whatever it landed on - can include other logic instead\n",
    "    def move_agent(self, agent_move):\n",
    "        x, y = self.agent_coords\n",
    "        self.grid[x,y] = 0\n",
    "        self.agent_coords = agent_move[0] , agent_move[1]\n",
    "        self.grid[agent_move[0] , agent_move[1]] = -1\n",
    "\n",
    "    def get_grid(self):\n",
    "        return self.grid.tolist()\n",
    "\n",
    "    def plot_grid(self, snapshot, ax=None):\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.set_facecolor('white')\n",
    "        else:\n",
    "            ax.clear()\n",
    "        \n",
    "        # Plot the grid\n",
    "        ax.imshow(np.array([[0]]), cmap=\"bone\", extent=[0, self.size, 0, self.size])\n",
    "\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                cell_value = snapshot[i][j]\n",
    "                if cell_value == -1:\n",
    "                    # Display agent image in the cell\n",
    "                    imagebox = OffsetImage(agent_img, zoom=0.08)\n",
    "                    ab = AnnotationBbox(imagebox, (j + 0.5, self.size - i - 0.5), frameon=False)\n",
    "                    ax.add_artist(ab)\n",
    "                elif cell_value == 1:\n",
    "                    # Display package image in the cell\n",
    "                    imagebox = OffsetImage(package_img, zoom=0.03)\n",
    "                    ab = AnnotationBbox(imagebox, (j + 0.5, self.size - i - 0.5), frameon=False)\n",
    "                    ax.add_artist(ab)\n",
    "                elif cell_value == 2:\n",
    "                    # Display destination image in the cell\n",
    "                    imagebox = OffsetImage(destinationB_img, zoom=0.05)\n",
    "                    ab = AnnotationBbox(imagebox, (j + 0.5, self.size - i - 0.5), frameon=False)\n",
    "                    ax.add_artist(ab)\n",
    "                else:\n",
    "                    ax.text(j + 0.5, self.size - i - 0.5, self.grid[i, j], ha='center', va='center', fontsize=20, color='black')\n",
    "        \n",
    "        # Set axis properties\n",
    "        ax.set_xlim(0, self.size)\n",
    "        ax.set_ylim(0, self.size)\n",
    "        ax.set_xticks(np.arange(self.size) + 1)\n",
    "        ax.set_yticks(np.arange(self.size) + 1)\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.grid(True, linewidth=2, color='white')\n",
    "        \n",
    "        # Set title\n",
    "        ax.set_title(\"Package Delivery Agent\")\n",
    "        \n",
    "        # Show the plot\n",
    "        #plt.show()\n",
    "        return ax\n",
    "\n",
    "agent_img = plt.imread('agent.jpg')\n",
    "package_img = plt.imread('package.jpg')\n",
    "destinationB_img = plt.imread('destinationB.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Class\n",
    "The `Agent` class is used to intialize the agent that interacts with the environment and learns to make decisions.\n",
    "\n",
    "**Methods:**\n",
    "- `choose_best_option(qmat)`: Uses the q-matrix and determines the best action based on the greedy policy. It accordingly returns which action should be taken to the agent.\n",
    "- `move(direction)`: makes the agent move by updating the position based on the chosen action. The new state and associated reward is updated.\n",
    "- `reset_agent()`: resets the agent's state to a random position within the environment.\n",
    "\n",
    "### Hypers Class\n",
    "The Hypers class is defined to store the hyperparameters used in reinforcement learning process. They will influence the agent's learning behavior & exploration strategy. The hyperparameters will determine how the agent explores the environment, learns from experiences, and makes updated to take informed decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#An agent class. Currently barebones\n",
    "class Agent():\n",
    "    def __init__(self, environ, opts):\n",
    "        self.environ = environ\n",
    "        self.reset_agent()\n",
    "        self.opts = opts\n",
    "        \n",
    "\n",
    "    # choose the best action based on the greedy policy on Q-values\n",
    "    # filed qmat = Q-matrix of size n*n*options where n=grid_size, options=actions\n",
    "    def choose_best_option(self, qmat):\n",
    "        state_qs = qmat[self.x, self.y, self.collected, self.environ.a[0], self.environ.a[1]]\n",
    "        action = np.argmax(state_qs) # choose action based on greedy policy where 0=up, 1=down, 2=left, 3=right\n",
    "        return action\n",
    "\n",
    "    def move(self, direction):\n",
    "\n",
    "        if direction == self.opts[0]:\n",
    "            if self.x-1 >= 0:\n",
    "                self.x = self.x-1\n",
    "        if direction == self.opts[1]:\n",
    "            if self.x+1 < self.environ.size:\n",
    "                self.x = self.x+1\n",
    "        if direction == self.opts[2]:\n",
    "            if self.y-1 >= 0:\n",
    "                self.y = self.y-1\n",
    "        if direction == self.opts[3]:\n",
    "            if self.y+1 < self.environ.size:\n",
    "                self.y=self.y+1\n",
    "        \n",
    "        if self.collected == 0:\n",
    "            if (self.x, self.y) == self.environ.a:\n",
    "                self.collected = 1\n",
    "                reward = self.environ.reward_pickup\n",
    "            else:\n",
    "                reward = self.environ.reward_move\n",
    "        \n",
    "        else:\n",
    "            if (self.x, self.y) == self.environ.b:\n",
    "                self.end_episode = 1\n",
    "                reward = self.environ.reward_deliver\n",
    "            else:\n",
    "                reward = self.environ.reward_move\n",
    "\n",
    "        return (self.x, self.y, self.collected), reward, self.end_episode\n",
    "    \n",
    "    def reset_agent(self):\n",
    "        self.x, self.y = rd.randint(0, self.environ.size-1), rd.randint(0, self.environ.size-1)\n",
    "        while ((self.x, self.y) == (self.environ.a or self.environ.b)):\n",
    "            self.x, self.y = rd.randint(0, self.environ.size-1), rd.randint(0, self.environ.size-1)\n",
    "        self.collected = 0\n",
    "        self.end_episode = 0\n",
    "\n",
    "    \n",
    "#Class to store hyperparameters\n",
    "class Hypers():\n",
    "    def __init__(self, eps, gamma, alpha, steps_per_ep, total_iter):\n",
    "        self.eps = eps\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.steps_per_ep = steps_per_ep\n",
    "        self.total_iter = total_iter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning class:\n",
    "The QL class trains the agent usiing Q-Leaarning algorithm.\n",
    "\n",
    "**Attributes:**\n",
    "\n",
    "- `env_size (int)`: Size of the environment grid.\n",
    "\n",
    "- `agent_opts (list)`: List of available agent movement options.\n",
    "\n",
    "- `hypers (object)`: Hyperparameters for training.\n",
    "\n",
    "- `qmat (numpy.ndarray)`: Q-value matrix to store Q-values for state-action pairs.\n",
    "- `distance_history (list)`: List to store the difference between chosen and optimal paths during training.\n",
    "- `rewards (list)`: list storing empirical rewards from each episode\n",
    "- `rolling mean empirical_reward`: list storing rolling means of empirical reward\n",
    "- `ep_length (list)`: list storing episode lengths\n",
    "- `rolling_mean_ep_length (list)`: list storing rolling means of episode length\n",
    "\n",
    "**Methods:**\n",
    "\n",
    "- `__init__(self, env_size, agent_opts, hypers)`:Constructor for QL class.\n",
    "        \n",
    "- `train(self, save_every=500)`:Train the agent using Q-learning algorithm.\n",
    "        \n",
    "- `load_qmat(self, filename, overwrite=False)`: Load a Q-value matrix from a file.\n",
    "        \n",
    "- `save_qmat(self, name)`:Save the current Q-value matrix to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QL:\n",
    "    def __init__(self, env_size, agent_opts, hypers):\n",
    "        self.env_size = env_size\n",
    "        self.agent_opts = agent_opts\n",
    "        self.hypers = hypers\n",
    "        self.qmat = np.zeros([env_size, env_size, 2, env_size, env_size, len(agent_opts)])\n",
    "        self.distance_history = []\n",
    "        self.episode_converged = 0\n",
    "        self.ep_length = []\n",
    "        self.rolling_mean_ep_length = []\n",
    "        self.rewards = []\n",
    "        self.rolling_mean_empirical_reward = []\n",
    "\n",
    "    def train(self, save_every = 500):\n",
    "\n",
    "        #each episode training loop\n",
    "        for i in range(self.hypers.total_iter):\n",
    "            # Create a new environment and agent for each iteration\n",
    "            env = Environment(size=self.env_size)\n",
    "            agent = Agent(env, self.agent_opts)\n",
    "            env.set_agent_start(agent)\n",
    "            agent.reset_agent()\n",
    "\n",
    "            #calculate length of optimal path\n",
    "            optimal_path = abs(agent.x-env.a[0]) + abs(agent.y-env.a[1]) + abs(env.a[0]-env.b[0]) + abs(env.a[1]-env.b[1])\n",
    "            \n",
    "            #empty variable for per-episode reward\n",
    "            per_ep_reward = 0\n",
    "\n",
    "            #step update loop\n",
    "            for s in range(self.hypers.steps_per_ep):\n",
    "                # Choose between explore and exploit (epsilon-greedy)\n",
    "                if rd.random() < (self.hypers.eps)**(i+1): #epsilon-decay for each episode\n",
    "                    action = rd.randint(0, len(self.agent_opts)-1)\n",
    "                else:\n",
    "                    action = agent.choose_best_option(self.qmat)\n",
    "\n",
    "                state = (agent.x, agent.y, agent.collected, env.a[0], env.a[1])\n",
    "\n",
    "                # Perform agent's action and receive new position, reward, and end of episode flag\n",
    "                new_agent_pos, reward, end_episode = agent.move(self.agent_opts[action])\n",
    "\n",
    "                next_state = (new_agent_pos[0], new_agent_pos[1], new_agent_pos[2], env.a[0], env.a[1])\n",
    "\n",
    "                # Determine the best action in the next state\n",
    "                next_action = np.argmax(self.qmat[next_state])\n",
    "\n",
    "                # Update Q-values using Q-learning equation\n",
    "                self.qmat[state][action] += self.hypers.alpha * (reward + (self.hypers.gamma * (self.qmat[next_state][next_action])) - self.qmat[state][action])\n",
    "\n",
    "                per_ep_reward += reward\n",
    "\n",
    "                if end_episode == 1:\n",
    "                    self.ep_length.append(s)\n",
    "                    break\n",
    "                \n",
    "                if s == self.hypers.steps_per_ep - 1:\n",
    "                    self.ep_length.append(s)\n",
    "\n",
    "            #cleanup step logging metrics from training\n",
    "            self.rewards.append(per_ep_reward)\n",
    "            if i > 50:\n",
    "                self.rolling_mean_empirical_reward.append(sum(self.rewards[i-50:i])/50)\n",
    "\n",
    "            #Add rolling mean of episode length based on mean length of last five episodes\n",
    "            if i > 50:\n",
    "                self.rolling_mean_ep_length.append(sum(self.ep_length[i-50:i])/50)\n",
    "            \n",
    "            #Add diff between chosen and optimal paths to training history\n",
    "            self.distance_history.append(s - optimal_path + 1)\n",
    "\n",
    "            #save q_matrix to logs for future operation\n",
    "            if i%save_every == 0:\n",
    "                self.save_qmat(f\"logs/episode{i}training.npy\")\n",
    "\n",
    "            #Will log the epoch of convergence when last five hundred distance histories have an average distance from optimal of less than 0.001\n",
    "            #This is quite a high threshold for convergence\n",
    "            if self.episode_converged == 0 and i > 500:\n",
    "                if sum(self.distance_history[(i-500):])/500 < 0.001:\n",
    "                    self.episode_converged = i\n",
    "\n",
    "        return self.qmat\n",
    "    \n",
    "    def load_qmat(self, filename, overwrite = False):\n",
    "        qmat = np.load(filename, allow_pickle = False)\n",
    "        if overwrite:\n",
    "            self.qmat = qmat\n",
    "        return qmat\n",
    "\n",
    "    def save_qmat(self, name):\n",
    "        np.save(name, self.qmat, allow_pickle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train QL agent:\n",
    "This code demonstrates training of a Q-Learning agent in the grid environment.\n",
    "1. Set the environment size (env_size) and available movement options (opts).\n",
    "2. Define hyperparameters using the Hypers class to configure the training process.\n",
    "3. Initialize a QL (Q-Learning) agent with the specified environment size, movement options, and hyperparameters.\n",
    "4. Train the agent using the train() method to obtain the Q-value matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "env_size = 5  # Set your environment size\n",
    "opts = [\"north\", \"south\", \"west\", \"east\"]\n",
    "hypers = Hypers(\n",
    "    eps = 0.4, \n",
    "    gamma = 0.8,\n",
    "    alpha = 0.01,\n",
    "    steps_per_ep = 100,\n",
    "    total_iter = 30000\n",
    ")\n",
    "ql = QL(env_size, opts, hypers)\n",
    "qmat = ql.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run visualization:\n",
    " Runs a visualization of the agent's movement in the environment.\n",
    "\n",
    "**Attributes:**\n",
    "- `env (Environment)`: The environment instance where the agent is placed.\n",
    "- `agent (Agent)`: The agent instance to be visualized.\n",
    "- `qmat (numpy.ndarray)`: The Q-value matrix used for making decisions.\n",
    "\n",
    "\n",
    "Run visualization function simulates the agent's movement in the environment based on the actions selected using the Q-value matrix.The function terminates either after a fixed number of steps or when the agent reaches an end state (as indicated by end == 1). \n",
    "\n",
    "*Please note* that when running a visualisation, wait for it to close rather than closing the plot manually, or the cell will have to be interrupted in order to run a new visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_visualisation(qmat, opts = [\"north\", \"south\", \"west\", \"east\"], size=5):\n",
    "    env = Environment(size)\n",
    "    agent = Agent(env, opts)\n",
    "    env.set_agent_start(agent)\n",
    "    env.move_agent((agent.x, agent.y, agent.collected))\n",
    "    snapshots=[]\n",
    "\n",
    "    # Create a figure and axis outside the loop\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_facecolor('white')\n",
    "\n",
    "    # Initialize the plot once with the initial grid\n",
    "    im = ax.imshow(env.grid, cmap=\"bone\", extent=[0, env.size, 0, env.size])\n",
    "\n",
    "    # Run the simulation steps\n",
    "    for step in range(20):\n",
    "\n",
    "        action = agent.choose_best_option(qmat)\n",
    "        pos, _, end = agent.move(agent.opts[action])\n",
    "        env.move_agent(pos)\n",
    "        snapshots.append(env.get_grid())\n",
    "        # Update the grid data for the imshow object\n",
    "        env.plot_grid(snapshots[step], ax)\n",
    "        \n",
    "        # Redraw the plot\n",
    "        fig.canvas.draw()\n",
    "        plt.pause(0.35)  # Add a short pause for visualization\n",
    "        if end == 1:\n",
    "            break\n",
    "    plt.pause(1.5)\n",
    "    plt.close()\n",
    "\n",
    "run_visualisation(qmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Earlier Training Performance\n",
    "\n",
    "Earlier in training the agent has identified how to collect the package occasionally, but not yet had enough experience of collecting the package and delivering it, resulting in erratic behaviour after package collection. Often, it will fail to properly collect the package in the first place!\n",
    "\n",
    "*Please Note*: it is worth running this cell multiple times to observe the variable and inconsistent behaviour. For some configurations, the Q-table will have values which are quite close to the target values, resulting in accurate behaviour, and in others, it will not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "qm = ql.load_qmat(\"logs/episode1500training.npy\")\n",
    "run_visualisation(qm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    "\n",
    "Note that due to the spikiness and variability of training, rolling means are preferable to per-episode values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fully trained at episode 26211\n"
     ]
    }
   ],
   "source": [
    "print(f\"Fully trained at episode {ql.episode_converged}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distance History**\n",
    "\n",
    "This graph of distance history represents the difference between the optimal path to the solution, and the path that the Q-learning tranied agent took. For a fully converged model, distance histories will be zero, as we expect the trained agent to take the optimal route every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ql.distance_history)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Distance From Optimal Solution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rolling Mean Episode Length**\n",
    "\n",
    "Over time, we expect shorter episodes, as the agent tends to exploit its policy rather than exploring the environment freely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ql.rolling_mean_ep_length)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Rolling Mean of Episode Length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ql.rolling_mean_empirical_reward)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Rolling Mean Empirical Reward\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
