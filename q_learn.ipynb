{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    \n",
    "    #fields:\n",
    "    #a grid of size defined by parameter\n",
    "    #b, a tuple repsenting the delivery location\n",
    "    #a, a tuple representing the package location\n",
    "    #size, an int representing the horizontal and vertical dimensions of the grid\n",
    "    def __init__(self, size:int, a = None):\n",
    "        self.size = size\n",
    "        self.b = (size-1, size-1)\n",
    "        self.package = False #boolean determining whether package has been collected\n",
    "        self.grid, self.a, self.agent = self.setup_grid(size, a)\n",
    "        #potentially need a rewards directory for grid states:\n",
    "        #give, say, 50 for (a, agent.collected = 0), and 50 for (a, agent.collected = 1), and -1 for ANY other states\n",
    "        #make this a dict or a lookup table\n",
    "\n",
    "\n",
    "    #method to set up the original grid including a location\n",
    "    def setup_grid(self, size, a):\n",
    "        grid = np.zeros((size,size))\n",
    "        #set a location\n",
    "        if a is None:\n",
    "            x, y = rd.randint(0, size-1), rd.randint(0, size-1)\n",
    "            while (x,y) == self.b:\n",
    "                x, y = rd.randint(0, size-1), rd.randint(0, size-1)\n",
    "            grid[x,y] = 1  #A represented by 1\n",
    "        else:\n",
    "            x, y = a\n",
    "            grid[x,y] = 1\n",
    "        #set agent starting location\n",
    "        agent_x, agent_y = rd.randint(0, size-1), rd.randint(0, size-1)\n",
    "        while ((agent_x, agent_y) == ((x,y) or self.b)):\n",
    "            agent_x, agent_y = rd.randint(0, size-1), rd.randint(0, size-1)\n",
    "        grid[agent_x, agent_y] = -1\n",
    "        grid[self.b] = 2  #B represented by 2\n",
    "        return grid, (x,y), (agent_x, agent_y)\n",
    "    \n",
    "    def get_state(self):\n",
    "        return state(self.a, self.b, self.agent, self.package)\n",
    "    \n",
    "    # def get_reward_from_state(self, state):\n",
    "    #     if state == ()# coordinates here for agent getting a or b\n",
    "    #         return 50\n",
    "    #     return -1        \n",
    "    \n",
    "    #Method which updates the location of the agent on the grid. Currently just zeroes whatever it landed on, setting package collection to true if applicable\n",
    "    def move_agent(self, direction):\n",
    "        agent_x, agent_y = self.agent\n",
    "        self.grid[agent_x, agent_y] = 0\n",
    "        if direction==\"left\":\n",
    "            if agent_x-1 >= 0:\n",
    "                self.agent = (agent_x - 1, agent_y)\n",
    "        if direction==\"right\":\n",
    "            if agent_x + 1 < self.size:\n",
    "                self.agent = (agent_x + 1, agent_y)\n",
    "        if direction==\"up\":\n",
    "            if agent_y-1 >= 0:\n",
    "                self.agent = (agent_x, agent_y - 1)\n",
    "        if direction==\"down\":\n",
    "            if agent_y + 1 < self.size:\n",
    "                self.agent = (agent_x, agent_y +1)\n",
    "        self.grid[self.agent] = -1\n",
    "        if self.agent == self.a:\n",
    "            self.package = True\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class state:\n",
    "    a: tuple\n",
    "    b: tuple\n",
    "    agent: tuple\n",
    "    package: bool\n",
    "    #noot: noot\n",
    "\n",
    "#q table declared as a dictionary of {state, reward:int}\n",
    "\n",
    "\n",
    "\n",
    "#An agent class. Currently barebones\n",
    "class Agent():\n",
    "    def __init__(self):\n",
    "        self.q_table = {}\n",
    "        self.opts = [\"up\", \"down\", \"left\", \"right\"]\n",
    "\n",
    "    def choose_best_option():\n",
    "        #agent acts under policy\n",
    "        #gets a rewad from the state\n",
    "        #calls the Update Q table method\n",
    "        #repeats to end of epoch or task completion\n",
    "        pass\n",
    "\n",
    "    def q_table_update(q_table, state, reward):\n",
    "        pass #I'm kinda high while writing this so I have no idea if this will work \n",
    "    #check each possible move that the agent could make - this can actually test the agent's own logic\n",
    "    #calculate the VALUE of the states of each move\n",
    "    #update own state as a result based on future rewards - values learned in exploration?\n",
    "    #this can be a method of the agent maybe\n",
    "    #Ensure that moves\n",
    "\n",
    "    def move(self):\n",
    "        return rd.choice(self.opts)\n",
    "    \n",
    "#Class to store hyperparameters\n",
    "class Hypers():\n",
    "    def __init__(self, eps, gamma, steps_per_ep):\n",
    "        self.eps = eps # learning rate\n",
    "        self.gamma = gamma # discount rate\n",
    "        self.steps_per_ep = steps_per_ep #steps per episode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learn Notes\n",
    "\n",
    "Rewards come from the environment, agent has values on states that it figures out from the rewards that the environment offers it.\n",
    "\n",
    "Game has multiple episodes (like, tens of thousands).\n",
    "\n",
    "Episode has multiple (hundreds?) timesteps. At each episode, we'll have a different location of A from all 24 possible states.\n",
    "\n",
    "Q table learned across all 24 A locations, 25 agent locations, 4 possible actions, and the pre/post plant situation.\n",
    "\n",
    "So in total 24 * 25 * 2 states, and four possible actions in each state.\n",
    "\n",
    "4800 state-action space in total for this toy example.\n",
    "\n",
    "#### SPECULATIVELY;\n",
    "\n",
    "If we're constructing this Q-table and using the Bellman equation to backpropagate the rewards from end-state into the Q table, we are effectively \"learning\" a table which tells us what future rewards are going to be in future states. This lets us, in our _current_ state, choose the action which will best be producing future rewards. So our first near empty Q table will have the agent wander around effectively randomly.\n",
    "\n",
    "\n",
    "Start with a fixed A and fixed B, to see if you CAN actually learn the Q values. Only THEN do we change to a dynamic location of A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  2.]]\n",
      "[[ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.]\n",
      " [ 0.  0.  0.  0.  2.]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m grid\u001b[39m.\u001b[39mmove_agent(bond\u001b[39m.\u001b[39mmove())\n\u001b[0;32m     16\u001b[0m \u001b[39mprint\u001b[39m(grid\u001b[39m.\u001b[39mgrid)\n\u001b[1;32m---> 17\u001b[0m time\u001b[39m.\u001b[39;49msleep(\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Demo\n",
    "\n",
    "hypers = Hypers(\n",
    "    eps = 1e-3, \n",
    "    gamma = 0.1, \n",
    "    steps_per_ep = 100\n",
    ")\n",
    "\n",
    "import random as rd\n",
    "grid = Environment(size = 5)\n",
    "bond = Agent()\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    grid.move_agent(bond.move())\n",
    "    print(grid.grid)\n",
    "    time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
