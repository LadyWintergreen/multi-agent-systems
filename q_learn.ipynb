{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.12.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random as rd\n",
    "import time\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "# from matplotlib import widgets\n",
    "import matplotlib\n",
    "from matplotlib import cm as cm\n",
    "import scipy as SP\n",
    "from PIL import Image\n",
    "import threading\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "import plotly\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, iplot\n",
    "import plotly.graph_objs as go\n",
    "from scipy import stats \n",
    "from IPython.display import clear_output\n",
    "plotly.offline.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.use('TkAgg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    \n",
    "    #fields:\n",
    "    #a grid of size defined by parameter\n",
    "    #b, a tuple repsenting the delivery location\n",
    "    #a, a tuple representing the package location\n",
    "    #size, an int representing the horizontal and vertical dimensions of the grid\n",
    "    def __init__(self, size:int, a = None):\n",
    "        self.size = size\n",
    "        self.b = (size-1, size-1)\n",
    "        self.grid, self.a = self.setup_grid(size, a)\n",
    "        self.reward_pickup = 100\n",
    "        self.reward_deliver = 100\n",
    "        self.reward_move = -1\n",
    "\n",
    "    #method to set up the original grid including a location\n",
    "    def setup_grid(self, size, a):\n",
    "        grid = np.zeros((size,size))\n",
    "        if a is None:\n",
    "            x, y = rd.randint(0, size-1), rd.randint(0, size-1)\n",
    "            while (x,y) == self.b:\n",
    "                x, y = rd.randint(0, size-1), rd.randint(0, size-1)\n",
    "            grid[x,y] = 1  #A represented by 1\n",
    "        else:\n",
    "            x,y=a\n",
    "            grid[x,y] = 1\n",
    "        grid[size-1,size-1] = 2  #B represented by 2\n",
    "        return grid, (x,y)\n",
    "    \n",
    "    #method to add start location of agent to grid as well\n",
    " #TODO: consider whether to initialise agent as part of environment initialisation\n",
    "    def set_agent_start(self, agent):\n",
    "        self.agent_coords = (agent.x, agent.y)\n",
    "    \n",
    "    #Method which updates the location of the agent on the grid. Currently just zeroes whatever it landed on - can include other logic instead\n",
    "    def move_agent(self, agent_move):\n",
    "        x, y = self.agent_coords\n",
    "        self.grid[x,y] = 0\n",
    "        self.agent_coords = agent_move[0] , agent_move[1]\n",
    "        self.grid[agent_move[0] , agent_move[1]] = -1\n",
    "\n",
    "    def get_grid(self):\n",
    "        return self.grid.tolist()\n",
    "\n",
    "    def plot_grid(self, snapshot, ax=None):\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.set_facecolor('white')\n",
    "        else:\n",
    "            ax.clear()\n",
    "        # Create a figure and axis with a white background\n",
    "        #fig, ax = plt.subplots()\n",
    "        #ax.set_facecolor('white')  # Set background color to white\n",
    "        \n",
    "        # Plot the grid\n",
    "        ax.imshow(np.array([[0]]), cmap=\"bone\", extent=[0, self.size, 0, self.size])\n",
    "\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                cell_value = snapshot[i][j]\n",
    "                if cell_value == -1:\n",
    "                    # Display agent image in the cell\n",
    "                    imagebox = OffsetImage(agent_img, zoom=0.08)\n",
    "                    ab = AnnotationBbox(imagebox, (j + 0.5, self.size - i - 0.5), frameon=False)\n",
    "                    ax.add_artist(ab)\n",
    "                elif cell_value == 1:\n",
    "                    # Display package image in the cell\n",
    "                    imagebox = OffsetImage(package_img, zoom=0.03)\n",
    "                    ab = AnnotationBbox(imagebox, (j + 0.5, self.size - i - 0.5), frameon=False)\n",
    "                    ax.add_artist(ab)\n",
    "                elif cell_value == 2:\n",
    "                    # Display destination image in the cell\n",
    "                    imagebox = OffsetImage(destinationB_img, zoom=0.05)\n",
    "                    ab = AnnotationBbox(imagebox, (j + 0.5, self.size - i - 0.5), frameon=False)\n",
    "                    ax.add_artist(ab)\n",
    "                else:\n",
    "                    ax.text(j + 0.5, self.size - i - 0.5, self.grid[i, j], ha='center', va='center', fontsize=20, color='black')\n",
    "        \n",
    "        # Set axis properties\n",
    "        ax.set_xlim(0, self.size)\n",
    "        ax.set_ylim(0, self.size)\n",
    "        ax.set_xticks(np.arange(self.size) + 1)\n",
    "        ax.set_yticks(np.arange(self.size) + 1)\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.grid(True, linewidth=2, color='white')\n",
    "        \n",
    "        # Set title\n",
    "        ax.set_title(\"Package Delivery Agent\")\n",
    "        \n",
    "        # Show the plot\n",
    "        #plt.show()\n",
    "        return ax\n",
    "\n",
    "agent_img = plt.imread('agent.jpg')\n",
    "package_img = plt.imread('package.jpg')\n",
    "destinationB_img = plt.imread('destinationB.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#An agent class. Currently barebones\n",
    "class Agent():\n",
    "    def __init__(self, environ, opts):\n",
    "        self.environ = environ\n",
    "        self.reset_agent()\n",
    "        self.opts = opts\n",
    "        \n",
    "\n",
    "    # choose the best action based on the greedy policy on Q-values\n",
    "    # filed qmat = Q-matrix of size n*n*options where n=grid_size, options=actions\n",
    "    def choose_best_option(self, qmat):\n",
    "        if self.collected == 0:\n",
    "            state_qs = qmat[self.x-self.environ.a[0] + self.environ.size-1,\n",
    "                            self.y-self.environ.a[1] + self.environ.size-1,\n",
    "                            self.collected] # 1*1*options\n",
    "        else:\n",
    "            state_qs = qmat[self.x-self.environ.b[0] + self.environ.size-1,\n",
    "                            self.y-self.environ.b[1] + self.environ.size-1,\n",
    "                            self.collected] # 1*1*options\n",
    "        action = np.argmax(state_qs) # choose action based on greedy policy where 0=up, 1=down, 2=left, 3=right\n",
    "        return action\n",
    "\n",
    "    #Ensure that moves\n",
    "    \n",
    "    def move(self, direction):\n",
    "\n",
    "        if direction == self.opts[0]:\n",
    "            if self.x-1 >= 0:\n",
    "                self.x = self.x-1\n",
    "        if direction == self.opts[1]:\n",
    "            if self.x+1 < self.environ.size:\n",
    "                self.x = self.x+1\n",
    "        if direction == self.opts[2]:\n",
    "            if self.y-1 >= 0:\n",
    "                self.y = self.y-1\n",
    "        if direction == self.opts[3]:\n",
    "            if self.y+1 < self.environ.size:\n",
    "                self.y=self.y+1\n",
    "        \n",
    "        if self.collected == 0:\n",
    "            if (self.x, self.y) == self.environ.a:\n",
    "                self.collected = 1\n",
    "                reward = self.environ.reward_pickup\n",
    "            else:\n",
    "                reward = self.environ.reward_move\n",
    "        \n",
    "        else:\n",
    "            if (self.x, self.y) == self.environ.b:\n",
    "                self.end_episode = 1\n",
    "                reward = self.environ.reward_deliver\n",
    "            else:\n",
    "                reward = self.environ.reward_move\n",
    "\n",
    "        return (self.x, self.y, self.collected), reward, self.end_episode\n",
    "    \n",
    "    def reset_agent(self):\n",
    "        self.x, self.y = rd.randint(0, self.environ.size-1), rd.randint(0, self.environ.size-1)\n",
    "        while ((self.x, self.y) == (self.environ.a or self.environ.b)):\n",
    "            self.x, self.y = rd.randint(0, self.environ.size-1), rd.randint(0, self.environ.size-1)\n",
    "        self.collected = 0\n",
    "        self.end_episode = 0\n",
    "\n",
    "    \n",
    "#Class to store hyperparameters\n",
    "class Hypers():\n",
    "    def __init__(self, eps, gamma, alpha, steps_per_ep, total_iter):\n",
    "        self.eps = eps\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.steps_per_ep = steps_per_ep\n",
    "        self.total_iter = total_iter\n",
    "\n",
    "##Method to initiate agent coordinates should go in setup method since both the Environment and the Agent need to know\n",
    "#Or we need a hierarchy - consult w/ group\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learn Notes\n",
    "\n",
    "Rewards come from the environment, agent has values on states that it figures out from the rewards that the environment offers it.\n",
    "\n",
    "Game has multiple episodes (like, tens of thousands).\n",
    "\n",
    "Episode has multiple (hundreds?) timesteps. At each episode, we'll have a different location of A from all 24 possible states.\n",
    "\n",
    "Q table learned across all 24 A locations, 25 agent locations, 4 possible actions, and the pre/post plant situation.\n",
    "\n",
    "So in total 24 * 25 * 2 states, and four possible actions in each state.\n",
    "\n",
    "4800 state-action space in total for this toy example.\n",
    "\n",
    "#### SPECULATIVELY;\n",
    "\n",
    "If we're constructing this Q-table and using the Bellman equation to backpropagate the rewards from end-state into the Q table, we are effectively \"learning\" a table which tells us what future rewards are going to be in future states. This lets us, in our _current_ state, choose the action which will best be producing future rewards. So our first near empty Q table will have the agent wander around effectively randomly.\n",
    "\n",
    "\n",
    "Start with a fixed A and fixed B, to see if you CAN actually learn the Q values. Only THEN do we change to a dynamic location of A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QL:\n",
    "    def __init__(self, env_size, agent_opts, hypers):\n",
    "        self.env_size = env_size\n",
    "        self.agent_opts = agent_opts\n",
    "        self.hypers = hypers\n",
    "        self.qmat = np.zeros([2*(env_size-1)+1, 2*(env_size-1)+1, 2, len(agent_opts)])\n",
    "        self.training_history = []\n",
    "\n",
    "    def train(self, save_every = 1000):\n",
    "        for i in range(self.hypers.total_iter):\n",
    "            # Create a new environment and agent for each iteration\n",
    "            env = Environment(size=self.env_size)\n",
    "            agent = Agent(env, self.agent_opts)\n",
    "            env.set_agent_start(agent)\n",
    "            agent.reset_agent()\n",
    "\n",
    "            \n",
    "\n",
    "            for s in range(self.hypers.steps_per_ep):\n",
    "                # Choose between explore and exploit (epsilon-greedy)\n",
    "                if rd.random() < (self.hypers.eps)**(i+1): #epsilon-decay for each episode\n",
    "                    action = rd.randint(0, len(self.agent_opts)-1)\n",
    "                else:\n",
    "                    action = agent.choose_best_option(self.qmat)\n",
    "\n",
    "                if agent.collected == 0:\n",
    "                    state = (agent.x-env.a[0] + self.env_size-1, agent.y-env.a[1] + self.env_size-1, agent.collected)\n",
    "                else:\n",
    "                    state = (agent.x-env.b[0] + self.env_size-1, agent.y-env.b[1] + self.env_size-1, agent.collected)\n",
    "\n",
    "                # Perform agent's action and receive new position, reward, and end of episode flag\n",
    "                new_agent_pos, reward, end_episode = agent.move(self.agent_opts[action])\n",
    "                if agent.collected == 0:\n",
    "                    next_state = (new_agent_pos[0]-env.a[0] + self.env_size-1, new_agent_pos[1]-env.a[1] + self.env_size-1, agent.collected)\n",
    "                else:\n",
    "                    next_state = (new_agent_pos[0]-env.b[0] + self.env_size-1, new_agent_pos[1]-env.b[1] + self.env_size-1, agent.collected) \n",
    "\n",
    "                # Determine the best action in the next state\n",
    "                next_action = np.argmax(self.qmat[next_state])\n",
    "\n",
    "                \n",
    "                # Update Q-values using Q-learning equation\n",
    "                self.qmat[state][action] += self.hypers.alpha * (reward + (self.hypers.gamma * (self.qmat[next_state][next_action])) - self.qmat[state][action])\n",
    "\n",
    "                if end_episode == 1:\n",
    "                    break\n",
    "            # # Log reward value to training history\n",
    "            # self.training_history.append(reward)\n",
    "\n",
    "            # # save the qmatrix for later operation\n",
    "            # if i%save_every == 0:\n",
    "            #     self.save_qmat(f\"logs/episode{s}training.npy\")\n",
    "        return self.qmat\n",
    "    \n",
    "    # def load_qmat(self, filename):\n",
    "    #     self.qmat = np.load(filename)\n",
    "     \n",
    "    #method saving qmatrix to npy file for later testing\n",
    "    # def save_qmat(self, name):\n",
    "    #     np.save(name, self.qmat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "env_size = 5  # Set your environment size\n",
    "opts = [\"up\", \"down\", \"left\", \"right\"]\n",
    "hypers = Hypers(\n",
    "    eps = 0.4, \n",
    "    gamma = 0.8,\n",
    "    alpha = 0.01,\n",
    "    steps_per_ep = 100,\n",
    "    total_iter = 50000\n",
    "    # eps_decay_rate = 1000\n",
    ")\n",
    "ql = QL(env_size, opts, hypers)\n",
    "qmat = ql.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your images and create an instance of the Environment class\n",
    "env = Environment(size=5)\n",
    "bond = Agent(env, opts)\n",
    "env.set_agent_start(bond)\n",
    "env.move_agent((bond.x, bond.y, bond.collected))\n",
    "snapshots=[]\n",
    "snapshot_index = 0\n",
    "# Create a figure and axis outside the loop\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_facecolor('white')\n",
    "\n",
    "# Initialize the plot once with the initial grid\n",
    "im = ax.imshow(env.grid, cmap=\"bone\", extent=[0, env.size, 0, env.size])\n",
    "\n",
    "# Run the simulation steps\n",
    "for step in range(20):\n",
    "    if bond.collected == 0:\n",
    "        state = (bond.x - env.a[0] + env.size - 1, bond.y - env.a[1] + env.size - 1, bond.collected)\n",
    "    else:\n",
    "        state = (bond.x - env.b[0] + env.size - 1, bond.y - env.b[1] + env.size - 1, bond.collected)\n",
    "\n",
    "    action = bond.choose_best_option(qmat)\n",
    "    \n",
    "    pos, _, end = bond.move(bond.opts[action])\n",
    "    env.move_agent(pos)\n",
    "    snapshots.append(env.get_grid())\n",
    "    \n",
    "    # Update the grid data for the imshow object\n",
    "    #im.set_data(env.grid)\n",
    "    env.plot_grid(snapshots[snapshot_index], ax)\n",
    "    \n",
    "    # Redraw the plot\n",
    "    fig.canvas.draw()\n",
    "    plt.pause(0.2)  # Add a short pause for visualization\n",
    "    snapshot_index += 1\n",
    "    \n",
    "    if end == 1:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
