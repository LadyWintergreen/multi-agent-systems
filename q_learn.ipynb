{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    \n",
    "    #fields:\n",
    "    #a grid of size defined by parameter\n",
    "    #b, a tuple repsenting the delivery location\n",
    "    #a, a tuple representing the package location\n",
    "    #size, an int representing the horizontal and vertical dimensions of the grid\n",
    "    def __init__(self, size:int, a = None):\n",
    "        self.size = size\n",
    "        self.b = (size-1, size-1)\n",
    "        self.grid, self.a = self.setup_grid(size, a)\n",
    "        #potentially need a rewards directory for grid states:\n",
    "        #give, say, 50 for (a, agent.collected = 0), and 50 for (a, agent.collected = 1), and -1 for ANY other states\n",
    "        \n",
    "\n",
    "    #method to set up the original grid including a location\n",
    "    def setup_grid(self, size, a):\n",
    "        grid = np.zeros((size,size))\n",
    "        if a is None:\n",
    "            x, y = rd.randint(0, size-1), rd.randint(0, size-1)\n",
    "            while (x,y) == self.b:\n",
    "                x, y = rd.randint(0, size-1), rd.randint(0, size-1)\n",
    "            grid[x,y] = 1  #A represented by 1\n",
    "        else:\n",
    "            x, y = a\n",
    "            grid[x,y] = 1\n",
    "        grid[size-1,size-1] = 2  #B represented by 2\n",
    "        return grid, (x,y)\n",
    "    \n",
    "    #method to add start location of agent to grid as well\n",
    " #TODO: consider whether to initialise agent as part of environment initialisation\n",
    " #pass agent coordinates into constructor instead lmao DENSE\n",
    "    def set_agent_start(self, agent):\n",
    "        self.agent_coords = (agent.x, agent.y)\n",
    "    \n",
    "    #Method which updates the location of the agent on the grid. Currently just zeroes whatever it landed on - can include other logic instead\n",
    "    def move_agent(self, agent_move):\n",
    "        x, y = self.agent_coords\n",
    "        self.grid[x,y] = 0\n",
    "        self.agent_coords = agent_move\n",
    "        self.grid[agent_move] = -1   \n",
    "\n",
    "#An agent class. Currently barebones\n",
    "class Agent():\n",
    "    def __init__(self, environ):\n",
    "        self.environ = environ\n",
    "        self.x, self.y = rd.randint(0, environ.size-1), rd.randint(0, environ.size-1)\n",
    "        while ((self.x, self.y) == (environ.a or environ.b)):\n",
    "            self.x, self.y = rd.randint(0, environ.size-1), rd.randint(0, environ.size-1)\n",
    "        self.opts = [\"up\", \"down\", \"left\", \"right\"]\n",
    "        self.package = 0\n",
    "\n",
    "    def choose_best_option():\n",
    "        #some sort of policy calculation\n",
    "        #return: best option\n",
    "        pass\n",
    "\n",
    "    #Ensure that moves\n",
    "    def move(self, direction):\n",
    "        if direction==\"left\":\n",
    "            if self.x-1 >= 0:\n",
    "                self.x = self.x-1\n",
    "        if direction==\"right\":\n",
    "            if self.x+1 < self.environ.size:\n",
    "                self.x = self.x+1\n",
    "        if direction==\"up\":\n",
    "            if self.y-1 >= 0:\n",
    "                self.y = self.y-1\n",
    "        if direction==\"down\":\n",
    "            if self.y+1 < self.environ.size:\n",
    "                self.y=self.y+1\n",
    "        return self.x, self.y\n",
    "    \n",
    "#Class to store hyperparameters\n",
    "class Hypers():\n",
    "    def __init__(self, eps, gamma, steps_per_ep):\n",
    "        self.eps = eps\n",
    "        self.gamma = gamma\n",
    "        self.steps_per_ep = steps_per_ep\n",
    "\n",
    "##Method to initiate agent coordinates should go in setup method since both the Environment and the Agent need to know\n",
    "#Or we need a hierarchy - consult w/ group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learn Notes\n",
    "\n",
    "Rewards come from the environment, agent has values on states that it figures out from the rewards that the environment offers it.\n",
    "\n",
    "Game has multiple episodes (like, tens of thousands).\n",
    "\n",
    "Episode has multiple (hundreds?) timesteps. At each episode, we'll have a different location of A from all 24 possible states.\n",
    "\n",
    "Q table learned across all 24 A locations, 25 agent locations, 4 possible actions, and the pre/post plant situation.\n",
    "\n",
    "So in total 24 * 25 * 2 states, and four possible actions in each state.\n",
    "\n",
    "4800 state-action space in total for this toy example.\n",
    "\n",
    "#### SPECULATIVELY;\n",
    "\n",
    "If we're constructing this Q-table and using the Bellman equation to backpropagate the rewards from end-state into the Q table, we are effectively \"learning\" a table which tells us what future rewards are going to be in future states. This lets us, in our _current_ state, choose the action which will best be producing future rewards. So our first near empty Q table will have the agent wander around effectively randomly.\n",
    "\n",
    "\n",
    "Start with a fixed A and fixed B, to see if you CAN actually learn the Q values. Only THEN do we change to a dynamic location of A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  2.]]\n",
      "[[ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  2.]]\n",
      "[[ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0. -1.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  2.]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m grid\u001b[39m.\u001b[39mmove_agent(bond\u001b[39m.\u001b[39mmove(new_location))\n\u001b[1;32m     18\u001b[0m \u001b[39mprint\u001b[39m(grid\u001b[39m.\u001b[39mgrid)\n\u001b[0;32m---> 19\u001b[0m time\u001b[39m.\u001b[39;49msleep(\u001b[39m1\u001b[39;49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Demo\n",
    "\n",
    "hypers = Hypers(\n",
    "    eps = 1e-3, \n",
    "    gamma = 0.1, \n",
    "    steps_per_ep = 100\n",
    ")\n",
    "\n",
    "import random as rd\n",
    "grid = Environment(size = 5)\n",
    "bond = Agent(grid)\n",
    "grid.set_agent_start(bond)\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    new_location = rd.choice(bond.opts)\n",
    "    grid.move_agent(bond.move(new_location))\n",
    "    print(grid.grid)\n",
    "    time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
