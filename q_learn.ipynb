{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtnqbBVgdbPi",
        "outputId": "149b7b5a-0dca-4be0-bade-2bbadefa96b8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-2.25.2.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# importing all necessary libraries\n",
        "import numpy as np\n",
        "import random as rd\n",
        "import time\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "# from matplotlib import widgets\n",
        "import matplotlib\n",
        "from matplotlib import cm as cm\n",
        "import scipy as SP\n",
        "from PIL import Image\n",
        "import threading\n",
        "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
        "import plotly\n",
        "from plotly.offline import download_plotlyjs, init_notebook_mode, iplot\n",
        "import plotly.graph_objs as go\n",
        "from scipy import stats\n",
        "from IPython.display import clear_output\n",
        "plotly.offline.init_notebook_mode(connected=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXF6HM6zdbPn"
      },
      "outputs": [],
      "source": [
        "matplotlib.use('TkAgg')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment Class\n",
        "\n",
        "The `Environment` class has been used to initialize the grid-based environment in which the package delivery agent operates. It is initialized with a grid size, random agent starting location and package location 'A'. The environment has a fixed location for destination 'B', which is the bottom-right corner of the grid.\n",
        "\n",
        "**Methods:**\n",
        "\n",
        "- `set_agent_start(agent)`: Sets the agent's starting location in the environment.\n",
        "- `move_agent(agent_move)`: Updates the agent's location on the grid in respect to the movement action taken by the agent.\n",
        "- `get_grid()`: This will give the current state of the grid.\n",
        "- `plot_grid(snapshot, ax=None)`: This is the implementation of the visualization for the grid state throughout the journey of the agent from its start location to end destination. Visualization is implemented using Matplotlib. It displays images representing the agent, package, and destination locations.\n",
        "\n",
        "### Reward Structure Explanation:\n",
        "\n",
        "In the context of the package delivery problem, a reward structure is used to provide feedback to the agent's decisions and actions. This feedback is used as a guide to urge the agent towards making optimal decisions to achieve the goal of delivering the package efficiently.\n",
        "\n",
        "- `reward_pickup`: A positive reward given to the agent when it picks up the package. It encourages the agent to collect the package.\n",
        "- `reward_deliver`: A positive reward given to the agent when it successfully delivers the package to the destination. It incentivizes the agent to complete the delivery.\n",
        "- `reward_move`: A negative reward given to the agent for each move it makes. If the move is not helping it reach the package or deliver it, the reward will be negative. This will ensure that the agent minimizes unnecessary movements.\n",
        "\n",
        "These reward values are used to help the agent learn via reinforcement learning, over the period of training the agent will learn the best actions and decisions to make that will maximize its cumulative reward over time. It will learn to pick up the package, deliver it to the destination, and minimize the number of moves required to successfully complete this task."
      ],
      "metadata": {
        "id": "EsErT_44hs11"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSXLpitqdbPo"
      },
      "outputs": [],
      "source": [
        "class Environment():\n",
        "    def __init__(self, size:int, a = None):\n",
        "        \"\"\"\n",
        "          Initialize a grid of size defined by parameter\n",
        "          b; a tuple repsenting the delivery location\n",
        "          a; a tuple representing the package location\n",
        "          size; an int representing the horizontal and vertical dimensions of the grid\n",
        "        \"\"\"\n",
        "        self.size = size\n",
        "        self.b = (size-1, size-1)\n",
        "        self.grid, self.a = self.setup_grid(size, a)\n",
        "        self.reward_pickup = 100\n",
        "        self.reward_deliver = 100\n",
        "        self.reward_move = -1\n",
        "\n",
        "    #method to set up the original grid including a location\n",
        "    def setup_grid(self, size, a):\n",
        "        grid = np.zeros((size,size))\n",
        "        if a is None:\n",
        "            x, y = rd.randint(0, size-1), rd.randint(0, size-1)\n",
        "            while (x,y) == self.b:\n",
        "                x, y = rd.randint(0, size-1), rd.randint(0, size-1)\n",
        "            grid[x,y] = 1  #A represented by 1\n",
        "        else:\n",
        "            x,y=a\n",
        "            grid[x,y] = 1\n",
        "        grid[size-1,size-1] = 2  #B represented by 2\n",
        "        return grid, (x,y)\n",
        "\n",
        "    #method to add start location of agent to grid as well\n",
        "    def set_agent_start(self, agent):\n",
        "        self.agent_coords = (agent.x, agent.y)\n",
        "\n",
        "    #Method which updates the location of the agent on the grid. Currently just zeroes whatever it landed on - can include other logic instead\n",
        "    def move_agent(self, agent_move):\n",
        "        x, y = self.agent_coords\n",
        "        self.grid[x,y] = 0\n",
        "        self.agent_coords = agent_move[0] , agent_move[1]\n",
        "        self.grid[agent_move[0] , agent_move[1]] = -1\n",
        "\n",
        "    def get_grid(self):\n",
        "        return self.grid.tolist()\n",
        "\n",
        "    # visulaize the agent movement through iterations\n",
        "    def plot_grid(self, snapshot, ax=None):\n",
        "        if ax is None:\n",
        "            fig, ax = plt.subplots()\n",
        "            ax.set_facecolor('white')\n",
        "        else:\n",
        "            ax.clear()\n",
        "\n",
        "        # Plot the grid\n",
        "        ax.imshow(np.array([[0]]), cmap=\"bone\", extent=[0, self.size, 0, self.size])\n",
        "\n",
        "        for i in range(self.size):\n",
        "            for j in range(self.size):\n",
        "                cell_value = snapshot[i][j]\n",
        "                if cell_value == -1:\n",
        "                    # Display agent image in the cell\n",
        "                    imagebox = OffsetImage(agent_img, zoom=0.08)\n",
        "                    ab = AnnotationBbox(imagebox, (j + 0.5, self.size - i - 0.5), frameon=False)\n",
        "                    ax.add_artist(ab)\n",
        "                elif cell_value == 1:\n",
        "                    # Display package image in the cell\n",
        "                    imagebox = OffsetImage(package_img, zoom=0.03)\n",
        "                    ab = AnnotationBbox(imagebox, (j + 0.5, self.size - i - 0.5), frameon=False)\n",
        "                    ax.add_artist(ab)\n",
        "                elif cell_value == 2:\n",
        "                    # Display destination image in the cell\n",
        "                    imagebox = OffsetImage(destinationB_img, zoom=0.05)\n",
        "                    ab = AnnotationBbox(imagebox, (j + 0.5, self.size - i - 0.5), frameon=False)\n",
        "                    ax.add_artist(ab)\n",
        "                else:\n",
        "                    ax.text(j + 0.5, self.size - i - 0.5, self.grid[i, j], ha='center', va='center', fontsize=20, color='black')\n",
        "\n",
        "        # Set axis properties\n",
        "        ax.set_xlim(0, self.size)\n",
        "        ax.set_ylim(0, self.size)\n",
        "        ax.set_xticks(np.arange(self.size) + 1)\n",
        "        ax.set_yticks(np.arange(self.size) + 1)\n",
        "        ax.set_xticklabels([])\n",
        "        ax.set_yticklabels([])\n",
        "        ax.grid(True, linewidth=2, color='white')\n",
        "\n",
        "        # Set title\n",
        "        ax.set_title(\"Package Delivery Agent\")\n",
        "\n",
        "        # Show the plot\n",
        "        #plt.show()\n",
        "        return ax\n",
        "\n",
        "agent_img = plt.imread('agent.jpg')\n",
        "package_img = plt.imread('package.jpg')\n",
        "destinationB_img = plt.imread('destinationB.jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agent Class\n",
        "The `Agent` class is used to intialize the agent that interacts with the environment and learns to make decisions.\n",
        "\n",
        "**Methods:**\n",
        "- `choose_best_option(qmat)`: Uses the q-matrix and determines the best action based on the greedy policy. It accordingly returns which action should be taken to the agent.\n",
        "- `move(direction)`: makes the agent move by updating the position based on the chosen action. The new state and associated reward is updated.\n",
        "- `reset_agent()`: resets the agent's state to a random position within the environment.\n",
        "\n",
        "### Hypers Class\n",
        "The Hypers class is defined to store the hyperparameters used in reinforcement learning process. They will influence the agent's learning behavior & exploration strategy. The hyperparameters will determine how the agent explores the environment, learns from experiences, and makes updated to take informed decisions."
      ],
      "metadata": {
        "id": "mZkvPNAPiD9a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zchk_ipEdbPq"
      },
      "outputs": [],
      "source": [
        "class Agent():\n",
        "    def __init__(self, environ, opts):\n",
        "        self.environ = environ\n",
        "        self.reset_agent()\n",
        "        self.opts = opts\n",
        "\n",
        "    # choose the best action based on the greedy policy on Q-values\n",
        "    # filed qmat = Q-matrix of size n*n*options where n=grid_size, options=actions\n",
        "    def choose_best_option(self, qmat):\n",
        "        if self.collected == 0:\n",
        "            state_qs = qmat[self.x-self.environ.a[0] + self.environ.size-1,\n",
        "                            self.y-self.environ.a[1] + self.environ.size-1,\n",
        "                            self.collected] # 1*1*options\n",
        "        else:\n",
        "            state_qs = qmat[self.x-self.environ.b[0] + self.environ.size-1,\n",
        "                            self.y-self.environ.b[1] + self.environ.size-1,\n",
        "                            self.collected] # 1*1*options\n",
        "        action = np.argmax(state_qs) # choose action based on greedy policy where 0=up, 1=down, 2=left, 3=right\n",
        "        return action\n",
        "\n",
        "    # allow agent to move by taking action up, down, left and right.\n",
        "    def move(self, direction):\n",
        "\n",
        "        if direction == self.opts[0]:\n",
        "            if self.x-1 >= 0:\n",
        "                self.x = self.x-1\n",
        "        if direction == self.opts[1]:\n",
        "            if self.x+1 < self.environ.size:\n",
        "                self.x = self.x+1\n",
        "        if direction == self.opts[2]:\n",
        "            if self.y-1 >= 0:\n",
        "                self.y = self.y-1\n",
        "        if direction == self.opts[3]:\n",
        "            if self.y+1 < self.environ.size:\n",
        "                self.y=self.y+1\n",
        "\n",
        "        if self.collected == 0:\n",
        "            if (self.x, self.y) == self.environ.a:\n",
        "                self.collected = 1\n",
        "                reward = self.environ.reward_pickup\n",
        "            else:\n",
        "                reward = self.environ.reward_move\n",
        "\n",
        "        else:\n",
        "            if (self.x, self.y) == self.environ.b:\n",
        "                self.end_episode = 1\n",
        "                reward = self.environ.reward_deliver\n",
        "            else:\n",
        "                reward = self.environ.reward_move\n",
        "\n",
        "        return (self.x, self.y, self.collected), reward, self.end_episode\n",
        "\n",
        "    def reset_agent(self):\n",
        "        self.x, self.y = rd.randint(0, self.environ.size-1), rd.randint(0, self.environ.size-1)\n",
        "        while ((self.x, self.y) == (self.environ.a or self.environ.b)):\n",
        "            self.x, self.y = rd.randint(0, self.environ.size-1), rd.randint(0, self.environ.size-1)\n",
        "        self.collected = 0\n",
        "        self.end_episode = 0\n",
        "\n",
        "#Class to store hyperparameters\n",
        "class Hypers():\n",
        "    def __init__(self, eps, gamma, alpha, steps_per_ep, total_iter):\n",
        "        \"\"\"\n",
        "            Initializes the class with the following hyperparameters:\n",
        "          eps: Exploration factor (epsilon) defines the exploration - exploitation probability.\n",
        "          gamma: Discount factor that determines the importance of future rewards in the agent's decision-making.\n",
        "          alpha: Learning rate controls how quickly the agent adapts to new information to learn it.\n",
        "          steps_per_ep: The maximum number of steps allowed per episode during training.\n",
        "          total_iter: The total number of training iterations or episodes that the agent will undergo.\n",
        "        \"\"\"\n",
        "        self.eps = eps\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.steps_per_ep = steps_per_ep\n",
        "        self.total_iter = total_iter"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q-Learning class:\n",
        "The QL class trains the agent usiing Q-Leaarning algorithm.\n",
        "\n",
        "**Attributes:**\n",
        "\n",
        "- `env_size (int)`: Size of the environment grid.\n",
        "\n",
        "- `agent_opts (list)`: List of available agent movement options.\n",
        "\n",
        "- `hypers (object)`: Hyperparameters for training.\n",
        "\n",
        "- `qmat (numpy.ndarray)`: Q-value matrix to store Q-values for state-action pairs.\n",
        "- `distance_history (list)`: List to store the difference between chosen and optimal paths during training.\n",
        "\n",
        "**Methods:**\n",
        "\n",
        "- `__init__(self, env_size, agent_opts, hypers)`:Constructor for QL class.\n",
        "        \n",
        "- `train(self, save_every=500)`:Train the agent using Q-learning algorithm.\n",
        "        \n",
        "- `load_qmat(self, filename, overwrite=False)`: Load a Q-value matrix from a file.\n",
        "        \n",
        "- `save_qmat(self, name)`:Save the current Q-value matrix to a file.\n",
        "\n"
      ],
      "metadata": {
        "id": "xbeauuoZ2WTH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "If_579wkdbPu"
      },
      "outputs": [],
      "source": [
        "class QL:\n",
        "    def __init__(self, env_size, agent_opts, hypers):\n",
        "       \"\"\"\n",
        "        Initialize QL class with the following arguments.\n",
        "            env_size (int): Size of the environment grid.\n",
        "            agent_opts (list): List of available agent movement options.\n",
        "            hypers (object): Hyperparameters for training.\n",
        "        \"\"\"\n",
        "        self.env_size = env_size\n",
        "        self.agent_opts = agent_opts\n",
        "        self.hypers = hypers\n",
        "        self.qmat = np.zeros([2*(env_size-1)+1, 2*(env_size-1)+1, 2, len(agent_opts)])\n",
        "        self.distance_history = []\n",
        "\n",
        "    def train(self, save_every = 500):\n",
        "      \"\"\"\n",
        "        Train the agent using Q-learning algorithm.\n",
        "\n",
        "        Args:\n",
        "            save_every (int): Frequency of saving the Q-value matrix.\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: Trained Q-value matrix.\n",
        "        \"\"\"\n",
        "        for i in range(self.hypers.total_iter):\n",
        "            # Create a new environment and agent for each iteration\n",
        "            env = Environment(size=self.env_size)\n",
        "            agent = Agent(env, self.agent_opts)\n",
        "            env.set_agent_start(agent)\n",
        "            agent.reset_agent()\n",
        "\n",
        "            #calculate length of optimal path\n",
        "            optimal_path = abs(agent.x-env.a[0]) + abs(agent.y-env.a[1]) + abs(env.a[0]-env.b[0]) + abs(env.a[1]-env.b[1])\n",
        "\n",
        "            for s in range(self.hypers.steps_per_ep):\n",
        "                # Choose between explore and exploit (epsilon-greedy)\n",
        "                if rd.random() < (self.hypers.eps)**(i+1): #epsilon-decay for each episode\n",
        "                    action = rd.randint(0, len(self.agent_opts)-1)\n",
        "                else:\n",
        "                    action = agent.choose_best_option(self.qmat)\n",
        "\n",
        "                if agent.collected == 0:\n",
        "                    state = (agent.x-env.a[0] + self.env_size-1, agent.y-env.a[1] + self.env_size-1, agent.collected)\n",
        "                else:\n",
        "                    state = (agent.x-env.b[0] + self.env_size-1, agent.y-env.b[1] + self.env_size-1, agent.collected)\n",
        "\n",
        "                # Perform agent's action and receive new position, reward, and end of episode flag\n",
        "                new_agent_pos, reward, end_episode = agent.move(self.agent_opts[action])\n",
        "                if agent.collected == 0:\n",
        "                    next_state = (new_agent_pos[0]-env.a[0] + self.env_size-1, new_agent_pos[1]-env.a[1] + self.env_size-1, agent.collected)\n",
        "                else:\n",
        "                    next_state = (new_agent_pos[0]-env.b[0] + self.env_size-1, new_agent_pos[1]-env.b[1] + self.env_size-1, agent.collected)\n",
        "\n",
        "                # Determine the best action in the next state\n",
        "                next_action = np.argmax(self.qmat[next_state])\n",
        "\n",
        "                # Update Q-values using Q-learning equation\n",
        "                self.qmat[state][action] += self.hypers.alpha * (reward + (self.hypers.gamma * (self.qmat[next_state][next_action])) - self.qmat[state][action])\n",
        "\n",
        "                if end_episode == 1:\n",
        "                    break\n",
        "\n",
        "            #Add diff between chosen and optimal paths to training history\n",
        "            self.distance_history.append(s - optimal_path + 1)\n",
        "\n",
        "            #save q_matrix to logs for future operation\n",
        "            if i%save_every == 0:\n",
        "                self.save_qmat(f\"logs/episode{i}training.npy\")\n",
        "        return self.qmat\n",
        "\n",
        "    def load_qmat(self, filename, overwrite = False):\n",
        "      \"\"\"\n",
        "        Load a Q-value matrix from a file.\n",
        "\n",
        "        Args:\n",
        "            filename (str): Name of the file to load the Q-value matrix from.\n",
        "            overwrite (bool): If True, overwrite the current Q-value matrix with the loaded one.\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: Loaded Q-value matrix.\n",
        "        \"\"\"\n",
        "        qmat = np.load(filename, allow_pickle=False)\n",
        "        if overwrite:\n",
        "            self.qmat = qmat\n",
        "        return qmat\n",
        "\n",
        "    def save_qmat(self, name):\n",
        "      \"\"\"\n",
        "        Save the current Q-value matrix to a file.\n",
        "\n",
        "        Args:\n",
        "            name (str): Name of the file to save the Q-value matrix to.\n",
        "        \"\"\"\n",
        "        np.save(name, self.qmat, allow_pickle=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Train QL agent:\n",
        "This code demonstrates training of a Q-Learning agent in the grid environment.\n",
        "1. Set the environment size (env_size) and available movement options (opts).\n",
        "2. Define hyperparameters using the Hypers class to configure the training process.\n",
        "3. Initialize a QL (Q-Learning) agent with the specified environment size, movement options, and hyperparameters.\n",
        "4. Train the agent using the train() method to obtain the Q-value matrix."
      ],
      "metadata": {
        "id": "vArarRX05MZt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixRLKa3KdbPv"
      },
      "outputs": [],
      "source": [
        "# Train\n",
        "env_size = 5  # Set your environment size\n",
        "opts = [\"up\", \"down\", \"left\", \"right\"]\n",
        "# Configure Hyperparameters for Training\n",
        "hypers = Hypers(\n",
        "    eps=0.4,            # Epsilon value for epsilon-greedy exploration\n",
        "    gamma=0.8,          # Discount factor for future rewards in Q-learning\n",
        "    alpha=0.01,         # Learning rate for updating Q-values\n",
        "    steps_per_ep=100,   # Maximum number of steps per episode\n",
        "    total_iter=20000    # Total number of training iterations\n",
        "    # eps_decay_rate=1000\n",
        ")\n",
        "# Initialize QL Agent and Train\n",
        "\"\"\"\n",
        "Create an instance of the QL class with the specified environment size, movement options, and hyperparameters.\n",
        "Train the agent using the train() method to learn Q-values and obtain the Q-value matrix (qmat).\n",
        "\"\"\"\n",
        "ql = QL(env_size, opts, hypers)\n",
        "qmat = ql.train()  # Train the agent and obtain the Q-value matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Run visualization:\n",
        " Runs a visualization of the agent's movement in the environment.\n",
        "\n",
        "**Attributes:**\n",
        "- `env (Environment)`: The environment instance where the agent is placed.\n",
        "- `agent (Agent)`: The agent instance to be visualized.\n",
        "- `qmat (numpy.ndarray)`: The Q-value matrix used for making decisions.\n",
        "\n",
        "\n",
        "Run visualization function simulates the agent's movement in the environment based on the actions selected using the Q-value matrix.The function terminates either after a fixed number of steps or when the agent reaches an end state (as indicated by end == 1)."
      ],
      "metadata": {
        "id": "M8e7YqMp6JkW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXXcn2U_dbPw"
      },
      "outputs": [],
      "source": [
        "env = Environment(size=5)\n",
        "agent = Agent(env, opts)\n",
        "\n",
        "def run_visualisation(env, agent, qmat):\n",
        "  # Set the agent's starting position in the environment\n",
        "    env.set_agent_start(agent)\n",
        "    env.move_agent((agent.x, agent.y, agent.collected))\n",
        "    snapshots=[]\n",
        "\n",
        "    # Create a figure and axis outside the loop\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_facecolor('white')\n",
        "\n",
        "    # Initialize the plot once with the initial grid\n",
        "    im = ax.imshow(env.grid, cmap=\"bone\", extent=[0, env.size, 0, env.size])\n",
        "\n",
        "    # Run the simulation steps\n",
        "    for step in range(40):\n",
        "        action = agent.choose_best_option(qmat)# Choose the best action according to the Q-value matrix\n",
        "        pos, _, end = agent.move(agent.opts[action])\n",
        "        env.move_agent(pos)\n",
        "        snapshots.append(env.get_grid())\n",
        "        env.plot_grid(snapshots[step], ax)\n",
        "\n",
        "        # Redraw the plot\n",
        "        fig.canvas.draw()\n",
        "        plt.pause(0.2)  # Add a short pause for visualization\n",
        "        if end == 1:\n",
        "            break\n",
        "\n",
        "run_visualisation(env, agent, qmat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eP8A_DgDdbPw"
      },
      "outputs": [],
      "source": [
        "#Generate and display a graph of the training history.\n",
        "def graph_training_history(ql):\n",
        "    plt.plot(ql.distance_history)  # Plot the distance history data\n",
        "    plt.xlabel(\"Episode\")          # Set the label for the x-axis\n",
        "    plt.ylabel(\"Distance From Optimal Solution\")  # Set the label for the y-axis\n",
        "    plt.title(\"Training History\")   # Set the title for the graph\n",
        "    plt.show()                     # Display the generated graph\n",
        "\n",
        "graph_training_history(ql)# Call the function to generate and display the graph"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}