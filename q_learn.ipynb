{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rd\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import Button, Slider\n",
    "import matplotlib.cm as cm\n",
    "import scipy as SP\n",
    "\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    \n",
    "    #fields:\n",
    "    #a grid of size defined by parameter\n",
    "    #b, a tuple repsenting the delivery location\n",
    "    #a, a tuple representing the package location\n",
    "    #size, an int representing the horizontal and vertical dimensions of the grid\n",
    "    def __init__(self, size:int, a = None):\n",
    "        self.size = size\n",
    "        self.b = (size-1, size-1)\n",
    "        self.grid, self.a = self.setup_grid(size, a)\n",
    "        self.reward_pickup = 100\n",
    "        self.reward_deliver = 100\n",
    "        self.reward_move = -1\n",
    "        ## IS THIS WHERE THE AGENT SHOULD BE INITIATED?\n",
    "\n",
    "    #method to set up the original grid including a location\n",
    "    def setup_grid(self, size, a):\n",
    "        grid = np.zeros((size,size))\n",
    "        if a is None:\n",
    "            x, y = rd.randint(0, size-1), rd.randint(0, size-1)\n",
    "            while (x,y) == self.b:\n",
    "                x, y = rd.randint(0, size-1), rd.randint(0, size-1)\n",
    "            grid[x,y] = 1  #A represented by 1\n",
    "        else:\n",
    "            x,y=a\n",
    "            grid[x,y] = 1\n",
    "        grid[size-1,size-1] = 2  #B represented by 2\n",
    "        return grid, (x,y)\n",
    "    \n",
    "    #method to add start location of agent to grid as well\n",
    " #TODO: consider whether to initialise agent as part of environment initialisation\n",
    " #pass agent coordinates into constructor instead lmao DENSE\n",
    "    def set_agent_start(self, agent):\n",
    "        self.agent_coords = (agent.x, agent.y)\n",
    "    \n",
    "    #Method which updates the location of the agent on the grid. Currently just zeroes whatever it landed on - can include other logic instead\n",
    "    def move_agent(self, agent_move):\n",
    "        x, y = self.agent_coords\n",
    "        self.grid[x,y] = 0\n",
    "        self.agent_coords = agent_move[0] , agent_move[1]\n",
    "        self.grid[agent_move[0] , agent_move[1]] = -1\n",
    "        \n",
    "    def plot_grid(self):\n",
    "    #Create a figure and axis with a white background\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_facecolor('white')  # Set background color to white\n",
    "\n",
    "        # Plot the grid\n",
    "        ax.imshow(np.array([[0]]), cmap=\"bone\", extent=[0, self.size, 0, self.size])\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                ax.text(j + 0.5, self.size - i - 0.5,self.grid[i,j],  ha='center', va='center', fontsize=20, color='white')\n",
    "\n",
    "        # Set axis properties\n",
    "        ax.set_xlim(0, self.size)\n",
    "        ax.set_ylim(0, self.size)\n",
    "        ax.set_xticks(np.arange(self.size) + 1)\n",
    "        ax.set_yticks(np.arange(self.size) + 1)\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.grid(True, linewidth=2, color='white')\n",
    "\n",
    "        # Set title\n",
    "        ax.set_title(\"5x5 Grid Visualization\")\n",
    "\n",
    "        #Show the plot\n",
    "        plt.show()\n",
    "\n",
    "#An agent class. Currently barebones\n",
    "class Agent():\n",
    "    def __init__(self, environ):\n",
    "        self.environ = environ\n",
    "        self.reset_agent()\n",
    "        self.opts = [\"up\", \"down\", \"left\", \"right\"]\n",
    "        \n",
    "\n",
    "    # choose the best action based on the greedy policy on Q-values\n",
    "    # filed qmat = Q-matrix of size n*n*options where n=grid_size, options=actions\n",
    "    def choose_best_option(self, qmat):\n",
    "        if self.collected == 0:\n",
    "            state_qs = qmat[self.x-self.environ.a[0] + self.environ.size-1,\n",
    "                            self.y-self.environ.a[1] + self.environ.size-1,\n",
    "                            self.collected] # 1*1*options\n",
    "        else:\n",
    "            state_qs = qmat[self.x-self.environ.b[0] + self.environ.size-1,\n",
    "                            self.y-self.environ.b[1] + self.environ.size-1,\n",
    "                            self.collected] # 1*1*options\n",
    "        action = np.argmax(state_qs) # choose action based on greedy policy where 0=up, 1=down, 2=left, 3=right\n",
    "        return action\n",
    "\n",
    "    #Ensure that moves\n",
    "    def move(self, direction):\n",
    "\n",
    "        if direction==\"up\":\n",
    "            if self.x-1 >= 0:\n",
    "                self.x = self.x-1\n",
    "        if direction==\"down\":\n",
    "            if self.x+1 < self.environ.size:\n",
    "                self.x = self.x+1\n",
    "        if direction==\"left\":\n",
    "            if self.y-1 >= 0:\n",
    "                self.y = self.y-1\n",
    "        if direction==\"right\":\n",
    "            if self.y+1 < self.environ.size:\n",
    "                self.y=self.y+1\n",
    "        \n",
    "        if self.collected == 0:\n",
    "            if (self.x, self.y) == self.environ.a:\n",
    "                self.collected = 1\n",
    "                reward = self.environ.reward_pickup\n",
    "            else:\n",
    "                reward = self.environ.reward_move\n",
    "        \n",
    "        else:\n",
    "            if (self.x, self.y) == self.environ.b:\n",
    "                self.end_episode = 1\n",
    "                reward = self.environ.reward_deliver\n",
    "            else:\n",
    "                reward = self.environ.reward_move\n",
    "\n",
    "        return (self.x, self.y, self.collected), reward, self.end_episode\n",
    "    \n",
    "    def reset_agent(self):\n",
    "        self.x, self.y = rd.randint(0, self.environ.size-1), rd.randint(0, self.environ.size-1)\n",
    "        while ((self.x, self.y) == (self.environ.a or self.environ.b)):\n",
    "            self.x, self.y = rd.randint(0, self.environ.size-1), rd.randint(0, self.environ.size-1)\n",
    "        self.collected = 0\n",
    "        self.end_episode = 0\n",
    "\n",
    "    \n",
    "#Class to store hyperparameters\n",
    "class Hypers():\n",
    "    def __init__(self, eps, gamma, alpha, steps_per_ep, total_iter):\n",
    "        self.eps = eps\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.steps_per_ep = steps_per_ep\n",
    "        self.total_iter = total_iter\n",
    "\n",
    "##Method to initiate agent coordinates should go in setup method since both the Environment and the Agent need to know\n",
    "#Or we need a hierarchy - consult w/ group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learn Notes\n",
    "\n",
    "Rewards come from the environment, agent has values on states that it figures out from the rewards that the environment offers it.\n",
    "\n",
    "Game has multiple episodes (like, tens of thousands).\n",
    "\n",
    "Episode has multiple (hundreds?) timesteps. At each episode, we'll have a different location of A from all 24 possible states.\n",
    "\n",
    "Q table learned across all 24 A locations, 25 agent locations, 4 possible actions, and the pre/post plant situation.\n",
    "\n",
    "So in total 24 * 25 * 2 states, and four possible actions in each state.\n",
    "\n",
    "4800 state-action space in total for this toy example.\n",
    "\n",
    "#### SPECULATIVELY;\n",
    "\n",
    "If we're constructing this Q-table and using the Bellman equation to backpropagate the rewards from end-state into the Q table, we are effectively \"learning\" a table which tells us what future rewards are going to be in future states. This lets us, in our _current_ state, choose the action which will best be producing future rewards. So our first near empty Q table will have the agent wander around effectively randomly.\n",
    "\n",
    "\n",
    "Start with a fixed A and fixed B, to see if you CAN actually learn the Q values. Only THEN do we change to a dynamic location of A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo\n",
    "\n",
    "hypers = Hypers(\n",
    "    eps = 0.4, \n",
    "    gamma = 0.8,\n",
    "    alpha = 0.01,\n",
    "    steps_per_ep = 100,\n",
    "    total_iter = 50000\n",
    ")\n",
    "\n",
    "import random as rd\n",
    "\n",
    "env = Environment(size=5)\n",
    "bond = Agent(env)\n",
    "env.set_agent_start(bond)\n",
    "\n",
    "\n",
    "qmat = np.zeros([2*(env.size-1)+1 , 2*(env.size-1)+1 , 2 , len(bond.opts)])\n",
    "\n",
    "for i in range(hypers.total_iter):\n",
    "    # print(\"qmat = \", np.argmax(qmat,3))\n",
    "    env = Environment(size=5)\n",
    "    env.set_agent_start(bond)\n",
    "    bond = Agent(env)\n",
    "    bond.reset_agent()\n",
    "    \n",
    "    # print(\"pickup - a: \",env.a)\n",
    "    # print(\"agent - pos: \",bond.x,bond.y)\n",
    "    \n",
    "    for s in range(hypers.steps_per_ep):\n",
    "\n",
    "        # choose between explore and exploit (epsilon-greedy)\n",
    "        if rd.random()<hypers.eps:\n",
    "            action = rd.randint(0,len(bond.opts)-1)\n",
    "        else:\n",
    "            action = bond.choose_best_option(qmat)\n",
    "\n",
    "\n",
    "        if bond.collected == 0:\n",
    "            state = (bond.x-env.a[0] + env.size-1 , bond.y-env.a[1] + env.size-1 , bond.collected)\n",
    "        else:\n",
    "            state = (bond.x-env.b[0] + env.size-1 , bond.y-env.b[1] + env.size-1 , bond.collected)\n",
    "\n",
    "        new_agent_pos, reward, end_episode = bond.move(bond.opts[action])\n",
    "        if bond.collected == 0:\n",
    "            next_state = (new_agent_pos[0]-env.a[0] + env.size-1 , new_agent_pos[1]-env.a[1] + env.size-1 , bond.collected)\n",
    "        else:\n",
    "            next_state = (new_agent_pos[0]-env.b[0] + env.size-1 , new_agent_pos[1]-env.b[1] + env.size-1 , bond.collected) \n",
    "        \n",
    "        env.move_agent(new_agent_pos)\n",
    "        \n",
    "        next_action = np.argmax(qmat[next_state])\n",
    "        qmat[state][action]+=hypers.alpha*(reward + (hypers.gamma*(qmat[next_state][next_action])) - qmat[state][action])\n",
    "        \n",
    "\n",
    "        # print(\"s, reward: \", s, reward)\n",
    "        if end_episode == 1:\n",
    "            # print(\"qmat = \", np.argmax(qmat,3))\n",
    "            # print(\"end of episode\")\n",
    "            break\n",
    "\n",
    "\n",
    "# for i in range(10):\n",
    "#     env.move_agent(bond.move(rd.choice(bond.opts)))\n",
    "#     print(env.grid)\n",
    "#     env.plot_grid()\n",
    "#     time.sleep(1)\n",
    "\n",
    "\n",
    "# class QL():\n",
    "#     def __init__(self, environ, agent):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "env = Environment(size = 5) #, a = (0,4)\n",
    "bond = Agent(env)\n",
    "env.set_agent_start(bond)\n",
    "\n",
    "# bond.x = 4\n",
    "# bond.y = 0\n",
    "env.move_agent((bond.x, bond.y, bond.collected))\n",
    "# env.plot_grid()\n",
    "for step in range(20):\n",
    "    \n",
    "    if bond.collected == 0:\n",
    "        state = (bond.x-env.a[0] + env.size-1 , bond.y-env.a[1] + env.size-1 , bond.collected)\n",
    "    else:\n",
    "        state = (bond.x-env.b[0] + env.size-1 , bond.y-env.b[1] + env.size-1 , bond.collected)\n",
    "\n",
    "    action = bond.choose_best_option(qmat)\n",
    "    \n",
    "    pos,_,end = bond.move(bond.opts[action])\n",
    "    env.move_agent(pos) \n",
    "    # print(env.grid)\n",
    "    # env.plot_grid()\n",
    "    if end == 1:\n",
    "        print(\"end\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
