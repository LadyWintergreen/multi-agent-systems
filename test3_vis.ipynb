{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "import numpy as np\n",
    "import itertools\n",
    "import random as rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.use(\"TkAgg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "Purpose of this branch is to test convergence properties of the network using one-hot-vectors for the state space instead of 0-3, as the OHV format may be easier for the network to operate on. Due to the need to operate on zero vectors we will\n",
    "\n",
    "- add a small noise function gamma to each OHV\n",
    "- use He initialisation for weight initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    \n",
    "    #fields:\n",
    "    #a grid of size defined by parameter\n",
    "    #b, a tuple repsenting the delivery location\n",
    "    #a, a tuple representing the package location\n",
    "    #size, an int representing the horizontal and vertical dimensions of the grid\n",
    "    def __init__(self, size:int):\n",
    "        self.size = size\n",
    "        self.actions = ['north', 'south', 'west', 'east']\n",
    "        self.reward_pickup = 10\n",
    "        self.reward_deliver = 10\n",
    "        self.reward_move = -0.1\n",
    "        self.reset_env()\n",
    "    \n",
    "    def reset_env(self):\n",
    "        combinations = list(itertools.product(list(range(self.size)), repeat=2))\n",
    "        rd.shuffle(combinations)\n",
    "        self.a, self.b, self.agent_coords = combinations[0:3]\n",
    "        self.collected = 0\n",
    "        self.done = False\n",
    "        self.terminated = False\n",
    "        self.test_length = 0 #an additional parameter to determine how long the environment has been active for\n",
    "        self.setup_grid()\n",
    "\n",
    "    def coordinates_to_noisy_ohv(self, coordinate:int):\n",
    "        ohv = np.zeros(4)\n",
    "        noise = np.random.normal(-1e-5, 1e-5, ohv.shape)\n",
    "        ohv[coordinate] = 1\n",
    "        ohv = ohv + noise\n",
    "        return ohv\n",
    "\n",
    "    #method to set up the original grid including a location\n",
    "    def setup_grid(self):\n",
    "        self.grid = np.zeros((self.size, self.size))\n",
    "            \n",
    "        # Place the \"a\" and \"b\" locations on the grid\n",
    "        if self.a is None:\n",
    "            x, y = rd.randint(0, self.size - 1), rd.randint(0, self.size - 1)\n",
    "            while (x, y) == self.b:\n",
    "                x, y = rd.randint(0, self.size - 1), rd.randint(0, self.size - 1)\n",
    "            self.a = (x, y)  # A represented by 1\n",
    "        else:\n",
    "            x, y = self.a\n",
    "        self.grid[x, y] = 1  # Place \"a\" on the grid\n",
    "\n",
    "        if self.b is None:\n",
    "            x, y = rd.randint(0, self.size - 1), rd.randint(0, self.size - 1)\n",
    "            while (x, y) == self.a:\n",
    "                x, y = rd.randint(0, self.size - 1), rd.randint(0, self.size - 1)\n",
    "            self.b = (x, y)  # B represented by 2\n",
    "        else:\n",
    "            x, y = self.b\n",
    "        self.grid[x, y] = 2  # Place \"b\" on the grid\n",
    "\n",
    "        # Place the \"home\" location (0) on the grid\n",
    "        x, y = self.agent_coords\n",
    "        self.grid[x, y] = 0\n",
    "\n",
    "    def get_state(self):\n",
    "        #This method unpacks the coordinates into a tuple which can be used to index the qmatrix or for easy incorporation to the Deep Q Network\n",
    "        coords = [self.agent_coords[0], self.agent_coords[1], self.a[0], self.a[1], self.b[0], self.b[1]]\n",
    "        coords = [self.coordinates_to_noisy_ohv(c) for c in coords]\n",
    "        coords.append(np.array(self.collected))\n",
    "        return np.hstack(coords)\n",
    "        return np.ndarray.flatten(np.array(coords))\n",
    "\n",
    "        return np.array([self.agent_coords[0], self.agent_coords[1], self.collected, self.a[0], self.a[1], self.b[0], self.b[1]])\n",
    "    \n",
    "    #Method which updates the location of the agent on the grid. Currently just zeroes whatever it landed on - can include other logic instead\n",
    "    def move_agent(self, action):\n",
    "        #For agent move, note that 0 = up, 1=down, 2=left, 3=right\n",
    "        self.test_length += 1\n",
    "\n",
    "        #Assign new coordinates for agent to exist at\n",
    "        new_x, new_y = self.agent_coords\n",
    "        if action == 'north' and self.agent_coords[1] > 0:\n",
    "            new_y -= 1\n",
    "        elif action == 'south' and self.agent_coords[1] < self.size - 1:\n",
    "            new_y += 1\n",
    "        elif action == 'west' and self.agent_coords[0] > 0:\n",
    "            new_x -= 1\n",
    "        elif action == 'east' and self.agent_coords[0] < self.size - 1:\n",
    "             new_x += 1\n",
    "        \n",
    "        #Update the grid based on new agent coordinates\n",
    "        self.grid[self.agent_coords] = 0\n",
    "        self.agent_coords = (new_x, new_y)\n",
    "        self.grid[self.agent_coords] = -1\n",
    "\n",
    "        #Handle logic based on new agent location\n",
    "        if self.collected == 0:\n",
    "            if self.agent_coords == self.a:\n",
    "                self.collected = 1\n",
    "                reward = self.reward_pickup\n",
    "            else:\n",
    "                reward = self.reward_move\n",
    "        \n",
    "        else:\n",
    "            if self.agent_coords == self.b:\n",
    "                self.done = True\n",
    "                reward = self.reward_deliver\n",
    "            else:\n",
    "                reward = self.reward_move\n",
    "\n",
    "                #This code was considered test code which could be useful in exiting early\n",
    "        if self.test_length > 50:\n",
    "            self.terminated = True\n",
    "        return reward, self.done, self.terminated\n",
    "\n",
    "    def get_grid(self):\n",
    "        return self.grid.tolist()\n",
    "\n",
    "    def plot_grid(self, snapshot, ax=None):\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.set_facecolor('white')\n",
    "        else:\n",
    "            ax.clear()\n",
    "        \n",
    "        # Plot the grid\n",
    "        ax.imshow(np.array([[0]]), cmap=\"bone\", extent=[0, self.size, 0, self.size])\n",
    "\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                cell_value = snapshot[i][j]\n",
    "                if cell_value == -1:\n",
    "                    # Display agent image in the cell\n",
    "                    imagebox = OffsetImage(agent_img, zoom=0.08)\n",
    "                    ab = AnnotationBbox(imagebox, (j + 0.5, self.size - i - 0.5), frameon=False)\n",
    "                    ax.add_artist(ab)\n",
    "                elif cell_value == 1:\n",
    "                    # Display package image in the cell\n",
    "                    imagebox = OffsetImage(package_img, zoom=0.03)\n",
    "                    ab = AnnotationBbox(imagebox, (j + 0.5, self.size - i - 0.5), frameon=False)\n",
    "                    ax.add_artist(ab)\n",
    "                elif cell_value == 2:\n",
    "                    # Display destination image in the cell\n",
    "                    imagebox = OffsetImage(destinationB_img, zoom=0.05)\n",
    "                    ab = AnnotationBbox(imagebox, (j + 0.5, self.size - i - 0.5), frameon=False)\n",
    "                    ax.add_artist(ab)\n",
    "                else:\n",
    "                    ax.text(j + 0.5, self.size - i - 0.5, self.grid[i, j], ha='center', va='center', fontsize=20, color='black')\n",
    "        \n",
    "        # Set axis properties\n",
    "        ax.set_xlim(0, self.size)\n",
    "        ax.set_ylim(0, self.size)\n",
    "        ax.set_xticks(np.arange(self.size) + 1)\n",
    "        ax.set_yticks(np.arange(self.size) + 1)\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.grid(True, linewidth=2, color='white')\n",
    "        \n",
    "        # Set title\n",
    "        ax.set_title(\"Package Delivery Agent\")\n",
    "        \n",
    "        # Show the plot\n",
    "        #plt.show()\n",
    "        return ax\n",
    "\n",
    "agent_img = plt.imread('agent.jpg')\n",
    "package_img = plt.imread('package.jpg')\n",
    "destinationB_img = plt.imread('destinationB.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the TransportAgent agent\n",
    "#new statespace size should be 25 - this represents six 4-size one-hot-vectors, plus a collected flag\n",
    "class TransportAgent:\n",
    "    def __init__(self, statespace_size=25, gamma=0.99, learning_rate=0.997, start_epsilon=1.0,\n",
    "                 epsilon_decay_factor=0.997, min_epsilon=0.1, replay_buffer_size=1000,\n",
    "                 batch_size=200, network_copy_frequency=500):\n",
    "        self.statespace_size = statespace_size\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model2 = self.prepare_torch()\n",
    "        self.memory = []  # Using a list instead of a deque\n",
    "        self.epsilon = start_epsilon\n",
    "        self.epsilon_min = min_epsilon\n",
    "        self.epsilon_decay_factor = epsilon_decay_factor\n",
    "        self.replay_buffer_size = replay_buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.network_copy_frequency = network_copy_frequency\n",
    "        self.steps_since_copy = 0  # Counter for network copy\n",
    "\n",
    "    def prepare_torch(self):\n",
    "        l1 = self.statespace_size\n",
    "        l2 = 150\n",
    "        l3 = 100\n",
    "        l4 = 4\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(l1, l2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(l2, l3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(l3, l4)\n",
    "        )\n",
    "        model2 = copy.deepcopy(self.model)\n",
    "        model2.load_state_dict(self.model.state_dict())\n",
    "        self.loss_fn = torch.nn.HuberLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        return model2\n",
    "\n",
    "    def update_target(self):\n",
    "        if self.steps_since_copy >= self.network_copy_frequency:\n",
    "            self.model2.load_state_dict(self.model.state_dict())\n",
    "            self.steps_since_copy = 0\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if len(self.memory) > self.replay_buffer_size:\n",
    "            self.memory.pop(0)  # Remove the oldest experience\n",
    "\n",
    "    def get_qvals(self, state):\n",
    "        state1 = torch.from_numpy(state).float()\n",
    "        qvals_torch = self.model(state1)\n",
    "        qvals = qvals_torch.data.numpy()\n",
    "        return qvals\n",
    "\n",
    "    def get_maxQ(self, s):\n",
    "        return torch.max(self.model2(torch.from_numpy(s).float())).float()\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(range(4))  # Random action\n",
    "        q_values = self.get_qvals(state)\n",
    "        return np.argmax(q_values[0])  # Greedy action\n",
    "\n",
    "    def process_minibatch(self, minibatch):\n",
    "        states = []\n",
    "        actions = []\n",
    "        targets = []\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            q_values = self.get_qvals(state)\n",
    "            if done:\n",
    "                q_values[action] = reward\n",
    "            else:\n",
    "                q_values[action] = reward + self.gamma * self.get_maxQ(next_state)\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            targets.append(q_values[0])\n",
    "        return np.array(states), np.array(actions), np.array(targets) #this is returning a thruple of state transitions?\n",
    "\n",
    "    def train_one_step(self, states, actions, targets):\n",
    "        state1_batch = torch.from_numpy(states).float()\n",
    "        # state1_batch = torch.Tensor([torch.from_numpy(s).float() for s in states])\n",
    "        # print(f\"Somehow this is {state1_batch.shape}\")\n",
    "        # print(f\"Shape of top {state1_batch[0].shape}\")\n",
    "        # print(f\"shape of states: {states.shape}\")\n",
    "        action_batch = torch.Tensor(actions)\n",
    "        # print(f\"and actions have shape {action_batch.shape}\")\n",
    "        # print(f\"while one action has shape{action_batch_1.shape}\")\n",
    "        Q1 = self.model(state1_batch)\n",
    "        X = Q1.gather(dim=1, index=action_batch.long().unsqueeze(dim=1)).squeeze()\n",
    "        Y = torch.tensor(targets)\n",
    "        loss = torch.nn.MSELoss()(X, Y)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_visualisation(agent, max_steps = 20, size=4):\n",
    "    #Assumes a fully trained transport agent. \n",
    "    #Target network should either have been trained in the preceeding step, or loaded from a pickled pytorch weights file\n",
    "    env = Environment(size)\n",
    "        \n",
    "    # Create a figure and axis outside the loop\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_facecolor('white')\n",
    "\n",
    "    # Initialize the plot once with the initial grid\n",
    "    m = ax.imshow(env.grid, cmap=\"bone\", extent=[0, env.size, 0, env.size])\n",
    "    snapshots = []\n",
    "    for step in range(max_steps):\n",
    "        state = env.get_state()\n",
    "        action = agent.act(state)\n",
    "        _, done, _ = env.move_agent(env.actions[action])\n",
    "        snapshots.append(env.get_grid())\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    for s in snapshots:\n",
    "        env.plot_grid(s, ax)\n",
    "        # Redraw the plot\n",
    "        fig.canvas.draw()\n",
    "        plt.pause(0.2)  # Add a short pause for visualization\n",
    "    plt.pause(1.5)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Total Reward: 25.199999999999974, Epsilon: 0.99999 | Loss: n/a\n",
      "Terminated: True | Done: True\n",
      "Episode: 2, Total Reward: 45.39999999999996, Epsilon: 0.9999800001000001 | Loss: n/a\n",
      "Terminated: True | Done: True\n",
      "Episode: 3, Total Reward: 15.100000000000007, Epsilon: 0.9999700002999992 | Loss: n/a\n",
      "Terminated: True | Done: True\n",
      "Episode: 4, Total Reward: 15.100000000000014, Epsilon: 0.9999600005999962 | Loss: 18062769.91546756\n",
      "Terminated: True | Done: True\n",
      "Episode: 5, Total Reward: 35.29999999999997, Epsilon: 0.9999500009999903 | Loss: 8720.002710865992\n",
      "Terminated: True | Done: True\n",
      "Episode: 6, Total Reward: 8.300000000000024, Epsilon: 0.9999400014999804 | Loss: 25.727411879210912\n",
      "Terminated: True | Done: True\n",
      "Episode: 7, Total Reward: 15.099999999999984, Epsilon: 0.9999300020999654 | Loss: 26.181568968529795\n",
      "Terminated: True | Done: True\n",
      "Episode: 8, Total Reward: 55.49999999999995, Epsilon: 0.9999200027999444 | Loss: 18.67709950839772\n",
      "Terminated: True | Done: True\n",
      "Episode: 9, Total Reward: 15.100000000000001, Epsilon: 0.9999100035999164 | Loss: 1918.2614482337353\n",
      "Terminated: True | Done: True\n",
      "Episode: 10, Total Reward: 15.100000000000017, Epsilon: 0.9999000044998805 | Loss: 6.475047233057957\n",
      "Terminated: True | Done: True\n",
      "Episode: 11, Total Reward: 25.199999999999967, Epsilon: 0.9998900054998355 | Loss: 5.378552712646186\n",
      "Terminated: True | Done: True\n",
      "Episode: 12, Total Reward: 45.39999999999999, Epsilon: 0.9998800065997806 | Loss: 17328.34381262929\n",
      "Terminated: True | Done: True\n",
      "Episode: 13, Total Reward: 8.800000000000018, Epsilon: 0.9998700077997147 | Loss: 165.9857850534874\n",
      "Terminated: True | Done: True\n",
      "Episode: 14, Total Reward: 45.4, Epsilon: 0.9998600090996367 | Loss: 2.0632218776964675\n",
      "Terminated: True | Done: True\n",
      "Episode: 15, Total Reward: 65.59999999999994, Epsilon: 0.9998500104995457 | Loss: 2.3695793876460955\n",
      "Terminated: True | Done: True\n",
      "Episode: 16, Total Reward: 1.2000000000000313, Epsilon: 0.9998400119994407 | Loss: 1.8986988086449472\n",
      "Terminated: True | Done: True\n",
      "Episode: 17, Total Reward: 13.400000000000006, Epsilon: 0.9998300135993208 | Loss: 1.46999212047633\n",
      "Terminated: True | Done: True\n",
      "Episode: 18, Total Reward: 13.400000000000006, Epsilon: 0.9998200152991848 | Loss: 1.1411790019449066\n",
      "Terminated: True | Done: True\n",
      "Episode: 19, Total Reward: 14.500000000000016, Epsilon: 0.999810017099032 | Loss: 1.0514811774094899\n",
      "Terminated: True | Done: True\n",
      "Episode: 20, Total Reward: 12.000000000000007, Epsilon: 0.999800018998861 | Loss: 0.8477409700976639\n",
      "Terminated: True | Done: True\n",
      "Episode: 21, Total Reward: 45.39999999999999, Epsilon: 0.999790020998671 | Loss: 0.12786483438229004\n",
      "Terminated: True | Done: True\n",
      "Episode: 22, Total Reward: 55.49999999999999, Epsilon: 0.999780023098461 | Loss: 0.4706054426474022\n",
      "Terminated: True | Done: True\n",
      "Episode: 23, Total Reward: 35.29999999999999, Epsilon: 0.9997700252982301 | Loss: 0.7011457674351393\n",
      "Terminated: True | Done: True\n",
      "Episode: 24, Total Reward: 8.700000000000003, Epsilon: 0.9997600275979772 | Loss: 0.9426986943120542\n",
      "Terminated: True | Done: True\n",
      "Episode: 25, Total Reward: 14.600000000000005, Epsilon: 0.9997500299977012 | Loss: 1.0333354387964522\n",
      "Terminated: True | Done: True\n",
      "Episode: 26, Total Reward: 45.39999999999998, Epsilon: 0.9997400324974013 | Loss: 1.173392796048931\n",
      "Terminated: True | Done: True\n",
      "Episode: 27, Total Reward: 55.499999999999986, Epsilon: 0.9997300350970764 | Loss: 1.3178730139545365\n",
      "Terminated: True | Done: True\n",
      "Episode: 28, Total Reward: 15.099999999999977, Epsilon: 0.9997200377967255 | Loss: 1.3973339351953245\n",
      "Terminated: True | Done: True\n",
      "Episode: 29, Total Reward: 65.60000000000004, Epsilon: 0.9997100405963475 | Loss: 1.7893481090957044\n",
      "Terminated: True | Done: True\n",
      "Episode: 30, Total Reward: 13.100000000000017, Epsilon: 0.9997000434959415 | Loss: 1.6161636030170279\n",
      "Terminated: True | Done: True\n",
      "Episode: 31, Total Reward: 35.30000000000001, Epsilon: 0.9996900464955066 | Loss: 1.1951709620508493\n",
      "Terminated: True | Done: True\n",
      "Episode: 32, Total Reward: 55.49999999999996, Epsilon: 0.9996800495950418 | Loss: 1.2267075917300057\n",
      "Terminated: True | Done: True\n",
      "Episode: 33, Total Reward: 7.400000000000022, Epsilon: 0.9996700527945458 | Loss: 1.5224358078557998\n",
      "Terminated: True | Done: True\n",
      "Episode: 34, Total Reward: 14.100000000000007, Epsilon: 0.9996600560940179 | Loss: 1.499915708283909\n",
      "Terminated: True | Done: True\n",
      "Episode: 35, Total Reward: 9.300000000000002, Epsilon: 0.999650059493457 | Loss: 1.4081647926514302\n",
      "Terminated: True | Done: True\n",
      "Episode: 36, Total Reward: 15.100000000000007, Epsilon: 0.9996400629928621 | Loss: 1.0809773715103375\n",
      "Terminated: True | Done: True\n",
      "Episode: 37, Total Reward: 25.199999999999946, Epsilon: 0.9996300665922322 | Loss: 1.270452255711836\n",
      "Terminated: True | Done: True\n",
      "Episode: 38, Total Reward: 10.400000000000018, Epsilon: 0.9996200702915663 | Loss: 1.3911364335490732\n",
      "Terminated: True | Done: True\n",
      "Episode: 39, Total Reward: 35.29999999999998, Epsilon: 0.9996100740908634 | Loss: 1.0221268584330876\n",
      "Terminated: True | Done: True\n",
      "Episode: 40, Total Reward: 12.200000000000003, Epsilon: 0.9996000779901226 | Loss: 1.1313886053860187\n",
      "Terminated: True | Done: True\n",
      "Episode: 41, Total Reward: 25.19999999999998, Epsilon: 0.9995900819893427 | Loss: 1.2180685333761514\n",
      "Terminated: True | Done: True\n",
      "Episode: 42, Total Reward: 15.09999999999999, Epsilon: 0.9995800860885229 | Loss: 1.2892201531167125\n",
      "Terminated: True | Done: True\n",
      "Episode: 43, Total Reward: 14.400000000000007, Epsilon: 0.9995700902876621 | Loss: 1.3601234765916035\n",
      "Terminated: True | Done: True\n",
      "Episode: 44, Total Reward: 25.19999999999997, Epsilon: 0.9995600945867592 | Loss: 1.2635000231219273\n",
      "Terminated: True | Done: True\n",
      "Episode: 45, Total Reward: 15.100000000000007, Epsilon: 0.9995500989858134 | Loss: 1.0449759428407632\n",
      "Terminated: True | Done: True\n",
      "Episode: 46, Total Reward: 14.900000000000002, Epsilon: 0.9995401034848236 | Loss: 0.8858892324398149\n",
      "Terminated: True | Done: True\n",
      "Episode: 47, Total Reward: 12.400000000000006, Epsilon: 0.9995301080837887 | Loss: 0.964882995073612\n",
      "Terminated: True | Done: True\n",
      "Episode: 48, Total Reward: 10.200000000000017, Epsilon: 0.9995201127827079 | Loss: 0.7115048696100712\n",
      "Terminated: True | Done: True\n",
      "Episode: 49, Total Reward: -7.700000000000113, Epsilon: 0.9995101175815801 | Loss: 0.30557959841870563\n",
      "Terminated: True | Done: True\n",
      "Episode: 50, Total Reward: 45.39999999999995, Epsilon: 0.9995001224804044 | Loss: 0.12758943440470225\n",
      "Terminated: True | Done: True\n",
      "Episode: 51, Total Reward: 14.900000000000002, Epsilon: 0.9994901274791796 | Loss: 0.0781314847507637\n",
      "Terminated: True | Done: True\n",
      "Episode: 52, Total Reward: 10.300000000000015, Epsilon: 0.9994801325779049 | Loss: 0.2442550586825801\n",
      "Terminated: True | Done: True\n",
      "Episode: 53, Total Reward: 15.100000000000001, Epsilon: 0.9994701377765792 | Loss: 0.5231128829878335\n",
      "Terminated: True | Done: True\n",
      "Episode: 54, Total Reward: 15.100000000000003, Epsilon: 0.9994601430752015 | Loss: 0.6535012125238484\n",
      "Terminated: True | Done: True\n",
      "Episode: 55, Total Reward: 12.900000000000006, Epsilon: 0.9994501484737708 | Loss: 0.6256380625681518\n",
      "Terminated: True | Done: True\n",
      "Episode: 56, Total Reward: 7.000000000000002, Epsilon: 0.9994401539722861 | Loss: 0.6967340219645959\n",
      "Terminated: True | Done: True\n",
      "Episode: 57, Total Reward: 10.300000000000006, Epsilon: 0.9994301595707464 | Loss: 0.5689591091935231\n",
      "Terminated: True | Done: True\n",
      "Episode: 58, Total Reward: 11.900000000000006, Epsilon: 0.9994201652691508 | Loss: 0.3729970634042521\n",
      "Terminated: True | Done: True\n",
      "Episode: 59, Total Reward: 65.60000000000002, Epsilon: 0.9994101710674982 | Loss: 0.40050493936766596\n",
      "Terminated: True | Done: True\n",
      "Episode: 60, Total Reward: 25.19999999999996, Epsilon: 0.9994001769657875 | Loss: 0.31649408255722006\n",
      "Terminated: True | Done: True\n",
      "Episode: 61, Total Reward: 45.39999999999995, Epsilon: 0.9993901829640179 | Loss: 0.5724093731419713\n",
      "Terminated: True | Done: True\n",
      "Episode: 62, Total Reward: 25.199999999999974, Epsilon: 0.9993801890621884 | Loss: 0.5718723960659083\n",
      "Terminated: True | Done: True\n",
      "Episode: 63, Total Reward: 7.00000000000003, Epsilon: 0.9993701952602978 | Loss: 0.64605761395598\n",
      "Terminated: True | Done: True\n",
      "Episode: 64, Total Reward: 25.200000000000003, Epsilon: 0.9993602015583453 | Loss: 0.9662198513454082\n",
      "Terminated: True | Done: True\n",
      "Episode: 65, Total Reward: 65.59999999999998, Epsilon: 0.9993502079563297 | Loss: 1.2515131119711727\n",
      "Terminated: True | Done: True\n",
      "Episode: 66, Total Reward: 12.500000000000004, Epsilon: 0.9993402144542501 | Loss: 1.485591340374637\n",
      "Terminated: True | Done: True\n",
      "Episode: 67, Total Reward: 11.800000000000004, Epsilon: 0.9993302210521057 | Loss: 1.3459315339014644\n",
      "Terminated: True | Done: True\n",
      "Episode: 68, Total Reward: 14.200000000000005, Epsilon: 0.9993202277498952 | Loss: 1.148111283344527\n",
      "Terminated: True | Done: True\n",
      "Episode: 69, Total Reward: 12.800000000000015, Epsilon: 0.9993102345476177 | Loss: 1.123947847835921\n",
      "Terminated: True | Done: True\n",
      "Episode: 70, Total Reward: 1.8000000000000167, Epsilon: 0.9993002414452723 | Loss: 1.0002564578114643\n",
      "Terminated: True | Done: True\n",
      "Episode: 71, Total Reward: 45.39999999999999, Epsilon: 0.9992902484428579 | Loss: 0.6084924651510721\n",
      "Terminated: True | Done: True\n",
      "Episode: 72, Total Reward: 65.59999999999998, Epsilon: 0.9992802555403735 | Loss: 0.9625230355589998\n",
      "Terminated: True | Done: True\n",
      "Episode: 73, Total Reward: 25.199999999999992, Epsilon: 0.9992702627378182 | Loss: 1.293054600848871\n",
      "Terminated: True | Done: True\n",
      "Episode: 74, Total Reward: 14.600000000000014, Epsilon: 0.9992602700351908 | Loss: 1.3212084799472774\n",
      "Terminated: True | Done: True\n",
      "Episode: 75, Total Reward: 11.500000000000012, Epsilon: 0.9992502774324905 | Loss: 1.5060078268763664\n",
      "Terminated: True | Done: True\n",
      "Episode: 76, Total Reward: 13.20000000000001, Epsilon: 0.9992402849297163 | Loss: 1.5449489372117178\n",
      "Terminated: True | Done: True\n",
      "Episode: 77, Total Reward: -5.49999999999995, Epsilon: 0.999230292526867 | Loss: 1.2389856541840947\n",
      "Terminated: True | Done: True\n",
      "Episode: 78, Total Reward: 13.500000000000005, Epsilon: 0.9992203002239418 | Loss: 0.5240147071265018\n",
      "Terminated: True | Done: True\n",
      "Episode: 79, Total Reward: 10.600000000000014, Epsilon: 0.9992103080209396 | Loss: 0.24853369452466723\n",
      "Terminated: True | Done: True\n",
      "Episode: 80, Total Reward: 12.800000000000018, Epsilon: 0.9992003159178594 | Loss: 0.38418752401815476\n",
      "Terminated: True | Done: True\n",
      "Episode: 81, Total Reward: 13.000000000000004, Epsilon: 0.9991903239147003 | Loss: 0.3089371547782018\n",
      "Terminated: True | Done: True\n",
      "Episode: 82, Total Reward: 9.400000000000006, Epsilon: 0.9991803320114612 | Loss: 0.4491719422965414\n",
      "Terminated: True | Done: True\n",
      "Episode: 83, Total Reward: 11.90000000000001, Epsilon: 0.9991703402081411 | Loss: 0.629195260422596\n",
      "Terminated: True | Done: True\n",
      "Episode: 84, Total Reward: -2.999999999999952, Epsilon: 0.9991603485047391 | Loss: 0.6405534409648932\n",
      "Terminated: True | Done: True\n",
      "Episode: 85, Total Reward: 15.099999999999977, Epsilon: 0.9991503569012541 | Loss: 0.6855435339624391\n",
      "Terminated: True | Done: True\n",
      "Episode: 86, Total Reward: 65.60000000000007, Epsilon: 0.9991403653976851 | Loss: 0.9108516895303539\n",
      "Terminated: True | Done: True\n",
      "Episode: 87, Total Reward: 35.29999999999996, Epsilon: 0.9991303739940312 | Loss: 1.1013571082376967\n",
      "Terminated: True | Done: True\n",
      "Episode: 88, Total Reward: 25.19999999999996, Epsilon: 0.9991203826902912 | Loss: 1.0994948405845493\n",
      "Terminated: True | Done: True\n",
      "Episode: 89, Total Reward: 35.299999999999976, Epsilon: 0.9991103914864644 | Loss: 1.1036902631030363\n",
      "Terminated: True | Done: True\n",
      "Episode: 90, Total Reward: 14.300000000000004, Epsilon: 0.9991004003825495 | Loss: 0.9624259386022213\n",
      "Terminated: True | Done: True\n",
      "Episode: 91, Total Reward: 13.500000000000005, Epsilon: 0.9990904093785458 | Loss: 0.8742216033722038\n",
      "Terminated: True | Done: True\n",
      "Episode: 92, Total Reward: 13.200000000000012, Epsilon: 0.9990804184744521 | Loss: 0.5598188157592501\n",
      "Terminated: True | Done: True\n",
      "Episode: 93, Total Reward: 12.100000000000005, Epsilon: 0.9990704276702673 | Loss: 0.6305837585234347\n",
      "Terminated: True | Done: True\n",
      "Episode: 94, Total Reward: 8.100000000000001, Epsilon: 0.9990604369659907 | Loss: 0.3097127082734561\n",
      "Terminated: True | Done: True\n",
      "Episode: 95, Total Reward: 12.600000000000005, Epsilon: 0.9990504463616211 | Loss: 0.15241099497621977\n",
      "Terminated: True | Done: True\n",
      "Episode: 96, Total Reward: 12.60000000000002, Epsilon: 0.9990404558571575 | Loss: 0.16137342430010904\n",
      "Terminated: True | Done: True\n",
      "Episode: 97, Total Reward: 14.500000000000016, Epsilon: 0.999030465452599 | Loss: 0.1825710492874414\n",
      "Terminated: True | Done: True\n",
      "Episode: 98, Total Reward: 11.300000000000017, Epsilon: 0.9990204751479446 | Loss: 0.18992727652474653\n",
      "Terminated: True | Done: True\n",
      "Episode: 99, Total Reward: 15.099999999999966, Epsilon: 0.9990104849431931 | Loss: 0.006180235333101568\n",
      "Terminated: True | Done: True\n",
      "Episode: 100, Total Reward: 25.199999999999957, Epsilon: 0.9990004948383437 | Loss: 0.27954472916077494\n",
      "Terminated: True | Done: True\n",
      "Episode: 101, Total Reward: 11.700000000000012, Epsilon: 0.9989905048333954 | Loss: 0.3996987330124659\n",
      "Terminated: True | Done: True\n",
      "Episode: 102, Total Reward: 11.700000000000003, Epsilon: 0.998980514928347 | Loss: 0.35957521588048513\n",
      "Terminated: True | Done: True\n",
      "Episode: 103, Total Reward: 4.100000000000023, Epsilon: 0.9989705251231978 | Loss: 0.6868104813851833\n",
      "Terminated: True | Done: True\n",
      "Episode: 104, Total Reward: 25.19999999999999, Epsilon: 0.9989605354179466 | Loss: 0.7622876802788061\n",
      "Terminated: True | Done: True\n",
      "Episode: 105, Total Reward: 4.600000000000011, Epsilon: 0.9989505458125925 | Loss: 0.6828174338652155\n",
      "Terminated: True | Done: True\n",
      "Episode: 106, Total Reward: 25.19999999999998, Epsilon: 0.9989405563071344 | Loss: 0.528664071587663\n",
      "Terminated: True | Done: True\n",
      "Episode: 107, Total Reward: 11.100000000000003, Epsilon: 0.9989305669015713 | Loss: 0.5686238589850101\n",
      "Terminated: True | Done: True\n",
      "Episode: 108, Total Reward: 9.200000000000017, Epsilon: 0.9989205775959024 | Loss: 0.5924452077936042\n",
      "Terminated: True | Done: True\n",
      "Episode: 109, Total Reward: 14.800000000000017, Epsilon: 0.9989105883901265 | Loss: 0.4875718879478949\n",
      "Terminated: True | Done: True\n",
      "Episode: 110, Total Reward: 14.60000000000002, Epsilon: 0.9989005992842426 | Loss: 0.5022036955093166\n",
      "Terminated: True | Done: True\n",
      "Episode: 111, Total Reward: 35.299999999999976, Epsilon: 0.9988906102782498 | Loss: 0.5671713035371081\n",
      "Terminated: True | Done: True\n",
      "Episode: 112, Total Reward: 10.400000000000018, Epsilon: 0.9988806213721471 | Loss: 0.5100508352922162\n",
      "Terminated: True | Done: True\n",
      "Episode: 113, Total Reward: 7.900000000000012, Epsilon: 0.9988706325659333 | Loss: 0.5072630095272893\n",
      "Terminated: True | Done: True\n",
      "Episode: 114, Total Reward: 15.099999999999994, Epsilon: 0.9988606438596077 | Loss: 0.6558721348935482\n",
      "Terminated: True | Done: True\n",
      "Episode: 115, Total Reward: 14.100000000000005, Epsilon: 0.9988506552531692 | Loss: 0.7703734868374027\n",
      "Terminated: True | Done: True\n",
      "Episode: 116, Total Reward: 15.100000000000016, Epsilon: 0.9988406667466166 | Loss: 0.8147337487807461\n",
      "Terminated: True | Done: True\n",
      "Episode: 117, Total Reward: 25.199999999999953, Epsilon: 0.9988306783399492 | Loss: 0.7002824213282735\n",
      "Terminated: True | Done: True\n",
      "Episode: 118, Total Reward: 15.100000000000001, Epsilon: 0.9988206900331659 | Loss: 0.721185488151569\n",
      "Terminated: True | Done: True\n",
      "Episode: 119, Total Reward: 11.300000000000002, Epsilon: 0.9988107018262656 | Loss: 0.46088051754102277\n",
      "Terminated: True | Done: True\n",
      "Episode: 120, Total Reward: 95.90000000000006, Epsilon: 0.9988007137192474 | Loss: 0.7927791550025052\n",
      "Terminated: True | Done: True\n",
      "Episode: 121, Total Reward: 3.8000000000000282, Epsilon: 0.9987907257121103 | Loss: 1.0817630758917913\n",
      "Terminated: True | Done: True\n",
      "Episode: 122, Total Reward: 55.49999999999998, Epsilon: 0.9987807378048532 | Loss: 0.8800403617468535\n",
      "Terminated: True | Done: True\n",
      "Episode: 123, Total Reward: 55.499999999999964, Epsilon: 0.9987707499974752 | Loss: 1.0586221863826115\n",
      "Terminated: True | Done: True\n",
      "Episode: 124, Total Reward: 45.399999999999935, Epsilon: 0.9987607622899752 | Loss: 1.3217678438214695\n",
      "Terminated: True | Done: True\n",
      "Episode: 125, Total Reward: 25.19999999999997, Epsilon: 0.9987507746823524 | Loss: 1.5527432560920715\n",
      "Terminated: True | Done: True\n",
      "Episode: 126, Total Reward: 35.3, Epsilon: 0.9987407871746057 | Loss: 1.5919553573225058\n",
      "Terminated: True | Done: True\n",
      "Episode: 127, Total Reward: 65.59999999999997, Epsilon: 0.998730799766734 | Loss: 1.883881035973044\n",
      "Terminated: True | Done: True\n",
      "Episode: 128, Total Reward: 10.00000000000002, Epsilon: 0.9987208124587363 | Loss: 1.4638987674432642\n",
      "Terminated: True | Done: True\n",
      "Episode: 129, Total Reward: 25.199999999999953, Epsilon: 0.9987108252506118 | Loss: 1.7080835828594132\n",
      "Terminated: True | Done: True\n",
      "Episode: 130, Total Reward: 13.500000000000016, Epsilon: 0.9987008381423593 | Loss: 1.9769534317415152\n",
      "Terminated: True | Done: True\n",
      "Episode: 131, Total Reward: 14.50000000000001, Epsilon: 0.9986908511339779 | Loss: 1.7904002666473389\n",
      "Terminated: True | Done: True\n",
      "Episode: 132, Total Reward: 25.199999999999953, Epsilon: 0.9986808642254666 | Loss: 1.8830461034587784\n",
      "Terminated: True | Done: True\n",
      "Episode: 133, Total Reward: 15.10000000000001, Epsilon: 0.9986708774168245 | Loss: 1.7964125041868173\n",
      "Terminated: True | Done: True\n",
      "Episode: 134, Total Reward: 14.600000000000017, Epsilon: 0.9986608907080503 | Loss: 1.6952476224728994\n",
      "Terminated: True | Done: True\n",
      "Episode: 135, Total Reward: 25.199999999999957, Epsilon: 0.9986509040991433 | Loss: 1.7723517885395126\n",
      "Terminated: True | Done: True\n",
      "Episode: 136, Total Reward: 25.199999999999953, Epsilon: 0.9986409175901023 | Loss: 224.2055863177075\n",
      "Terminated: True | Done: True\n",
      "Episode: 137, Total Reward: 4.6000000000000165, Epsilon: 0.9986309311809265 | Loss: 589.2227189005949\n",
      "Terminated: True | Done: True\n",
      "Episode: 138, Total Reward: 15.100000000000003, Epsilon: 0.9986209448716147 | Loss: 10.896172140158859\n",
      "Terminated: True | Done: True\n",
      "Episode: 139, Total Reward: 13.700000000000008, Epsilon: 0.998610958662166 | Loss: 8.625536896632267\n",
      "Terminated: True | Done: True\n",
      "Episode: 140, Total Reward: 15.09999999999997, Epsilon: 0.9986009725525794 | Loss: 5.725822219661638\n",
      "Terminated: True | Done: True\n",
      "Episode: 141, Total Reward: 25.2, Epsilon: 0.9985909865428539 | Loss: 4.6963318422728895\n",
      "Terminated: True | Done: True\n",
      "Episode: 142, Total Reward: 35.29999999999996, Epsilon: 0.9985810006329886 | Loss: 3.974473920522952\n",
      "Terminated: True | Done: True\n",
      "Episode: 143, Total Reward: 45.399999999999984, Epsilon: 0.9985710148229823 | Loss: 2.536455299340042\n",
      "Terminated: True | Done: True\n",
      "Episode: 144, Total Reward: 10.800000000000015, Epsilon: 0.9985610291128341 | Loss: 1.7596086707520993\n",
      "Terminated: True | Done: True\n",
      "Episode: 145, Total Reward: 45.4, Epsilon: 0.998551043502543 | Loss: 1.8926127915288888\n",
      "Terminated: True | Done: True\n",
      "Episode: 146, Total Reward: 15.099999999999998, Epsilon: 0.998541057992108 | Loss: 1.9204731665405572\n",
      "Terminated: True | Done: True\n",
      "Episode: 147, Total Reward: 14.500000000000004, Epsilon: 0.9985310725815282 | Loss: 1.6687808245943303\n",
      "Terminated: True | Done: True\n",
      "Episode: 148, Total Reward: 55.5, Epsilon: 0.9985210872708024 | Loss: 1.656883180141449\n",
      "Terminated: True | Done: True\n",
      "Episode: 149, Total Reward: 15.100000000000001, Epsilon: 0.9985111020599297 | Loss: 1.7012465993563335\n",
      "Terminated: True | Done: True\n",
      "Episode: 150, Total Reward: 25.199999999999967, Epsilon: 0.9985011169489091 | Loss: 1.5868722270516789\n",
      "Terminated: True | Done: True\n",
      "Episode: 151, Total Reward: 35.29999999999997, Epsilon: 0.9984911319377396 | Loss: 1.5286020513843088\n",
      "Terminated: True | Done: True\n",
      "Episode: 152, Total Reward: 45.39999999999995, Epsilon: 0.9984811470264203 | Loss: 1.6812163719943924\n",
      "Terminated: True | Done: True\n",
      "Episode: 153, Total Reward: 12.500000000000004, Epsilon: 0.9984711622149501 | Loss: 1.907783285363928\n",
      "Terminated: True | Done: True\n",
      "Episode: 154, Total Reward: 45.39999999999996, Epsilon: 0.9984611775033281 | Loss: 2.054585751365213\n",
      "Terminated: True | Done: True\n",
      "Episode: 155, Total Reward: 10.300000000000006, Epsilon: 0.998451192891553 | Loss: 1.768654516851059\n",
      "Terminated: True | Done: True\n",
      "Episode: 156, Total Reward: 35.299999999999976, Epsilon: 0.9984412083796241 | Loss: 1.7601851563827664\n",
      "Terminated: True | Done: True\n",
      "Episode: 157, Total Reward: 55.49999999999997, Epsilon: 0.9984312239675404 | Loss: 1.7419308634365307\n",
      "Terminated: True | Done: True\n",
      "Episode: 158, Total Reward: 45.399999999999956, Epsilon: 0.9984212396553007 | Loss: 2.1568134018019136\n",
      "Terminated: True | Done: True\n",
      "Episode: 159, Total Reward: 14.70000000000001, Epsilon: 0.9984112554429042 | Loss: 2.301977970383384\n",
      "Terminated: True | Done: True\n",
      "Episode: 160, Total Reward: 106.00000000000003, Epsilon: 0.9984012713303498 | Loss: 2.0514180777119657\n",
      "Terminated: True | Done: True\n",
      "Episode: 161, Total Reward: 95.90000000000003, Epsilon: 0.9983912873176366 | Loss: 2.0017524817410637\n",
      "Terminated: True | Done: True\n",
      "Episode: 162, Total Reward: 55.499999999999936, Epsilon: 0.9983813034047635 | Loss: 1.9672181547856797\n",
      "Terminated: True | Done: True\n",
      "Episode: 163, Total Reward: 25.19999999999996, Epsilon: 0.9983713195917295 | Loss: 2.1710657676060996\n",
      "Terminated: True | Done: True\n",
      "Episode: 164, Total Reward: 14.400000000000004, Epsilon: 0.9983613358785337 | Loss: 2.38900860835766\n",
      "Terminated: True | Done: True\n",
      "Episode: 165, Total Reward: 13.100000000000017, Epsilon: 0.998351352265175 | Loss: 2.7163823817817256\n",
      "Terminated: True | Done: True\n",
      "Episode: 166, Total Reward: 25.199999999999957, Epsilon: 0.9983413687516524 | Loss: 2.527245622055203\n",
      "Terminated: True | Done: True\n",
      "Episode: 167, Total Reward: -0.6999999999999638, Epsilon: 0.9983313853379648 | Loss: 2.0700927921459433\n",
      "Terminated: True | Done: True\n",
      "Episode: 168, Total Reward: 9.500000000000009, Epsilon: 0.9983214020241115 | Loss: 1.520877349878026\n",
      "Terminated: True | Done: True\n",
      "Episode: 169, Total Reward: 14.900000000000004, Epsilon: 0.9983114188100912 | Loss: 1.3414473648903504\n",
      "Terminated: True | Done: True\n",
      "Episode: 170, Total Reward: 9.000000000000023, Epsilon: 0.9983014356959031 | Loss: 1.0568626302161388\n",
      "Terminated: True | Done: True\n",
      "Episode: 171, Total Reward: 12.200000000000005, Epsilon: 0.9982914526815462 | Loss: 0.5978178343735635\n",
      "Terminated: True | Done: True\n",
      "Episode: 172, Total Reward: 15.100000000000001, Epsilon: 0.9982814697670194 | Loss: 0.6378374239977669\n",
      "Terminated: True | Done: True\n",
      "Episode: 173, Total Reward: 15.09999999999999, Epsilon: 0.9982714869523217 | Loss: 0.6140378139109588\n",
      "Terminated: True | Done: True\n",
      "Episode: 174, Total Reward: 15.100000000000001, Epsilon: 0.9982615042374522 | Loss: 0.9027568101882935\n",
      "Terminated: True | Done: True\n",
      "Episode: 175, Total Reward: 11.100000000000009, Epsilon: 0.9982515216224098 | Loss: 0.7043545120856264\n",
      "Terminated: True | Done: True\n",
      "Episode: 176, Total Reward: 25.200000000000003, Epsilon: 0.9982415391071936 | Loss: 0.6488265003643784\n",
      "Terminated: True | Done: True\n",
      "Episode: 177, Total Reward: 75.7000000000001, Epsilon: 0.9982315566918025 | Loss: 1.01160738456483\n",
      "Terminated: True | Done: True\n",
      "Episode: 178, Total Reward: 15.000000000000007, Epsilon: 0.9982215743762356 | Loss: 1.4372000167003045\n",
      "Terminated: True | Done: True\n",
      "Episode: 179, Total Reward: 55.49999999999995, Epsilon: 0.9982115921604919 | Loss: 1.8362836031352772\n",
      "Terminated: True | Done: True\n",
      "Episode: 180, Total Reward: 8.30000000000001, Epsilon: 0.9982016100445704 | Loss: 2.004897067526809\n",
      "Terminated: True | Done: True\n",
      "Episode: 181, Total Reward: 25.199999999999996, Epsilon: 0.9981916280284701 | Loss: 1.5981768042433495\n",
      "Terminated: True | Done: True\n",
      "Episode: 182, Total Reward: 15.099999999999962, Epsilon: 0.9981816461121898 | Loss: 1.7232624596240473\n",
      "Terminated: True | Done: True\n",
      "Episode: 183, Total Reward: 8.500000000000007, Epsilon: 0.9981716642957288 | Loss: 1.3978867917998223\n",
      "Terminated: True | Done: True\n",
      "Episode: 184, Total Reward: 5.200000000000005, Epsilon: 0.9981616825790859 | Loss: 1.2627355827887854\n",
      "Terminated: True | Done: True\n",
      "Episode: 185, Total Reward: 13.700000000000014, Epsilon: 0.9981517009622601 | Loss: 0.22195327158730763\n",
      "Terminated: True | Done: True\n",
      "Episode: 186, Total Reward: 3.3000000000000336, Epsilon: 0.9981417194452505 | Loss: 0.28448076364336283\n",
      "Terminated: True | Done: True\n",
      "Episode: 187, Total Reward: 15.099999999999998, Epsilon: 0.9981317380280561 | Loss: 0.09940089176519655\n",
      "Terminated: True | Done: True\n",
      "Episode: 188, Total Reward: 15.100000000000003, Epsilon: 0.9981217567106758 | Loss: 0.3514518918213891\n",
      "Terminated: True | Done: True\n",
      "Episode: 189, Total Reward: -1.8000000000000203, Epsilon: 0.9981117754931088 | Loss: 0.40883530195463785\n",
      "Terminated: True | Done: True\n",
      "Episode: 190, Total Reward: 35.29999999999996, Epsilon: 0.998101794375354 | Loss: 0.4781087235200639\n",
      "Terminated: True | Done: True\n",
      "Episode: 191, Total Reward: 10.400000000000015, Epsilon: 0.9980918133574103 | Loss: 0.5518791201163311\n",
      "Terminated: True | Done: True\n",
      "Episode: 192, Total Reward: 8.200000000000008, Epsilon: 0.9980818324392767 | Loss: 0.4718152843415737\n",
      "Terminated: True | Done: True\n",
      "Episode: 193, Total Reward: 6.300000000000011, Epsilon: 0.9980718516209524 | Loss: 0.571082019220314\n",
      "Terminated: True | Done: True\n",
      "Episode: 194, Total Reward: 35.29999999999996, Epsilon: 0.9980618709024361 | Loss: 0.9602318347669115\n",
      "Terminated: True | Done: True\n",
      "Episode: 195, Total Reward: 14.200000000000005, Epsilon: 0.9980518902837272 | Loss: 1.5264313583572706\n",
      "Terminated: True | Done: True\n",
      "Episode: 196, Total Reward: 35.29999999999997, Epsilon: 0.9980419097648244 | Loss: 1.1732710754754496\n",
      "Terminated: True | Done: True\n",
      "Episode: 197, Total Reward: 55.49999999999995, Epsilon: 0.9980319293457268 | Loss: 1.6361241206234576\n",
      "Terminated: True | Done: True\n",
      "Episode: 198, Total Reward: 11.500000000000012, Epsilon: 0.9980219490264334 | Loss: 2.2039117902174765\n",
      "Terminated: True | Done: True\n",
      "Episode: 199, Total Reward: 0.29999999999999893, Epsilon: 0.9980119688069432 | Loss: 2.2342442504724667\n",
      "Terminated: True | Done: True\n",
      "Episode: 200, Total Reward: 5.0000000000000036, Epsilon: 0.9980019886872552 | Loss: 2.0556657714279076\n",
      "Terminated: True | Done: True\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # env = TransportEnv()  # Create the environment\n",
    "    env = Environment(4)\n",
    "    agent = TransportAgent(learning_rate=0.997, start_epsilon=1.0, epsilon_decay_factor=0.99999,\n",
    "                           min_epsilon=0.1, replay_buffer_size=500, batch_size=200, network_copy_frequency=100)\n",
    "\n",
    "    episodes = 200\n",
    "    snapshots=[]\n",
    "    all_loss = []\n",
    "    for episode in range(episodes):\n",
    "        env.reset_env()\n",
    "        state = env.get_state()  # Reset the environment and get the initial state\n",
    "        done = False\n",
    "        terminated = False\n",
    "        total_reward = 0\n",
    "        episode_losses = []\n",
    "       \n",
    "        while not done or not terminated:\n",
    "            # print(\"state\", state)\n",
    "            action = agent.act(state)\n",
    "            reward, done, terminated = env.move_agent(env.actions[action])\n",
    "            next_state = env.get_state()\n",
    "            # next_state, reward, done = env.step(env.actions[action])\n",
    "            total_reward += reward\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            # print(\"agent.memory[-1]\",agent.memory[-1])\n",
    "            \n",
    "\n",
    "            # Training\n",
    "            if len(agent.memory) > agent.batch_size:\n",
    "                minibatch_indices = np.random.choice(len(agent.memory), agent.batch_size, replace=False)\n",
    "                minibatch = [agent.memory[i] for i in minibatch_indices]\n",
    "                states_batch, actions_batch, targets_batch = agent.process_minibatch(minibatch)\n",
    "                loss = agent.train_one_step(states_batch, actions_batch, targets_batch)\n",
    "                episode_losses.append(loss)\n",
    "            agent.steps_since_copy += 1\n",
    "            agent.update_target()\n",
    "\n",
    "            state = next_state\n",
    "        agent.epsilon = max(agent.epsilon * agent.epsilon_decay_factor, agent.epsilon_min)\n",
    "        if len(episode_losses) > 0:\n",
    "            mean_loss = sum(episode_losses)/len(episode_losses)\n",
    "        else:\n",
    "            mean_loss = \"n/a\"\n",
    "        all_loss.append(mean_loss)\n",
    "        # print(\"state\", state)\n",
    "        print(f\"Episode: {episode + 1}, Total Reward: {total_reward}, Epsilon: {agent.epsilon} | Loss: {mean_loss}\")\n",
    "        print(f\"Terminated: {terminated} | Done: {done}\")\n",
    "    #run_visualisation(agent)\n",
    "    plt.plot(range(1, episodes + 1), all_loss, marker='o', linestyle='-')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Mean Loss')\n",
    "    plt.title('Mean Loss vs. Episode')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
